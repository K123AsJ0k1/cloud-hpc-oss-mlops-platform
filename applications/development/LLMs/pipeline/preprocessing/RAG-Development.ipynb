{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ca3187-954f-4037-b784-f35b05b07a1a",
   "metadata": {},
   "source": [
    "## Document Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab598ed-3313-4cc1-9ded-50aca2abfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_document_data(\n",
    "    document_url: str,\n",
    "    document_type: str\n",
    ") -> any:\n",
    "    data = None\n",
    "    response = requests.get(\n",
    "        url = document_url\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        if document_type == 'text':\n",
    "            data = response.text\n",
    "        if document_type == 'json':\n",
    "            data = json.loads(response.text)\n",
    "        # handle html later\n",
    "    return data\n",
    "\n",
    "def scrape_documents(\n",
    "    url_list: any,\n",
    "    timeout: int\n",
    ") -> any:\n",
    "    documents = []\n",
    "\n",
    "    text_files = [\n",
    "        'py',\n",
    "        'md',\n",
    "        'yaml',\n",
    "        'sh'\n",
    "    ]\n",
    "\n",
    "    json_files = [\n",
    "        'ipynb'\n",
    "    ]\n",
    "    index = 0\n",
    "    for url in url_list:\n",
    "        document = {\n",
    "            'name': '',\n",
    "            'data': ''\n",
    "        }\n",
    "        url_split = url.split('/')\n",
    "        if 'github' in url_split[2]:\n",
    "            if 'raw' in url_split[2]:\n",
    "                file_end = url_split[-1].split('.')[-1]\n",
    "                document['name'] = url_split[-1]\n",
    "                if file_end in text_files:\n",
    "                    document['data'] = get_document_data(\n",
    "                        document_url = url,\n",
    "                        document_type = 'text' \n",
    "                    )\n",
    "                if file_end in json_files:\n",
    "                    document['data'] = get_document_data(\n",
    "                        document_url = url,\n",
    "                        document_type = 'json' \n",
    "                    )\n",
    "        documents.append(document)\n",
    "        index = index + 1\n",
    "        if index < len(url_list):\n",
    "            time.sleep(timeout)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47cbff-32c6-4746-9271-20608d4bb72c",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cf9093-13b9-416c-aac8-5a9b645a14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_urls = [\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/tutorials/demo_notebooks/demo_pipeline/demo-pipeline.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/experiments/article/cloud-hpc/Cloud-HPC-FMNIST-Experiment.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/backend/functions/platforms/celery.py',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/frontend/functions/platforms/redis.py',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/deployment/monitoring/kustomization.yaml',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/deployment/production/stack.yaml'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073a7ca2-91fe-41b3-82d4-55046fb25a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_urls = [\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/tutorials/demo_notebooks/demo_pipeline/demo-pipeline.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/backend/functions/platforms/celery.py',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/deployment/production/stack.yaml'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5e7af2-2223-47e1-9283-0191baeb9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_documents = scrape_documents(\n",
    "    url_list = wanted_urls,\n",
    "    timeout = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf9879-3460-4a9d-b2f5-e768f1203bf6",
   "metadata": {},
   "source": [
    "## Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ab3022-230e-4301-94c5-541c23b395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "def tree_extract_imports(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(tree_extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def tree_extract_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(tree_extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "\n",
    "def tree_extract_code_and_dependencies(\n",
    "    node: any,\n",
    "    code_text: str\n",
    ") -> any:\n",
    "    codes = []\n",
    "    if not node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name')\n",
    "        if name is None:\n",
    "            code = code_text[start_byte:end_byte].decode('utf8')\n",
    "            if not 'def' in code:\n",
    "                dependencies = tree_extract_dependencies(node, code_text)\n",
    "                codes.append({\n",
    "                    'name': 'global',\n",
    "                    'code': code,\n",
    "                    'dependencies': dependencies\n",
    "                })\n",
    "    return codes\n",
    "\n",
    "def tree_extract_functions_and_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(tree_extract_functions_and_dependencies(child, code_text))\n",
    "    return functions\n",
    "\n",
    "def tree_get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def tree_get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions\n",
    "\n",
    "def tree_create_code_document(\n",
    "    code_imports: any,\n",
    "    code_functions: any,\n",
    "    function_item: any\n",
    ") -> any:\n",
    "    used_imports = tree_get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = tree_get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': function_item['name'],\n",
    "        'dependencies': function_item['dependencies'],\n",
    "        'code': function_item['code']\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "     \n",
    "def tree_format_code_document(\n",
    "    code_document: any\n",
    ") -> any:\n",
    "    formatted_document = ''\n",
    "    for doc_import in code_document['imports']:\n",
    "        formatted_document += doc_import + '\\n'\n",
    "\n",
    "    for doc_functions in code_document['functions']:\n",
    "        formatted_document += doc_functions + '\\n'\n",
    "\n",
    "    if 0 < len(code_document['dependencies']):\n",
    "        formatted_document += 'code dependencies\\n'\n",
    "\n",
    "        for doc_dependency in code_document['dependencies']:\n",
    "            formatted_document += doc_dependency + '\\n'\n",
    "\n",
    "    if code_document['name'] == 'global':\n",
    "        formatted_document += code_document['name'] + ' code\\n'\n",
    "    else:\n",
    "        formatted_document += 'function ' + code_document['name'] + ' code\\n'\n",
    "    \n",
    "    for line in code_document['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        doc_code = re.sub(r'#.*','', line)\n",
    "        if not bool(doc_code.strip()):\n",
    "            continue\n",
    "        formatted_document += doc_code + '\\n'    \n",
    "    return formatted_document\n",
    "\n",
    "def tree_create_python_code_and_function_documents(\n",
    "    code_document: any\n",
    "):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "   \n",
    "    tree = parser.parse(\n",
    "        bytes(\n",
    "            code_document,\n",
    "            \"utf8\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    code_imports = tree_extract_imports(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_global = tree_extract_code_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_functions = tree_extract_functions_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    initial_documents = []\n",
    "    for item in code_global:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    for item in code_functions:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    formatted_documents = []\n",
    "    seen_functions = []\n",
    "    for document in initial_documents:\n",
    "        if not document['name'] == 'global':\n",
    "            if document['name'] in seen_functions:\n",
    "                continue\n",
    "        \n",
    "        formatted_document = tree_format_code_document(\n",
    "            code_document = document\n",
    "        )\n",
    "\n",
    "        formatted_documents.append(formatted_document)\n",
    "        seen_functions.append(document['name'])\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1aea2-2782-4c98-8c1b-f6dac8e7762a",
   "metadata": {},
   "source": [
    "## Document Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ae795d-ce80-4fd7-80c7-efc4d1b03a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "\n",
    "def extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_document: any\n",
    "): \n",
    "    notebook_documents = {\n",
    "        'markdown': [],\n",
    "        'code': []\n",
    "    }\n",
    "\n",
    "    notebook = nbformat.from_dict(notebook_document)\n",
    "\n",
    "    index = 1\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            notebook_documents['markdown'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "        if cell.cell_type == 'code':\n",
    "            notebook_documents['code'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "    \n",
    "    return notebook_documents\n",
    "    \n",
    "def parse_markdown_into_text(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    text = soup.get_text()\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    text = re.sub(code_block_pattern, '', text)\n",
    "    text = text.rstrip('\\n')\n",
    "    text = text.replace('\\nsh', '\\n')\n",
    "    text = text.replace('\\nbash', '\\n')\n",
    "    return text\n",
    "\n",
    "def create_python_documents(\n",
    "    python_document: any\n",
    "): \n",
    "    joined_code = ''.join(python_document)\n",
    "    block_code_documents = tree_create_python_code_and_function_documents(\n",
    "        code_document = joined_code\n",
    "    )\n",
    "\n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    code_doc_index = 0\n",
    "    for code_doc in block_code_documents:\n",
    "        row_split = code_doc.split('\\n')\n",
    "        for row in row_split:\n",
    "            if 'function' in row and 'code' in row:\n",
    "                function_name = row.split(' ')[1]\n",
    "                if not function_name in seen_function_names:\n",
    "                    seen_function_names.append(function_name)\n",
    "                else:\n",
    "                    del block_code_documents[code_doc_index]\n",
    "        code_doc_index += 1\n",
    "\n",
    "    if 0 < len(block_code_documents):\n",
    "        index = 1\n",
    "        for code_doc in block_code_documents:\n",
    "            code_documents.append({\n",
    "                'index': index,\n",
    "                'data': code_doc\n",
    "            })\n",
    "            index += 1\n",
    "        \n",
    "    formatted_documents = {\n",
    "        'code': code_documents\n",
    "    }\n",
    "    return formatted_documents\n",
    "\n",
    "def create_notebook_documents(\n",
    "    notebook_document: any\n",
    "):\n",
    "    notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "        notebook_document = notebook_document\n",
    "    )\n",
    "\n",
    "    markdown_documents = []\n",
    "    for block in notebook_documents['markdown']:\n",
    "        joined_text = ''.join(block['data'])\n",
    "        markdown_text = parse_markdown_into_text(\n",
    "            markdown_text = joined_text\n",
    "        )\n",
    "        markdown_documents.append({\n",
    "            'index': block['id'],\n",
    "            'data': markdown_text\n",
    "        })\n",
    "        \n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    for block in notebook_documents['code']:\n",
    "        joined_code = ''.join(block['data'])\n",
    "        block_code_documents = tree_create_python_code_and_function_documents(\n",
    "            code_document = joined_code\n",
    "        )\n",
    "\n",
    "        code_doc_index = 0\n",
    "        for code_doc in block_code_documents:\n",
    "            row_split = code_doc.split('\\n')\n",
    "            for row in row_split:\n",
    "                if 'function' in row and 'code' in row:\n",
    "                    function_name = row.split(' ')[1]\n",
    "                    if not function_name in seen_function_names:\n",
    "                        seen_function_names.append(function_name)\n",
    "                    else:\n",
    "                        del block_code_documents[code_doc_index]\n",
    "            code_doc_index += 1\n",
    "        \n",
    "        if 0 < len(block_code_documents):\n",
    "            sub_indexes = False\n",
    "            if 1 < len(block_code_documents):\n",
    "                sub_indexes = True\n",
    "            index = 1\n",
    "            for code_doc in block_code_documents:\n",
    "                if sub_indexes:\n",
    "                    code_documents.append({\n",
    "                        'sub-index': index, \n",
    "                        'index': block['id'],\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                else:\n",
    "                    code_documents.append({ \n",
    "                        'index': block['id'],\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                index += 1\n",
    "            \n",
    "    formatted_documents = {\n",
    "        'markdown': markdown_documents,\n",
    "        'code': code_documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568c773-068e-473b-9164-4af3bfd496bb",
   "metadata": {},
   "source": [
    "## Mongo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ab5e19-1bf5-4f7d-a034-11abd3e0a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient as mc\n",
    "\n",
    "def mongo_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return isinstance(storage_client, mc.Connection)\n",
    "\n",
    "def mongo_setup_client(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    address: str,\n",
    "    port: str\n",
    ") -> any:\n",
    "    connection_prefix = 'mongodb://(username):(password)@(address):(port)/'\n",
    "    connection_address = connection_prefix.replace('(username)', username)\n",
    "    connection_address = connection_address.replace('(password)', password)\n",
    "    connection_address = connection_address.replace('(address)', address)\n",
    "    connection_address = connection_address.replace('(port)', port)\n",
    "    mongo_client = mc(\n",
    "        host = connection_address\n",
    "    )\n",
    "    return mongo_client\n",
    "\n",
    "def mongo_get_database(\n",
    "    mongo_client: any,\n",
    "    database_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        return database\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_check_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database_exists = database_name in mongo_client.list_database_names()\n",
    "        return database_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_list_databases(\n",
    "    mongo_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        databases = mongo_client.list_database_names()\n",
    "        return databases\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        mongo_client.drop_database(database_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_get_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collection = database[collection_name]\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def mongo_check_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        collection_exists = collection_name in database.list_collection_names()\n",
    "        return collection_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_update_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_many(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_list_collections(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collections = database.list_collection_names()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try: \n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        database.drop_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_create_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    document: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.insert_one(document)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_get_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    "):\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        document = collection.find_one(filter_query)\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        return None \n",
    "\n",
    "def mongo_list_documents(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any,\n",
    "    sorting_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        documents = list(collection.find(filter_query).sort(sorting_query))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_update_document(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_one(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_remove_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    ") -> bool:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.delete_one(filter_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b841eef-c6fb-4c98-868f-ecdfaea27127",
   "metadata": {},
   "source": [
    "# Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59331e4-8afd-43e1-8488-c75830bd44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_scraped_documents(\n",
    "    mongo_client: any,\n",
    "    database_prefix: str,\n",
    "    documents: any\n",
    "):\n",
    "    for document in documents:\n",
    "        file_name = document['name']\n",
    "        file_data = document['data']\n",
    "        \n",
    "        formatted_documents = {}\n",
    "        if '.ipynb' in file_name:\n",
    "            formatted_documents = create_notebook_documents(\n",
    "                notebook_document = file_data\n",
    "            )\n",
    "            document_database_name = database_prefix + '-workflows'\n",
    "        if '.py' in file_name:\n",
    "            formatted_documents = create_python_documents(\n",
    "                python_document = file_data\n",
    "            )\n",
    "            document_database_name = database_prefix + '-code'\n",
    "        \n",
    "        for doc_type, doc_data in formatted_documents.items():\n",
    "            for document in doc_data:\n",
    "                document_data = document['data']\n",
    "                document_index = document['index']\n",
    "                document_sub_index = 0\n",
    "\n",
    "                if 'sub-index' in document:\n",
    "                    document_sub_index = document['sub-index']\n",
    "                \n",
    "                result = mongo_create_document(\n",
    "                    mongo_client = mongo_client,\n",
    "                    database_name = document_database_name,\n",
    "                    collection_name = file_name,\n",
    "                    document = {\n",
    "                        'index': int(document_index),\n",
    "                        'sub-index': int(document_sub_index),\n",
    "                        'type': doc_type,\n",
    "                        'data': document_data\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61adca13-a8bf-4a6b-b38a-2334e0c25271",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = mongo_setup_client(\n",
    "    username = 'mongo123',\n",
    "    password = 'mongo456',\n",
    "    address = '127.0.0.1',\n",
    "    port = '27017'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b2b78e-b21a-431c-ae82-ec933c4bc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_scraped_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    database_prefix = 'llm-rag',\n",
    "    documents = scraped_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24006697-4cac-40b3-9514-73898464b117",
   "metadata": {},
   "source": [
    "## LangChain Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993cccc3-a6fd-4096-a10e-3ccd8196b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def langchain_generate_code_document_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    document_chunks = splitter.create_documents([document])\n",
    "    document_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return document_chunks\n",
    "\n",
    "def langchain_generate_document_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    document_chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = document_chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aea149-4e62-4715-9ad1-03360ea64497",
   "metadata": {},
   "source": [
    "## Qdrant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87982169-a33a-4198-93f6-a1e9ebcb5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient as qc\n",
    "from qdrant_client import models\n",
    "\n",
    "def qdrant_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, qc.Connection)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_setup_client(\n",
    "    api_key: str,\n",
    "    address: str, \n",
    "    port: str\n",
    ") -> any:\n",
    "    try:\n",
    "        qdrant_client = qc(\n",
    "            host = address,\n",
    "            port = int(port),\n",
    "            api_key = api_key,\n",
    "            https = False\n",
    "        ) \n",
    "        return qdrant_client\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_create_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str,\n",
    "    configuration: any\n",
    ") -> any:\n",
    "    try:\n",
    "        result = qdrant_client.create_collection(\n",
    "            collection_name = collection_name,\n",
    "            vectors_config = configuration\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_get_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(\n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_list_collections(\n",
    "    qdrant_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_list = []\n",
    "        for description in collections.collections:\n",
    "            collection_list.append(description.name)\n",
    "        return collection_list\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_upsert_points(\n",
    "    qdrant_client: qc, \n",
    "    collection_name: str,\n",
    "    points: any\n",
    ") -> any:\n",
    "    try:\n",
    "        results = qdrant_client.upsert(\n",
    "            collection_name = collection_name, \n",
    "            points = points\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_search_data(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    scroll_filter: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.scroll(\n",
    "            collection_name = collection_name,\n",
    "            scroll_filter = scroll_filter,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "def qdrant_search_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    query_vector: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.search(\n",
    "            collection_name = collection_name,\n",
    "            query_vector = query_vector,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str, \n",
    "    vectors: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        results = qdrant_client.delete_vectors(\n",
    "            collection_name = collection_name,\n",
    "            vectors = vectors\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing document: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a005608-d938-4cbc-9f63-72796dc7848b",
   "metadata": {},
   "source": [
    "# Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acde4f66-6c16-47fa-b070-83ff480251e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_prefix = 'llm-rag'\n",
    "storage_structure = {}\n",
    "database_list = mongo_list_databases(\n",
    "    mongo_client = mongo_client\n",
    ")\n",
    "for database in database_list:\n",
    "    if database_prefix in database:\n",
    "        collection_list = mongo_list_collections(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database\n",
    "        )\n",
    "        storage_structure[database] = collection_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a641df3-658c-4ff7-932f-d619ef5192bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm-rag-code': ['celery.py'], 'llm-rag-workflows': ['demo-pipeline.ipynb']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ab797c-1593-4a73-9e62-23549e1cf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "\n",
    "storage_documents = {}\n",
    "for database, collections in storage_structure.items():\n",
    "    if not database in storage_documents:\n",
    "        storage_documents[database] = {}\n",
    "    for collection in collections:\n",
    "        collection_documents = mongo_list_documents(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database,\n",
    "            collection_name = collection,\n",
    "            filter_query = {},\n",
    "            sorting_query = [\n",
    "                ('index', ASCENDING),\n",
    "                ('sub-index', ASCENDING)\n",
    "            ]\n",
    "        )\n",
    "        storage_documents[database][collection] = collection_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6bfa8f2-38e7-4a9e-9902-43d2d4ff637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_generate_code_document_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    document_chunks = splitter.create_documents([document])\n",
    "    document_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return document_chunks\n",
    "\n",
    "def lanchain_generate_text_document_chunks(\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    document_chunks = splitter.create_documents([document])\n",
    "    document_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return document_chunks\n",
    "\n",
    "def langchain_generate_document_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    document_chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = document_chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9442fc86-e594-4ee0-9152-69fd6c92fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "def generate_document_vector_packet(\n",
    "    document: any,\n",
    "    configuration: any,\n",
    ") -> any:\n",
    "    document_type = document['type']\n",
    "    used_configuration = configuration[document_type]\n",
    "    \n",
    "    document_chunks = []\n",
    "    if document_type == 'code':\n",
    "        document_chunks = langchain_generate_code_document_chunks(\n",
    "            language = Language.PYTHON,\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    if document_type == 'markdown':\n",
    "        document_chunks = lanchain_generate_text_document_chunks(\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "        \n",
    "    vector_embedding = langchain_generate_document_chunk_embeddings(\n",
    "        model_name = used_configuration['model-name'],\n",
    "        document_chunks = document_chunks\n",
    "    )\n",
    "\n",
    "    packet = {\n",
    "        'chunks': document_chunks,\n",
    "        'embeddings': vector_embedding\n",
    "    }\n",
    "    \n",
    "    return packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "09d804dc-f342-4e58-97f6-e111f37bafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_code_document = storage_documents['llm-rag-code']['celery.py'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "da2aa6d5-1bf3-495d-a898-5657669d7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_document = storage_documents['llm-rag-workflows']['demo-pipeline.ipynb'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "abe16f60-7a6a-47fa-acb5-267d150c88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "example_code_vector_embedding = generate_document_vector_embedding(\n",
    "    document = example_code_document,\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 0,\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a568c1c3-8f25-46dd-a150-4e766072c4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "example_text_vector_embedding = generate_document_vector_embedding(\n",
    "    document = example_text_document,\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 0,\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d066c6-37c8-404c-9f7c-989c74dbf294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "qdrant_client = qdrant_setup_client(\n",
    "    api_key = 'qdrant_key',\n",
    "    address = '127.0.0.1', \n",
    "    port = '6333'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17724b9b-b18c-4dfa-81b1-40a6e7840072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcf76e9e-3863-5e32-89b0-3a23cb3e80b0\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "vector_id = '671b31b2f12238512f2cafdd' + '-1'\n",
    "vector_uuid = uuid.uuid5(uuid.NAMESPACE_DNS, vector_id)\n",
    "print(str(vector_uuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3d6a2df-c9b4-487d-b8bd-49b5f9703d9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "qdrant_search_data() got an unexpected keyword argument 'query_filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m existing_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mqdrant_search_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllm-rag-workflows-embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_filter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmust\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_hash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3c177a2a09134a89280e93580f24d2d9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: qdrant_search_data() got an unexpected keyword argument 'query_filter'"
     ]
    }
   ],
   "source": [
    "existing_chunk = qdrant_search_data(\n",
    "    qdrant_client = qdrant_client, \n",
    "    collection_name = 'llm-rag-workflows-embeddings',\n",
    "    query_filter = {\n",
    "        \"must\": [\n",
    "            {\n",
    "                \"key\": \"chunk_hash\", \n",
    "                \"match\": {\n",
    "                    \"value\": '3c177a2a09134a89280e93580f24d2d9'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    limit = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8d336f5-7f3e-44dc-becd-1564c76d8f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_chunks = qdrant_search_data(\n",
    "    qdrant_client = qdrant_client,\n",
    "    collection_name = 'llm-rag-workflows-embeddings',\n",
    "    scroll_filter = models.Filter(\n",
    "        must = [\n",
    "            models.FieldCondition(\n",
    "                key='chunk_hash',\n",
    "                match = models.MatchValue(value=\"3c177a2a09134a89280e93580f24d2d9\")\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    limit = 5\n",
    ")\n",
    "len(existing_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c81e336-e602-4d72-b7cd-1e1f4a7c8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vector = [-0.0008806156,0.07525727,-0.039082013,0.016676342,0.019496996,-0.0183225,0.10499067,0.007775761,-0.009872934,-0.03422199,-0.052432895,-0.0108636115,0.078131415,-0.0073763705,-0.024622837,0.0040045134,0.029577108,-0.060306843,-0.023875061,-0.05675382,0.02992669,0.037502546,0.008587845,0.1237185,-0.08706975,0.011235719,0.034271877,0.008478244,-0.02619887,0.004774774,0.018081691,-0.062485863,-0.01694757,-0.017412446,0.09253343,0.034455594,0.037253767,-0.07208341,0.057675894,-0.061799947,-0.018354114,-0.046897687,-0.023256231,0.003918217,-0.011575811,0.04200087,-0.05075061,-0.042651433,0.06726092,0.010764077,-0.047839403,-0.032385066,0.0016082315,-0.027480885,0.050767735,0.014470795,-0.01207069,0.002131377,-0.029967006,0.016777797,0.04471435,-0.06823574,0.045421507,-0.026454143,-0.009379231,-0.0102507975,-0.086166084,-0.06292877,-0.02551088,-0.055481803,-0.01607503,0.056653056,-0.00003854536,0.015639594,0.01604736,-0.08630768,0.020671349,-0.0911355,-0.030291177,-0.020271132,-0.07643146,-0.028326187,-0.040162094,0.006570532,0.11752413,-0.08517199,-0.027548267,-0.000120679266,0.06321616,-0.07607285,-0.047586102,-0.05989854,-0.004406365,0.031803537,0.0628275,-0.019911483,0.04521456,0.028922873,-0.041679494,0.040077474,-0.032429293,-0.034646623,-0.019596737,0.017813457,-0.017751787,0.053845163,-0.017280385,0.028979478,-0.051279325,-0.09619954,-0.0799265,-0.00690425,0.0343595,0.06598981,0.012640191,-0.03366004,0.012655348,0.11719685,0.0028017166,-0.023670634,0.14810836,0.008867465,-0.020808151,0.02270183,-0.08633888,-0.05448335,0.06953763,-4.0176204e-33,0.04469456,-0.011172801,0.0124054905,0.022486169,0.011005086,-0.008940501,-0.036023315,0.06895857,0.03385835,0.023042355,-0.036956806,-0.036285393,-0.019124702,0.037292037,-0.016623108,0.01791072,-0.0021501917,0.06982369,-0.0078103733,0.011628215,-0.025783157,-0.014041182,-0.094312415,0.024015969,0.04719983,0.0202716,0.0059256833,-0.023461312,-0.032074198,0.0030213667,-0.05630584,0.016784912,-0.041350484,-0.010510119,-0.032780495,0.10691323,0.015510786,-0.022785041,-0.003027085,-0.01625067,0.06831153,0.07440746,0.023756888,-0.018152632,0.012086324,-0.038340032,0.0063405824,-0.00086272124,0.0042086556,-0.025579445,-0.0050925,-0.024872497,0.026971173,-0.05092251,0.00025128145,-0.08662072,0.0539211,0.053984,0.040535644,-0.068476476,0.026984764,-0.06850057,-0.051462118,0.042254355,0.03951172,0.055016063,0.060717493,0.06802623,0.10957967,0.02805403,-0.06449128,-0.022178398,-0.00060761836,0.024709838,-0.016398955,-0.013924856,-0.003000167,-0.12039671,-0.099169396,0.065107524,-0.042745173,-0.0132074375,-0.109735966,-0.079504326,-0.013722966,0.07654049,-0.037829973,0.003370593,-0.050290912,0.03221142,-0.058589935,0.04097386,-0.04631355,0.03644445,-0.057122815,5.1131777e-34,-0.036972407,0.00331556,-0.05163985,-0.08187577,0.08633707,-0.025570264,0.15140791,-0.07599442,-0.09741537,0.041227363,-0.0020351557,-0.15771613,-0.06736124,0.041812494,0.12581694,0.03375963,-0.030657541,0.04516917,-0.061553475,0.018996006,-0.12033124,0.14018124,-0.019660208,0.008262802,0.058510397,-0.005143964,-0.0030865462,-0.018254856,-0.0015348716,-0.0881829,-0.00154272,-0.077873126,-0.08744284,0.09301118,0.0075481553,0.012195563,-0.044611398,-0.066665746,-0.038523663,-0.0030063111,0.075059116,0.009463261,0.02251678,0.13548115,-0.0037547234,0.045695756,0.022946576,-0.00007905387,-0.099864155,0.037794977,0.0062554525,-0.03725619,-0.064578496,-0.023896668,0.07721146,-0.10511356,-0.0059987954,-0.070528,-0.036697168,-0.022170186,0.054968473,0.06914329,-0.019348279,0.07726558,0.022413678,-0.047802236,-0.04092273,0.014742518,0.06048497,-0.0032601384,0.0014216868,0.024004899,0.030322358,0.02016151,0.0041131,-0.021624237,-0.03957565,0.07552214,-0.06119625,0.01242295,-0.005579958,-0.05356618,0.0481288,-0.015847042,-0.1067542,-0.0038143944,0.098899946,0.10055497,-0.1070229,-0.015711667,0.050163187,-0.015227208,0.0012801245,0.019454584,-0.014298509,-1.6927226e-8,0.021207215,0.09786308,-0.008242949,-0.0599221,-0.011646559,-0.01923452,-0.037953626,-0.003184353,-0.0009942062,-0.028584054,0.024394933,-0.10110541,0.058031734,0.045483343,0.0056083733,-0.030457597,-0.06479304,0.121118,0.04755318,0.0004189592,0.042921025,0.02015465,-0.06455849,-0.058625087,0.045303024,-0.034165505,-0.046124157,0.05671656,0.051498257,0.047485758,0.02200334,0.018044686,-0.020871876,0.050238602,0.07088494,-0.03550859,0.03834522,0.027851177,0.0061599715,-0.0051157954,0.029486923,0.057594728,0.02301489,0.033285838,0.0062656887,-0.010056981,-0.031857565,-0.029003937,0.039308313,0.006076029,-0.06574017,-0.053721618,0.076442,0.0115394825,-0.011749002,0.028453192,0.09319499,-0.033753525,-0.061961908,0.0005717238,0.08093922,0.09229381,-0.013735305,0.020243453]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ef1d6843-2b31-4049-97ad-44e1fe0ef047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7621589\n",
      "'run_id=train_task.outputs[\"run_id\"],'\n",
      "0.6500319\n",
      "'preprocess_task ='\n",
      "0.64411044\n",
      "'train_task = train('\n",
      "0.6295434\n",
      "'run_id = run.info.run_id'\n",
      "0.5800556\n",
      "')\\n        inference_task = inference('\n",
      "0.5717762\n",
      "'run_id)'\n",
      "0.5699499\n",
      "'test_set=preprocess_task.outputs[\"test_set\"],'\n",
      "0.55575526\n",
      "'train_set=preprocess_task.outputs[\"train_set\"],'\n",
      "0.5521839\n",
      "'):\\n    pull_task = pull_data(url=url)'\n",
      "0.5379623\n",
      "'deploy_model_task = deploy_model('\n"
     ]
    }
   ],
   "source": [
    "vectors = qdrant_search_vectors(\n",
    "    qdrant_client = qdrant_client,\n",
    "    collection_name = 'llm-rag-workflows-embeddings',\n",
    "    query_vector = sample_vector,\n",
    "    limit = 10\n",
    ")\n",
    "#print(vectors)\n",
    "for case in vectors:\n",
    "    print(case.score)\n",
    "    print(repr(case.payload['chunk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d85edbe6-3c92-4d96-b4a6-62ca8cf920e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Record(id='005732c9-4ce0-5230-bb85-96de0b7b6c1c', payload={'database': 'llm-rag-workflows', 'collection': 'demo-pipeline.ipynb', 'id': '671b31b2f12238512f2cafd1', 'chunk': 'if len(resp.history) == 0:', 'chunk_hash': '3c177a2a09134a89280e93580f24d2d9'}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id='b7416704-cf40-521d-865d-a4ba79fc56d4', payload={'database': 'llm-rag-workflows', 'collection': 'demo-pipeline.ipynb', 'id': '671b31b2f12238512f2cafd1', 'chunk': 'if len(resp.history) == 0:', 'chunk_hash': '3c177a2a09134a89280e93580f24d2d9'}, vector=None, shard_key=None, order_value=None)],\n",
       " None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "qdrant_client.scroll(\n",
    "    collection_name = 'llm-rag-workflows-embeddings',\n",
    "    scroll_filter = models.Filter(\n",
    "        must = [\n",
    "            models.FieldCondition(\n",
    "                key='chunk_hash',\n",
    "                match = models.MatchValue(value=\"3c177a2a09134a89280e93580f24d2d9\")\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    limit = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1462380-1ef4-45fa-bacc-51831c308e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2a3c9c46-b083-4ebc-8450-644c8bb9c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import uuid\n",
    "import re\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "# for preventing duplicates\n",
    "def format_chunk(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    chunk = re.sub(r'[^\\w\\s]', '', document_chunk)\n",
    "    chunk = re.sub(r'\\s+', ' ', chunk) \n",
    "    chunk = chunk.strip()\n",
    "    chunk = chunk.lower()\n",
    "    # This helps to remove unique hashes for duplicates such as:\n",
    "    # task_id = task_id )\n",
    "    # task_id = task_id \n",
    "    # task_id = task_id )\n",
    "    return chunk\n",
    "\n",
    "def generate_chunk_hash(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    cleaned_chunk = format_chunk(\n",
    "        document_chunk = document_chunk\n",
    "    )\n",
    "    return hashlib.md5(cleaned_chunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "vector_configuration = {\n",
    "    'code': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'markdown': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    }\n",
    "}\n",
    "\n",
    "# \n",
    "for database, collections in storage_documents.items():\n",
    "    vector_collection_name = database + '-embeddings'\n",
    "    for collection, documents in collections.items():\n",
    "        for document in documents:\n",
    "            document_id = str(document['_id'])\n",
    "            document_type = document['type']\n",
    "            \n",
    "            vector_packet = generate_document_vector_packet(\n",
    "                document = document,\n",
    "                configuration = vector_configuration\n",
    "            )\n",
    "            \n",
    "            document_chunks = vector_packet['chunks']\n",
    "            document_embeddings = vector_packet['embeddings']\n",
    "            if 0 < len(document_embeddings):\n",
    "                vector_collections = qdrant_list_collections(\n",
    "                    qdrant_client = qdrant_client\n",
    "                )\n",
    "\n",
    "                if not vector_collection_name in vector_collections:\n",
    "                    vector_collection_configuration = VectorParams(\n",
    "                          size = len(document_embeddings[0]), \n",
    "                          distance = Distance.COSINE\n",
    "                    )\n",
    "                    collection_created = qdrant_create_collection(\n",
    "                        qdrant_client = qdrant_client,\n",
    "                        collection_name = vector_collection_name,\n",
    "                        configuration = vector_collection_configuration\n",
    "                    )\n",
    "\n",
    "                vector_points = []\n",
    "                vector_index = 0\n",
    "                added_hashes = []\n",
    "                for chunk in document_chunks:\n",
    "                    vector_id = document_id + '-' + str(vector_index + 1)\n",
    "                    vector_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, vector_id))\n",
    "\n",
    "                    chunk_hash = generate_chunk_hash(\n",
    "                        document_chunk = chunk\n",
    "                    )\n",
    "                   \n",
    "                    existing_chunks = qdrant_search_data(\n",
    "                        qdrant_client = qdrant_client,\n",
    "                        collection_name = vector_collection_name,\n",
    "                        scroll_filter = models.Filter(\n",
    "                            must = [\n",
    "                                models.FieldCondition(\n",
    "                                    key = 'chunk_hash',\n",
    "                                    match = models.MatchValue(\n",
    "                                        value = chunk_hash\n",
    "                                    )\n",
    "                                )\n",
    "                            ]\n",
    "                        ),\n",
    "                        limit = 1\n",
    "                    )\n",
    "                    # Removes duplicates\n",
    "                    if len(existing_chunks[0]) == 0:\n",
    "                        if not chunk_hash in added_hashes:\n",
    "                            given_vector = document_embeddings[vector_index]\n",
    "\n",
    "                            chunk_point = PointStruct(\n",
    "                                id = vector_uuid, \n",
    "                                vector = given_vector,\n",
    "                                payload = {\n",
    "                                    'database': database,\n",
    "                                    'collection': collection,\n",
    "                                    'document': document_id,\n",
    "                                    'type': document_type,\n",
    "                                    'chunk': chunk,\n",
    "                                    'chunk_hash': chunk_hash\n",
    "                                }\n",
    "                            )\n",
    "                            added_hashes.append(chunk_hash)\n",
    "                            vector_points.append(chunk_point)\n",
    "                    vector_index += 1\n",
    "\n",
    "                if 0 < len(vector_points):\n",
    "                    points_stored = qdrant_upsert_points(\n",
    "                        qdrant_client = qdrant_client, \n",
    "                        collection_name = vector_collection_name,\n",
    "                        points = vector_points\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f1691-83b5-4ea8-9c94-3ca18a402421",
   "metadata": {},
   "source": [
    "## NLTK Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "703906a6-4dab-4ecb-8ee1-7cc62c060c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_document_keywords(\n",
    "    document: any\n",
    "):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = document.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = list(dict.fromkeys(tokens))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c488e6f-ea6a-48f1-a948-b86e61d88851",
   "metadata": {},
   "source": [
    "## Meili Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a2a5e0c5-f27a-4f25-acfe-a8f1b0648bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meilisearch as ms\n",
    "\n",
    "def meili_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, ms.Connection)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "def meili_setup_client(\n",
    "    host: str, \n",
    "    api_key: str\n",
    ") -> any:\n",
    "    try:\n",
    "        meili_client = ms.Client(\n",
    "            url = host, \n",
    "            api_key = api_key\n",
    "        )\n",
    "        return meili_client \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_get_index( \n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_check_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        meili_client.get_index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def meili_remove_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        response = meili_client.index(\n",
    "            index_name = index_name\n",
    "        ).delete()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_list_indexes(\n",
    "    meili_client: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        indexes = meili_client.get_indexes()\n",
    "        return indexes\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_add_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    documents: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.add_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_search_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    query: any, \n",
    "    options: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.search(\n",
    "            query = query, \n",
    "            options = options\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_update_documents(\n",
    "    meili_client, \n",
    "    index_name, \n",
    "    documents\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_delete_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    ids: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.delete_documents(\n",
    "            document_ids = ids\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517757ce-24c3-4a65-937f-840c30f49e7c",
   "metadata": {},
   "source": [
    "## Keyword Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6844a3a-3063-4eee-927f-e4b5d952a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meili_client = meili_setup_client(\n",
    "    host = 'http://127.0.0.1:7700', \n",
    "    api_key = 'meili_key'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f8a3d862-c240-45a4-afcb-69cd413a5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "for database, collections in storage_documents.items():\n",
    "    keyword_collection_name = database + '-keywords'\n",
    "    keyword_documents = []\n",
    "    for collection, documents in collections.items():\n",
    "        document_index = 0\n",
    "        for document in documents:\n",
    "            document_id = str(document['_id'])\n",
    "            document_data = document['data']\n",
    "            document_type = document['type']\n",
    "            \n",
    "            document_keywords = get_document_keywords(\n",
    "                document = document_data\n",
    "            )\n",
    "            \n",
    "            keyword_id = document_id + '-' + str(document_index + 1)\n",
    "            keyword_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, keyword_id))\n",
    "\n",
    "            payload = {\n",
    "                'id': keyword_uuid,\n",
    "                'database': database,\n",
    "                'collection': collection,\n",
    "                'document': document_id,\n",
    "                'type': document_type,\n",
    "                'keywords': document_keywords\n",
    "            }\n",
    "\n",
    "            keyword_documents.append(payload)\n",
    "        \n",
    "    stored = meili_add_documents(\n",
    "        meili_client = meili_client,\n",
    "        index_name = keyword_collection_name,\n",
    "        documents = keyword_documents\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9562f197-a9f0-470c-a41b-b14285008364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'dd06dcd8-c9c5-51bd-9d78-d8f5725af93c',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b1f12238512f2cafbc',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['demo', 'kfp', 'pipelin']},\n",
       " {'id': '41fa29aa-7fce-5ef7-8bd0-4a2a6428e5ad',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafbd',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['instal', 'requir']},\n",
       " {'id': '2fe312e6-e4cf-5a33-9c02-c4e99ff582bc',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafcf',\n",
       "  'type': 'code',\n",
       "  'keywords': ['global', 'code', 'bash', 'pip', 'instal', 'kfp~=1.8.14']},\n",
       " {'id': '30731e7b-17c4-51b7-8359-d8c47a0fb041',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafbe',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['import']},\n",
       " {'id': '7ac09ad2-7723-5857-a392-124513aa7dfc',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd0',\n",
       "  'type': 'code',\n",
       "  'keywords': ['import',\n",
       "   'warn',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'warnings.filterwarn',\n",
       "   'global',\n",
       "   '``',\n",
       "   'ignor',\n",
       "   \"''\",\n",
       "   'kfp',\n",
       "   'kfp.dsl',\n",
       "   'dsl',\n",
       "   'kfp.aw',\n",
       "   'use_aws_secret',\n",
       "   'kfp.v2.dsl',\n",
       "   'compon',\n",
       "   'input',\n",
       "   'output',\n",
       "   'dataset',\n",
       "   'metric',\n",
       "   'artifact',\n",
       "   'model']},\n",
       " {'id': '8dad92f0-bdbf-558e-84ce-7e99043d6514',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafbf',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['1.',\n",
       "   'connect',\n",
       "   'client',\n",
       "   'default',\n",
       "   'way',\n",
       "   'access',\n",
       "   'kubeflow',\n",
       "   'via',\n",
       "   'port-forward',\n",
       "   'enabl',\n",
       "   'get',\n",
       "   'start',\n",
       "   'quickli',\n",
       "   'without',\n",
       "   'impos',\n",
       "   'requir',\n",
       "   'environ',\n",
       "   'run',\n",
       "   'follow',\n",
       "   'istio',\n",
       "   \"'s\",\n",
       "   'ingress-gateway',\n",
       "   'local',\n",
       "   'port',\n",
       "   '8080',\n",
       "   'kubectl',\n",
       "   'svc/istio-ingressgateway',\n",
       "   '-n',\n",
       "   'istio-system',\n",
       "   '8080:80']},\n",
       " {'id': 'd2bc8d5a-1055-5897-9231-6ea5d2745e2b',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd1',\n",
       "  'type': 'code',\n",
       "  'keywords': ['import',\n",
       "   'request',\n",
       "   'urllib.pars',\n",
       "   'urlsplit',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'requests.sess',\n",
       "   's.get',\n",
       "   'runtimeerror',\n",
       "   'len',\n",
       "   're.search',\n",
       "   'redirect_url_obj._replac',\n",
       "   're.sub',\n",
       "   'redirect_url_obj.geturl',\n",
       "   's.post',\n",
       "   \"''\",\n",
       "   '``',\n",
       "   '.join',\n",
       "   'function',\n",
       "   'get_istio_auth_sess',\n",
       "   'def',\n",
       "   'url',\n",
       "   'str',\n",
       "   'usernam',\n",
       "   'password',\n",
       "   'dict',\n",
       "   'determin',\n",
       "   'specifi',\n",
       "   'secur',\n",
       "   'dex',\n",
       "   'tri',\n",
       "   'obtain',\n",
       "   'session',\n",
       "   'cooki',\n",
       "   'warn',\n",
       "   'staticpassword',\n",
       "   'ldap',\n",
       "   'authent',\n",
       "   'current',\n",
       "   'support',\n",
       "   'default',\n",
       "   'use',\n",
       "   'enabl',\n",
       "   'param',\n",
       "   'kubeflow',\n",
       "   'server',\n",
       "   'includ',\n",
       "   'protocol',\n",
       "   'return',\n",
       "   'auth',\n",
       "   'inform',\n",
       "   'auth_sess',\n",
       "   'endpoint_url',\n",
       "   'redirect_url',\n",
       "   'none',\n",
       "   'dex_login_url',\n",
       "   'is_secur',\n",
       "   'session_cooki',\n",
       "   'resp',\n",
       "   'allow_redirects=tru',\n",
       "   'resp.status_cod',\n",
       "   '200',\n",
       "   'rais',\n",
       "   'http',\n",
       "   'statu',\n",
       "   'get',\n",
       "   'resp.url',\n",
       "   'resp.histori',\n",
       "   '==',\n",
       "   'fals',\n",
       "   'els',\n",
       "   'true',\n",
       "   'redirect_url_obj',\n",
       "   '/auth',\n",
       "   'redirect_url_obj.path',\n",
       "   'path=re.sub',\n",
       "   '/auth/loc',\n",
       "   '/auth/',\n",
       "   '/login',\n",
       "   'data=',\n",
       "   'login',\n",
       "   'credenti',\n",
       "   'probabl',\n",
       "   'invalid',\n",
       "   'redirect',\n",
       "   'post',\n",
       "   \"'dex_login_url\",\n",
       "   'c.name',\n",
       "   'c.valu',\n",
       "   's.cooki']},\n",
       " {'id': '3fb9ee11-2c02-594f-8b26-b8a129189e64',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd2',\n",
       "  'type': 'code',\n",
       "  'keywords': ['import',\n",
       "   'kfp',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'get_istio_auth_sess',\n",
       "   'kfp.client',\n",
       "   'global',\n",
       "   'kubeflow_endpoint',\n",
       "   '``',\n",
       "   'http',\n",
       "   '//localhost:8080',\n",
       "   \"''\",\n",
       "   'kubeflow_usernam',\n",
       "   'user',\n",
       "   'example.com',\n",
       "   'kubeflow_password',\n",
       "   '12341234',\n",
       "   'auth_sess',\n",
       "   'url=kubeflow_endpoint',\n",
       "   'username=kubeflow_usernam',\n",
       "   'password=kubeflow_password',\n",
       "   'client',\n",
       "   'host=f',\n",
       "   '/pipelin',\n",
       "   'cookies=auth_sess',\n",
       "   'session_cooki']},\n",
       " {'id': '28ee785c-6c9f-585f-934e-208efdfa0003',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc0',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['2.',\n",
       "   'compon',\n",
       "   'differ',\n",
       "   'way',\n",
       "   'defin',\n",
       "   'kfp',\n",
       "   'use',\n",
       "   'decor',\n",
       "   'python',\n",
       "   'function-bas',\n",
       "   'annot',\n",
       "   'convert',\n",
       "   'function',\n",
       "   'factori',\n",
       "   'creat',\n",
       "   'pipelin',\n",
       "   'step',\n",
       "   'execut',\n",
       "   'exampl',\n",
       "   'also',\n",
       "   'specifi',\n",
       "   'base',\n",
       "   'contain',\n",
       "   'imag',\n",
       "   'run']},\n",
       " {'id': 'acbdd393-7f4e-5e2c-90cd-61eace6d6add',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc1',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['pull', 'data', 'compon']},\n",
       " {'id': '032f8af2-c1de-5977-bd8f-9a227385a9b0',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd3',\n",
       "  'type': 'code',\n",
       "  'keywords': ['code',\n",
       "   'depend',\n",
       "   'pd.read_csv',\n",
       "   'df.to_csv',\n",
       "   'function',\n",
       "   'pull_data',\n",
       "   'def',\n",
       "   'url',\n",
       "   'str',\n",
       "   'data',\n",
       "   'output',\n",
       "   'dataset',\n",
       "   '``',\n",
       "   \"''\",\n",
       "   'pull',\n",
       "   'component.',\n",
       "   'import',\n",
       "   'panda',\n",
       "   'pd',\n",
       "   'df',\n",
       "   'sep=',\n",
       "   'data.path',\n",
       "   'index=non']},\n",
       " {'id': 'a05746c8-abc5-51b1-9739-2e1ec0329a73',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc2',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['preprocess', 'compon']},\n",
       " {'id': '7c3b75f7-6d62-564c-a221-9c2d403a4ac3',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd4',\n",
       "  'type': 'code',\n",
       "  'keywords': ['sklearn.model_select',\n",
       "   'import',\n",
       "   'train_test_split',\n",
       "   'sklearn.preprocess',\n",
       "   'standardscal',\n",
       "   'pickl',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'pd.read_csv',\n",
       "   'train.drop',\n",
       "   'scaler.fit_transform',\n",
       "   'test.drop',\n",
       "   'scaler.transform',\n",
       "   'open',\n",
       "   'pickle.dump',\n",
       "   'train.to_csv',\n",
       "   'test.to_csv',\n",
       "   'function',\n",
       "   'preprocess',\n",
       "   'def',\n",
       "   'data',\n",
       "   'input',\n",
       "   'dataset',\n",
       "   'scaler_out',\n",
       "   'output',\n",
       "   'artifact',\n",
       "   'train_set',\n",
       "   'test_set',\n",
       "   'target',\n",
       "   'str',\n",
       "   '``',\n",
       "   'qualiti',\n",
       "   \"''\",\n",
       "   'component.',\n",
       "   'panda',\n",
       "   'pd',\n",
       "   'data.path',\n",
       "   'train',\n",
       "   'test',\n",
       "   'scaler',\n",
       "   'axis=1',\n",
       "   '.column',\n",
       "   'scaler_out.path',\n",
       "   \"'wb\",\n",
       "   'fp',\n",
       "   'pickle.highest_protocol',\n",
       "   'train_set.path',\n",
       "   'index=non',\n",
       "   'test_set.path']},\n",
       " {'id': '2c866171-610a-5443-9277-54fbad7d194d',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc3',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['train', 'compon']},\n",
       " {'id': '66e06fc8-5289-5a54-9920-77bb29e8e7af',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd5',\n",
       "  'type': 'code',\n",
       "  'keywords': ['type',\n",
       "   'import',\n",
       "   'namedtupl',\n",
       "   'log',\n",
       "   'sklearn.metr',\n",
       "   'mean_squared_error',\n",
       "   'mean_absolute_error',\n",
       "   'r2_score',\n",
       "   'mlflow',\n",
       "   'sklearn.linear_model',\n",
       "   'elasticnet',\n",
       "   'pickl',\n",
       "   'collect',\n",
       "   'ice',\n",
       "   'eval_metr',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'logging.basicconfig',\n",
       "   'logging.getlogg',\n",
       "   'np.sqrt',\n",
       "   'pd.read_csv',\n",
       "   'train.drop',\n",
       "   'test.drop',\n",
       "   'logger.info',\n",
       "   'mlflow.set_tracking_uri',\n",
       "   'mlflow.set_experi',\n",
       "   'mlflow.start_run',\n",
       "   'model.fit',\n",
       "   'model.predict',\n",
       "   'mlflow.log_param',\n",
       "   'mlflow.log_metr',\n",
       "   'mlflow.sklearn.log_model',\n",
       "   'np.save',\n",
       "   'mlflow.log_artifact',\n",
       "   'logging.info',\n",
       "   'open',\n",
       "   'pickle.dump',\n",
       "   'output',\n",
       "   'mlflow.get_artifact_uri',\n",
       "   'function',\n",
       "   'train',\n",
       "   'def',\n",
       "   'train_set',\n",
       "   'input',\n",
       "   'dataset',\n",
       "   'test_set',\n",
       "   'saved_model',\n",
       "   'model',\n",
       "   'mlflow_experiment_nam',\n",
       "   'str',\n",
       "   'mlflow_tracking_uri',\n",
       "   'mlflow_s3_endpoint_url',\n",
       "   'model_nam',\n",
       "   'alpha',\n",
       "   'float',\n",
       "   'l1_ratio',\n",
       "   'target',\n",
       "   '``',\n",
       "   'qualiti',\n",
       "   \"''\",\n",
       "   \"'storage_uri\",\n",
       "   \"'run_id\",\n",
       "   'component.',\n",
       "   'numpi',\n",
       "   'np',\n",
       "   'panda',\n",
       "   'pd',\n",
       "   'mlflow.sklearn',\n",
       "   'os',\n",
       "   'level=logging.info',\n",
       "   'logger',\n",
       "   '__name__',\n",
       "   'actual',\n",
       "   'pred',\n",
       "   'rmse',\n",
       "   'mae',\n",
       "   'r2',\n",
       "   'return',\n",
       "   'os.environ',\n",
       "   \"'mlflow_s3_endpoint_url\",\n",
       "   'train_set.path',\n",
       "   'test',\n",
       "   'test_set.path',\n",
       "   'train_x',\n",
       "   'axis=1',\n",
       "   'test_x',\n",
       "   'train_i',\n",
       "   'test_i',\n",
       "   'use',\n",
       "   'track',\n",
       "   'uri',\n",
       "   'experi',\n",
       "   'run',\n",
       "   'run_id',\n",
       "   'run.info.run_id',\n",
       "   'id',\n",
       "   'alpha=alpha',\n",
       "   'l1_ratio=l1_ratio',\n",
       "   'random_state=42',\n",
       "   'fit',\n",
       "   '...',\n",
       "   'predict',\n",
       "   'predicted_qu',\n",
       "   'alpha=',\n",
       "   'l1_ratio=',\n",
       "   'paramet',\n",
       "   'registered_model_name=',\n",
       "   'elasticnetwinemodel',\n",
       "   'serialization_format=',\n",
       "   'artifact',\n",
       "   'predictions.npi',\n",
       "   'local_path=',\n",
       "   'artifact_path=',\n",
       "   'predicted_qualities/',\n",
       "   'save',\n",
       "   'saved_model.path',\n",
       "   \"'wb\",\n",
       "   'fp',\n",
       "   'pickle.highest_protocol',\n",
       "   \"'output\"]},\n",
       " {'id': '06835980-b3f0-5ca5-a9e9-bfcfc08a93ef',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd6',\n",
       "  'type': 'code',\n",
       "  'keywords': ['sklearn.metr',\n",
       "   'import',\n",
       "   'mean_squared_error',\n",
       "   'mean_absolute_error',\n",
       "   'r2_score',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'np.sqrt',\n",
       "   'function',\n",
       "   'eval_metr',\n",
       "   'def',\n",
       "   'actual',\n",
       "   'pred',\n",
       "   'rmse',\n",
       "   'mae',\n",
       "   'r2',\n",
       "   'return']},\n",
       " {'id': '0de746f5-802f-5c06-bbd4-8bd0a0f58367',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc4',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['evalu', 'compon']},\n",
       " {'id': '4c014bf6-5ed9-5e4c-96ed-baf03ab29fc3',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd7',\n",
       "  'type': 'code',\n",
       "  'keywords': ['import',\n",
       "   'log',\n",
       "   'mlflow.track',\n",
       "   'mlflowclient',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'logging.basicconfig',\n",
       "   'logging.getlogg',\n",
       "   'client.get_run',\n",
       "   'logger.info',\n",
       "   'threshold_metrics.item',\n",
       "   'logger.error',\n",
       "   'function',\n",
       "   'evalu',\n",
       "   'def',\n",
       "   'run_id',\n",
       "   'str',\n",
       "   'mlflow_tracking_uri',\n",
       "   'threshold_metr',\n",
       "   'dict',\n",
       "   'bool',\n",
       "   '``',\n",
       "   \"''\",\n",
       "   'compon',\n",
       "   'compar',\n",
       "   'metric',\n",
       "   'train',\n",
       "   'given',\n",
       "   'threshold',\n",
       "   'arg',\n",
       "   'string',\n",
       "   'mlflow',\n",
       "   'run',\n",
       "   'id',\n",
       "   'track',\n",
       "   'uri',\n",
       "   'minimum',\n",
       "   'valu',\n",
       "   'return',\n",
       "   'indic',\n",
       "   'whether',\n",
       "   'pass',\n",
       "   'failed.',\n",
       "   'level=logging.info',\n",
       "   'logger',\n",
       "   '__name__',\n",
       "   'client',\n",
       "   'tracking_uri=mlflow_tracking_uri',\n",
       "   'info',\n",
       "   'training_metr',\n",
       "   'info.data.metr',\n",
       "   'key',\n",
       "   'fail',\n",
       "   'fals',\n",
       "   'true']},\n",
       " {'id': '53a7ce64-48e0-59b5-8989-f61f31277ba3',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc5',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['deploy', 'model', 'compon']},\n",
       " {'id': '463d7d13-7580-5e3d-bbc5-575993bc786e',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd8',\n",
       "  'type': 'code',\n",
       "  'keywords': ['import',\n",
       "   'log',\n",
       "   'kserv',\n",
       "   'util',\n",
       "   'v1beta1inferenceservic',\n",
       "   'kubernet',\n",
       "   'client',\n",
       "   'v1beta1inferenceservicespec',\n",
       "   'v1beta1predictorspec',\n",
       "   'v1beta1sklearnspec',\n",
       "   'kubernetes.cli',\n",
       "   'v1resourcerequir',\n",
       "   'kservecli',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'logging.basicconfig',\n",
       "   'logging.getlogg',\n",
       "   'logger.info',\n",
       "   'utils.get_default_target_namespac',\n",
       "   'client.v1objectmeta',\n",
       "   'kserve.cr',\n",
       "   'function',\n",
       "   'deploy_model',\n",
       "   'def',\n",
       "   'model_nam',\n",
       "   'str',\n",
       "   'storage_uri',\n",
       "   '``',\n",
       "   \"''\",\n",
       "   'deploy',\n",
       "   'model',\n",
       "   'infer',\n",
       "   'servic',\n",
       "   'kserve.',\n",
       "   'constant',\n",
       "   'level=logging.info',\n",
       "   'logger',\n",
       "   '__name__',\n",
       "   'model_uri',\n",
       "   'uri',\n",
       "   'namespac',\n",
       "   \"kserve_version='v1beta1\",\n",
       "   'api_vers',\n",
       "   'constants.kserve_group',\n",
       "   \"'/\",\n",
       "   'kserve_vers',\n",
       "   'isvc',\n",
       "   'kind',\n",
       "   'constants.kserve_kind',\n",
       "   'metadata',\n",
       "   'name',\n",
       "   'annot',\n",
       "   \"'sidecar.istio.io/inject\",\n",
       "   \"'fals\",\n",
       "   'spec',\n",
       "   'predictor=v1beta1predictorspec',\n",
       "   'service_account_name=',\n",
       "   'kserve-sa',\n",
       "   'min_replicas=1',\n",
       "   'max_replica',\n",
       "   'sklearn=v1beta1sklearnspec',\n",
       "   'storage_uri=model_uri',\n",
       "   'resources=v1resourcerequir',\n",
       "   'requests=',\n",
       "   'cpu',\n",
       "   '100m',\n",
       "   'memori',\n",
       "   '512mi',\n",
       "   'limits=',\n",
       "   '300m']},\n",
       " {'id': '6e7f6451-92ce-5d10-a785-6db6881c7ce0',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc6',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['infer', 'compon']},\n",
       " {'id': '9c486be6-7143-5afb-a0cf-2990e8428597',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafd9',\n",
       "  'type': 'code',\n",
       "  'keywords': ['import',\n",
       "   'log',\n",
       "   'request',\n",
       "   'urllib.pars',\n",
       "   'urlsplit',\n",
       "   'kserv',\n",
       "   'util',\n",
       "   'pickl',\n",
       "   'kservecli',\n",
       "   'ice',\n",
       "   'get_istio_auth_sess',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'logging.basicconfig',\n",
       "   'logging.getlogg',\n",
       "   'requests.sess',\n",
       "   's.get',\n",
       "   'runtimeerror',\n",
       "   'len',\n",
       "   're.search',\n",
       "   'redirect_url_obj._replac',\n",
       "   're.sub',\n",
       "   'redirect_url_obj.geturl',\n",
       "   's.post',\n",
       "   \"''\",\n",
       "   '``',\n",
       "   '.join',\n",
       "   'auth_sess',\n",
       "   'session_cooki',\n",
       "   '.replac',\n",
       "   'print',\n",
       "   'utils.get_default_target_namespac',\n",
       "   'logger.info',\n",
       "   'open',\n",
       "   'pickle.load',\n",
       "   'scaler.transform',\n",
       "   'kserve.get',\n",
       "   'input_sample.tolist',\n",
       "   'requests.post',\n",
       "   'response.json',\n",
       "   'function',\n",
       "   'infer',\n",
       "   'def',\n",
       "   'model_nam',\n",
       "   'str',\n",
       "   'scaler_in',\n",
       "   'input',\n",
       "   'artifact',\n",
       "   'test',\n",
       "   'inference.',\n",
       "   'level=logging.info',\n",
       "   'logger',\n",
       "   '__name__',\n",
       "   'url',\n",
       "   'usernam',\n",
       "   'password',\n",
       "   'dict',\n",
       "   'determin',\n",
       "   'specifi',\n",
       "   'secur',\n",
       "   'dex',\n",
       "   'tri',\n",
       "   'obtain',\n",
       "   'session',\n",
       "   'cooki',\n",
       "   'warn',\n",
       "   'staticpassword',\n",
       "   'ldap',\n",
       "   'authent',\n",
       "   'current',\n",
       "   'support',\n",
       "   'default',\n",
       "   'use',\n",
       "   'enabl',\n",
       "   'param',\n",
       "   'kubeflow',\n",
       "   'server',\n",
       "   'includ',\n",
       "   'protocol',\n",
       "   'return',\n",
       "   'auth',\n",
       "   'inform',\n",
       "   'endpoint_url',\n",
       "   'redirect_url',\n",
       "   'none',\n",
       "   'dex_login_url',\n",
       "   'is_secur',\n",
       "   'resp',\n",
       "   'allow_redirects=tru',\n",
       "   'resp.status_cod',\n",
       "   '200',\n",
       "   'rais',\n",
       "   'http',\n",
       "   'statu',\n",
       "   'get',\n",
       "   'resp.url',\n",
       "   'resp.histori',\n",
       "   '==',\n",
       "   'fals',\n",
       "   'els',\n",
       "   'true',\n",
       "   'redirect_url_obj',\n",
       "   '/auth',\n",
       "   'redirect_url_obj.path',\n",
       "   'path=re.sub',\n",
       "   '/auth/loc',\n",
       "   '/auth/',\n",
       "   '/login',\n",
       "   'data=',\n",
       "   'login',\n",
       "   'credenti',\n",
       "   'probabl',\n",
       "   'invalid',\n",
       "   'redirect',\n",
       "   'post',\n",
       "   \"'dex_login_url\",\n",
       "   'c.name',\n",
       "   'c.valu',\n",
       "   's.cooki',\n",
       "   'kubeflow_endpoint',\n",
       "   '//istio-ingressgateway.istio-system.svc.cluster.local:80',\n",
       "   'kubeflow_usernam',\n",
       "   'user',\n",
       "   'example.com',\n",
       "   'kubeflow_password',\n",
       "   '12341234',\n",
       "   'url=kubeflow_endpoint',\n",
       "   'username=kubeflow_usernam',\n",
       "   'password=kubeflow_password',\n",
       "   'token',\n",
       "   'authservice_session=',\n",
       "   'namespac',\n",
       "   'input_sampl',\n",
       "   '5.6',\n",
       "   '0.54',\n",
       "   '0.04',\n",
       "   '1.7',\n",
       "   '0.049',\n",
       "   '13',\n",
       "   '0.9942',\n",
       "   '3.72',\n",
       "   '0.58',\n",
       "   '11.4',\n",
       "   '11.3',\n",
       "   '0.34',\n",
       "   '0.45',\n",
       "   '0.082',\n",
       "   '15',\n",
       "   '0.9988',\n",
       "   '2.94',\n",
       "   '0.66',\n",
       "   '9.2',\n",
       "   'load',\n",
       "   'standard',\n",
       "   'scaler',\n",
       "   'scaler_in.path',\n",
       "   \"'rb\",\n",
       "   'fp',\n",
       "   'sampl',\n",
       "   'namespace=namespac',\n",
       "   'watch=tru',\n",
       "   'timeout_seconds=120',\n",
       "   'inference_servic',\n",
       "   'is_url',\n",
       "   '//istio-ingressgateway.istio-system.svc.cluster.local:80/v1/models/',\n",
       "   'predict',\n",
       "   'header',\n",
       "   'host',\n",
       "   '.example.com',\n",
       "   '\\\\ninfer',\n",
       "   'servic',\n",
       "   '\\\\n',\n",
       "   \"'statu\",\n",
       "   'inference_input',\n",
       "   \"'instanc\",\n",
       "   'respons',\n",
       "   'json=inference_input',\n",
       "   'headers=head',\n",
       "   'cookies=',\n",
       "   'authservice_sess',\n",
       "   'response.status_cod',\n",
       "   '\\\\npredict']},\n",
       " {'id': 'a446041d-f861-5758-b49a-798c61160617',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc7',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['3.', 'pipelin', 'definit']},\n",
       " {'id': 'ba718f02-4840-554c-97d2-1377a9a6b402',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafda',\n",
       "  'type': 'code',\n",
       "  'keywords': ['code',\n",
       "   'depend',\n",
       "   'pull_data',\n",
       "   'preprocess',\n",
       "   'train',\n",
       "   'train_task.appli',\n",
       "   'use_aws_secret',\n",
       "   'evalu',\n",
       "   'dsl.condit',\n",
       "   'deploy_model',\n",
       "   'infer',\n",
       "   'inference_task.aft',\n",
       "   'function',\n",
       "   'pipelin',\n",
       "   'def',\n",
       "   'url',\n",
       "   'str',\n",
       "   'target',\n",
       "   'mlflow_experiment_nam',\n",
       "   'mlflow_tracking_uri',\n",
       "   'mlflow_s3_endpoint_url',\n",
       "   'model_nam',\n",
       "   'alpha',\n",
       "   'float',\n",
       "   'l1_ratio',\n",
       "   'threshold_metr',\n",
       "   'dict',\n",
       "   'pull_task',\n",
       "   'url=url',\n",
       "   'preprocess_task',\n",
       "   'data=pull_task.output',\n",
       "   '``',\n",
       "   'data',\n",
       "   \"''\",\n",
       "   'train_task',\n",
       "   'train_set=preprocess_task.output',\n",
       "   'train_set',\n",
       "   'test_set=preprocess_task.output',\n",
       "   'test_set',\n",
       "   'target=target',\n",
       "   'mlflow_experiment_name=mlflow_experiment_nam',\n",
       "   'mlflow_tracking_uri=mlflow_tracking_uri',\n",
       "   'mlflow_s3_endpoint_url=mlflow_s3_endpoint_url',\n",
       "   'model_name=model_nam',\n",
       "   'alpha=alpha',\n",
       "   'l1_ratio=l1_ratio',\n",
       "   'secret_name=',\n",
       "   'aws-secret',\n",
       "   'evaluate_trask',\n",
       "   'run_id=train_task.output',\n",
       "   'run_id',\n",
       "   'threshold_metrics=threshold_metr',\n",
       "   'eval_pass',\n",
       "   'evaluate_trask.output',\n",
       "   '==',\n",
       "   'true',\n",
       "   'deploy_model_task',\n",
       "   'storage_uri=train_task.output',\n",
       "   'storage_uri',\n",
       "   'inference_task',\n",
       "   'scaler_in=preprocess_task.output',\n",
       "   'scaler_out']},\n",
       " {'id': '6da1816d-e82f-5618-a6f5-4c35a1496c18',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc8',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['pipelin', 'argument']},\n",
       " {'id': '30f9af94-5851-5b57-9548-73382fb7b8cf',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafdb',\n",
       "  'type': 'code',\n",
       "  'keywords': ['global',\n",
       "   'code',\n",
       "   'eval_threshold_metr',\n",
       "   \"'rmse\",\n",
       "   '0.9',\n",
       "   \"'r2\",\n",
       "   '0.3',\n",
       "   \"'mae\",\n",
       "   '0.8',\n",
       "   'argument',\n",
       "   '``',\n",
       "   'url',\n",
       "   \"''\",\n",
       "   'http',\n",
       "   '//archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n",
       "   'target',\n",
       "   'qualiti',\n",
       "   'mlflow_tracking_uri',\n",
       "   '//mlflow.mlflow.svc.cluster.local:5000',\n",
       "   'mlflow_s3_endpoint_url',\n",
       "   '//mlflow-minio-service.mlflow.svc.cluster.local:9000',\n",
       "   'mlflow_experiment_nam',\n",
       "   'demo-notebook',\n",
       "   'model_nam',\n",
       "   'wine-qu',\n",
       "   'alpha',\n",
       "   '0.5',\n",
       "   'l1_ratio',\n",
       "   'threshold_metr']},\n",
       " {'id': 'e8c45edf-9826-5cd9-8b69-66e2eac7d7ed',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafc9',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['4.', 'submit', 'run']},\n",
       " {'id': 'd52d16d2-7cee-5484-8ba4-0d4f8f4f2083',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafdc',\n",
       "  'type': 'code',\n",
       "  'keywords': ['code',\n",
       "   'depend',\n",
       "   'client.create_run_from_pipeline_func',\n",
       "   'global',\n",
       "   'run_nam',\n",
       "   '``',\n",
       "   'demo-run',\n",
       "   \"''\",\n",
       "   'experiment_nam',\n",
       "   'demo-experi',\n",
       "   'pipeline_func=pipelin',\n",
       "   'run_name=run_nam',\n",
       "   'experiment_name=experiment_nam',\n",
       "   'arguments=argu',\n",
       "   'mode=kfp.dsl.pipelineexecutionmode.v2_compat',\n",
       "   'enable_caching=fals',\n",
       "   'namespace=',\n",
       "   'kubeflow-user-example-com']},\n",
       " {'id': 'ee85fd61-e3c2-50fc-a90f-15e9fef05471',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafca',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['5.', 'check', 'run']},\n",
       " {'id': '0ef21fa6-d1f0-5070-a7bd-a98d0141172f',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafcb',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['kubeflow',\n",
       "   'pipelin',\n",
       "   'ui',\n",
       "   'default',\n",
       "   'way',\n",
       "   'access',\n",
       "   'via',\n",
       "   'port-forward',\n",
       "   'enabl',\n",
       "   'get',\n",
       "   'start',\n",
       "   'quickli',\n",
       "   'without',\n",
       "   'impos',\n",
       "   'requir',\n",
       "   'environ',\n",
       "   'run',\n",
       "   'follow',\n",
       "   'istio',\n",
       "   \"'s\",\n",
       "   'ingress-gateway',\n",
       "   'local',\n",
       "   'port',\n",
       "   '8080',\n",
       "   'kubectl',\n",
       "   'svc/istio-ingressgateway',\n",
       "   '-n',\n",
       "   'istio-system',\n",
       "   '8080:80',\n",
       "   'command',\n",
       "   'central',\n",
       "   'dashboard',\n",
       "   'open',\n",
       "   'browser',\n",
       "   'visit',\n",
       "   'http',\n",
       "   '//localhost:8080/',\n",
       "   'dex',\n",
       "   'login',\n",
       "   'screen',\n",
       "   'user',\n",
       "   'credenti',\n",
       "   'email',\n",
       "   'address',\n",
       "   'example.com',\n",
       "   'password',\n",
       "   '12341234']},\n",
       " {'id': '829bf69e-6c0a-512c-9588-9404d5825ca2',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafcc',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['mlflow',\n",
       "   'ui',\n",
       "   'access',\n",
       "   'open',\n",
       "   'termin',\n",
       "   'forward',\n",
       "   'local',\n",
       "   'port',\n",
       "   'server',\n",
       "   'kubectl',\n",
       "   '-n',\n",
       "   'port-forward',\n",
       "   'svc/mlflow',\n",
       "   '5000:5000',\n",
       "   \"'s\",\n",
       "   'reachabl',\n",
       "   'http',\n",
       "   '//localhost:5000']},\n",
       " {'id': 'e9257d72-912f-5ff9-b953-c584c4047f94',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafcd',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['6.',\n",
       "   'check',\n",
       "   'deploy',\n",
       "   'model',\n",
       "   'get',\n",
       "   'infer',\n",
       "   'servic',\n",
       "   'kubectl',\n",
       "   '-n',\n",
       "   'kubeflow-user-example-com',\n",
       "   'inferenceservic',\n",
       "   'pod',\n",
       "   'delet',\n",
       "   'wine-qu',\n",
       "   'someth',\n",
       "   'goe',\n",
       "   'wrong',\n",
       "   'log',\n",
       "   'kserve-contain',\n",
       "   'queue-proxi',\n",
       "   'storage-initi']},\n",
       " {'id': '12c40be4-4c9b-5d04-a173-1249c08df6ef',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document_id': '671b31b2f12238512f2cafce',\n",
       "  'type': 'markdown',\n",
       "  'keywords': ['7.',\n",
       "   'troubleshoot',\n",
       "   'infer',\n",
       "   \"n't\",\n",
       "   'work',\n",
       "   'tri',\n",
       "   'patch',\n",
       "   'knative-serv',\n",
       "   'config-domain',\n",
       "   'kubectl',\n",
       "   'cm',\n",
       "   '--',\n",
       "   '``',\n",
       "   'data',\n",
       "   \"''\",\n",
       "   'example.com',\n",
       "   '-n']}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a57eaf7d-fd2b-4272-bb39-c6feff6f71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = meili_client.index('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4b2d3886-a0aa-4890-8c06-4b988c8d2dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=7, index_uid='test', status='enqueued', type='documentAdditionOrUpdate', enqueued_at=datetime.datetime(2024, 10, 25, 12, 4, 54, 993824))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.add_documents(keyword_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f47b0044-86c0-4b7d-b09a-387213407dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=8, index_uid='test', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 10, 25, 12, 10, 17, 391692))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('test').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e34b6c9-aa7b-46bd-bfcb-4ba5b18e4118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=19, index_uid='llm-rag-workflows-keywords', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 10, 25, 12, 16, 26, 461341))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('llm-rag-workflows-keywords').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fb6d43e4-3f24-438b-8db1-03458d86c8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=20, index_uid='llm-rag-code-keywords', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 10, 25, 12, 16, 28, 891786))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('llm-rag-code-keywords').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e14c6e-d9a8-4a3b-b9f0-1f8b3f729ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
