{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ca3187-954f-4037-b784-f35b05b07a1a",
   "metadata": {},
   "source": [
    "## Document Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab598ed-3313-4cc1-9ded-50aca2abfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_document_data(\n",
    "    document_url: str,\n",
    "    document_type: str\n",
    ") -> any:\n",
    "    data = None\n",
    "    response = requests.get(\n",
    "        url = document_url\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        if document_type == 'text':\n",
    "            data = response.text\n",
    "        if document_type == 'json':\n",
    "            data = json.loads(response.text)\n",
    "        # handle html later\n",
    "    return data\n",
    "\n",
    "def scrape_documents(\n",
    "    url_list: any,\n",
    "    timeout: int\n",
    ") -> any:\n",
    "    documents = []\n",
    "\n",
    "    text_files = [\n",
    "        'py',\n",
    "        'md',\n",
    "        'yaml',\n",
    "        'sh'\n",
    "    ]\n",
    "\n",
    "    json_files = [\n",
    "        'ipynb'\n",
    "    ]\n",
    "    index = 0\n",
    "    for url in url_list:\n",
    "        document = {\n",
    "            'name': '',\n",
    "            'data': ''\n",
    "        }\n",
    "        url_split = url.split('/')\n",
    "        if 'github' in url_split[2]:\n",
    "            if 'raw' in url_split[2]:\n",
    "                file_end = url_split[-1].split('.')[-1]\n",
    "                document['name'] = url_split[-1]\n",
    "                if file_end in text_files:\n",
    "                    document['data'] = get_document_data(\n",
    "                        document_url = url,\n",
    "                        document_type = 'text' \n",
    "                    )\n",
    "                if file_end in json_files:\n",
    "                    document['data'] = get_document_data(\n",
    "                        document_url = url,\n",
    "                        document_type = 'json' \n",
    "                    )\n",
    "        documents.append(document)\n",
    "        index = index + 1\n",
    "        if index < len(url_list):\n",
    "            time.sleep(timeout)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47cbff-32c6-4746-9271-20608d4bb72c",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cf9093-13b9-416c-aac8-5a9b645a14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_urls = [\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/tutorials/demo_notebooks/demo_pipeline/demo-pipeline.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/experiments/article/cloud-hpc/Cloud-HPC-FMNIST-Experiment.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/backend/functions/platforms/celery.py',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/frontend/functions/platforms/redis.py',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/deployment/monitoring/kustomization.yaml',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/deployment/production/stack.yaml'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073a7ca2-91fe-41b3-82d4-55046fb25a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_urls = [\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/tutorials/demo_notebooks/demo_pipeline/demo-pipeline.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/backend/functions/platforms/celery.py',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/deployment/production/stack.yaml'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5e7af2-2223-47e1-9283-0191baeb9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_documents = scrape_documents(\n",
    "    url_list = wanted_urls,\n",
    "    timeout = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf9879-3460-4a9d-b2f5-e768f1203bf6",
   "metadata": {},
   "source": [
    "## Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ab3022-230e-4301-94c5-541c23b395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "def tree_extract_imports(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(tree_extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def tree_extract_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(tree_extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "\n",
    "def tree_extract_code_and_dependencies(\n",
    "    node: any,\n",
    "    code_text: str\n",
    ") -> any:\n",
    "    codes = []\n",
    "    if not node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name')\n",
    "        if name is None:\n",
    "            code = code_text[start_byte:end_byte].decode('utf8')\n",
    "            if not 'def' in code:\n",
    "                dependencies = tree_extract_dependencies(node, code_text)\n",
    "                codes.append({\n",
    "                    'name': 'global',\n",
    "                    'code': code,\n",
    "                    'dependencies': dependencies\n",
    "                })\n",
    "    return codes\n",
    "\n",
    "def tree_extract_functions_and_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(tree_extract_functions_and_dependencies(child, code_text))\n",
    "    return functions\n",
    "\n",
    "def tree_get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def tree_get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions\n",
    "\n",
    "def tree_create_code_document(\n",
    "    code_imports: any,\n",
    "    code_functions: any,\n",
    "    function_item: any\n",
    ") -> any:\n",
    "    used_imports = tree_get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = tree_get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': function_item['name'],\n",
    "        'dependencies': function_item['dependencies'],\n",
    "        'code': function_item['code']\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "     \n",
    "def tree_format_code_document(\n",
    "    code_document: any\n",
    ") -> any:\n",
    "    formatted_document = ''\n",
    "    for doc_import in code_document['imports']:\n",
    "        formatted_document += doc_import + '\\n'\n",
    "\n",
    "    for doc_functions in code_document['functions']:\n",
    "        formatted_document += doc_functions + '\\n'\n",
    "\n",
    "    if 0 < len(code_document['dependencies']):\n",
    "        formatted_document += 'code dependencies\\n'\n",
    "\n",
    "        for doc_dependency in code_document['dependencies']:\n",
    "            formatted_document += doc_dependency + '\\n'\n",
    "\n",
    "    if code_document['name'] == 'global':\n",
    "        formatted_document += code_document['name'] + ' code\\n'\n",
    "    else:\n",
    "        formatted_document += 'function ' + code_document['name'] + ' code\\n'\n",
    "    \n",
    "    for line in code_document['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        doc_code = re.sub(r'#.*','', line)\n",
    "        if not bool(doc_code.strip()):\n",
    "            continue\n",
    "        formatted_document += doc_code + '\\n'    \n",
    "    return formatted_document\n",
    "\n",
    "def tree_create_python_code_and_function_documents(\n",
    "    code_document: any\n",
    "):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "   \n",
    "    tree = parser.parse(\n",
    "        bytes(\n",
    "            code_document,\n",
    "            \"utf8\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    code_imports = tree_extract_imports(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_global = tree_extract_code_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_functions = tree_extract_functions_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    initial_documents = []\n",
    "    for item in code_global:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    for item in code_functions:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    formatted_documents = []\n",
    "    seen_functions = []\n",
    "    for document in initial_documents:\n",
    "        if not document['name'] == 'global':\n",
    "            if document['name'] in seen_functions:\n",
    "                continue\n",
    "        \n",
    "        formatted_document = tree_format_code_document(\n",
    "            code_document = document\n",
    "        )\n",
    "\n",
    "        formatted_documents.append(formatted_document)\n",
    "        seen_functions.append(document['name'])\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1aea2-2782-4c98-8c1b-f6dac8e7762a",
   "metadata": {},
   "source": [
    "## Document Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ae795d-ce80-4fd7-80c7-efc4d1b03a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "\n",
    "def extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_document: any\n",
    "): \n",
    "    notebook_documents = {\n",
    "        'markdown': [],\n",
    "        'code': []\n",
    "    }\n",
    "\n",
    "    notebook = nbformat.from_dict(notebook_document)\n",
    "\n",
    "    index = 1\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            notebook_documents['markdown'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "        if cell.cell_type == 'code':\n",
    "            notebook_documents['code'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "    \n",
    "    return notebook_documents\n",
    "    \n",
    "def parse_markdown_into_text(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    text = soup.get_text()\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    text = re.sub(code_block_pattern, '', text)\n",
    "    text = text.rstrip('\\n')\n",
    "    text = text.replace('\\nsh', '\\n')\n",
    "    text = text.replace('\\nbash', '\\n')\n",
    "    return text\n",
    "\n",
    "def create_python_documents(\n",
    "    python_document: any\n",
    "): \n",
    "    joined_code = ''.join(python_document)\n",
    "    block_code_documents = tree_create_python_code_and_function_documents(\n",
    "        code_document = joined_code\n",
    "    )\n",
    "\n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    code_doc_index = 0\n",
    "    for code_doc in block_code_documents:\n",
    "        row_split = code_doc.split('\\n')\n",
    "        for row in row_split:\n",
    "            if 'function' in row and 'code' in row:\n",
    "                function_name = row.split(' ')[1]\n",
    "                if not function_name in seen_function_names:\n",
    "                    seen_function_names.append(function_name)\n",
    "                else:\n",
    "                    del block_code_documents[code_doc_index]\n",
    "        code_doc_index += 1\n",
    "\n",
    "    if 0 < len(block_code_documents):\n",
    "        index = 1\n",
    "        for code_doc in block_code_documents:\n",
    "            code_documents.append({\n",
    "                'index': index,\n",
    "                'data': code_doc\n",
    "            })\n",
    "            index += 1\n",
    "        \n",
    "    formatted_documents = {\n",
    "        'code': code_documents\n",
    "    }\n",
    "    return formatted_documents\n",
    "\n",
    "def create_notebook_documents(\n",
    "    notebook_document: any\n",
    "):\n",
    "    notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "        notebook_document = notebook_document\n",
    "    )\n",
    "\n",
    "    markdown_documents = []\n",
    "    for block in notebook_documents['markdown']:\n",
    "        joined_text = ''.join(block['data'])\n",
    "        markdown_text = parse_markdown_into_text(\n",
    "            markdown_text = joined_text\n",
    "        )\n",
    "        markdown_documents.append({\n",
    "            'index': block['id'],\n",
    "            'data': markdown_text\n",
    "        })\n",
    "        \n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    for block in notebook_documents['code']:\n",
    "        joined_code = ''.join(block['data'])\n",
    "        block_code_documents = tree_create_python_code_and_function_documents(\n",
    "            code_document = joined_code\n",
    "        )\n",
    "\n",
    "        code_doc_index = 0\n",
    "        for code_doc in block_code_documents:\n",
    "            row_split = code_doc.split('\\n')\n",
    "            for row in row_split:\n",
    "                if 'function' in row and 'code' in row:\n",
    "                    function_name = row.split(' ')[1]\n",
    "                    if not function_name in seen_function_names:\n",
    "                        seen_function_names.append(function_name)\n",
    "                    else:\n",
    "                        del block_code_documents[code_doc_index]\n",
    "            code_doc_index += 1\n",
    "        \n",
    "        if 0 < len(block_code_documents):\n",
    "            sub_indexes = False\n",
    "            if 1 < len(block_code_documents):\n",
    "                sub_indexes = True\n",
    "            index = 1\n",
    "            for code_doc in block_code_documents:\n",
    "                if sub_indexes:\n",
    "                    code_documents.append({\n",
    "                        'sub-index': index, \n",
    "                        'index': block['id'],\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                else:\n",
    "                    code_documents.append({ \n",
    "                        'index': block['id'],\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                index += 1\n",
    "            \n",
    "    formatted_documents = {\n",
    "        'markdown': markdown_documents,\n",
    "        'code': code_documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568c773-068e-473b-9164-4af3bfd496bb",
   "metadata": {},
   "source": [
    "## Mongo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30ab5e19-1bf5-4f7d-a034-11abd3e0a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient as mc\n",
    "\n",
    "def mongo_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return isinstance(storage_client, mc.Connection)\n",
    "\n",
    "def mongo_setup_client(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    address: str,\n",
    "    port: str\n",
    ") -> any:\n",
    "    connection_prefix = 'mongodb://(username):(password)@(address):(port)/'\n",
    "    connection_address = connection_prefix.replace('(username)', username)\n",
    "    connection_address = connection_address.replace('(password)', password)\n",
    "    connection_address = connection_address.replace('(address)', address)\n",
    "    connection_address = connection_address.replace('(port)', port)\n",
    "    mongo_client = mc(\n",
    "        host = connection_address\n",
    "    )\n",
    "    return mongo_client\n",
    "\n",
    "def mongo_get_database(\n",
    "    mongo_client: any,\n",
    "    database_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        return database\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_check_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database_exists = database_name in mongo_client.list_database_names()\n",
    "        return database_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_list_databases(\n",
    "    mongo_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        databases = mongo_client.list_database_names()\n",
    "        return databases\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        mongo_client.drop_database(database_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_get_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collection = database[collection_name]\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def mongo_check_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        collection_exists = collection_name in database.list_collection_names()\n",
    "        return collection_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_update_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_many(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_list_collections(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collections = database.list_collection_names()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try: \n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        database.drop_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_create_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    document: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.insert_one(document)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_get_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    "):\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        document = collection.find_one(filter_query)\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None \n",
    "\n",
    "def mongo_list_documents(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any,\n",
    "    sorting_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        documents = list(collection.find(filter_query).sort(sorting_query))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_update_document(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_one(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_remove_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    ") -> bool:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.delete_one(filter_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b841eef-c6fb-4c98-868f-ecdfaea27127",
   "metadata": {},
   "source": [
    "# Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d59331e4-8afd-43e1-8488-c75830bd44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_scraped_documents(\n",
    "    mongo_client: any,\n",
    "    database_prefix: str,\n",
    "    documents: any\n",
    "):\n",
    "    for document in documents:\n",
    "        file_name = document['name']\n",
    "        file_data = document['data']\n",
    "        \n",
    "        formatted_documents = {}\n",
    "        if '.ipynb' in file_name:\n",
    "            formatted_documents = create_notebook_documents(\n",
    "                notebook_document = file_data\n",
    "            )\n",
    "            document_database_name = database_prefix + '-workflows'\n",
    "        if '.py' in file_name:\n",
    "            formatted_documents = create_python_documents(\n",
    "                python_document = file_data\n",
    "            )\n",
    "            document_database_name = database_prefix + '-code'\n",
    "        \n",
    "        for doc_type, doc_data in formatted_documents.items():\n",
    "            for document in doc_data:\n",
    "                document_data = document['data']\n",
    "                document_index = document['index']\n",
    "                document_sub_index = 0\n",
    "\n",
    "                if 'sub-index' in document:\n",
    "                    document_sub_index = document['sub-index']\n",
    "                \n",
    "                result = mongo_create_document(\n",
    "                    mongo_client = mongo_client,\n",
    "                    database_name = document_database_name,\n",
    "                    collection_name = file_name,\n",
    "                    document = {\n",
    "                        'index': int(document_index),\n",
    "                        'sub-index': int(document_sub_index),\n",
    "                        'type': doc_type,\n",
    "                        'data': document_data\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61adca13-a8bf-4a6b-b38a-2334e0c25271",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = mongo_setup_client(\n",
    "    username = 'mongo123',\n",
    "    password = 'mongo456',\n",
    "    address = '127.0.0.1',\n",
    "    port = '27017'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b2b78e-b21a-431c-ae82-ec933c4bc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_scraped_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    database_prefix = 'llm-rag',\n",
    "    documents = scraped_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24006697-4cac-40b3-9514-73898464b117",
   "metadata": {},
   "source": [
    "## LangChain Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993cccc3-a6fd-4096-a10e-3ccd8196b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def langchain_generate_code_document_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    document_chunks = splitter.create_documents([document])\n",
    "    document_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return document_chunks\n",
    "\n",
    "def langchain_generate_document_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    document_chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = document_chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aea149-4e62-4715-9ad1-03360ea64497",
   "metadata": {},
   "source": [
    "## Qdrant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87982169-a33a-4198-93f6-a1e9ebcb5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient as qc\n",
    "from qdrant_client import models\n",
    "\n",
    "def qdrant_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, qc.Connection)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_setup_client(\n",
    "    api_key: str,\n",
    "    address: str, \n",
    "    port: str\n",
    ") -> any:\n",
    "    try:\n",
    "        qdrant_client = qc(\n",
    "            host = address,\n",
    "            port = int(port),\n",
    "            api_key = api_key,\n",
    "            https = False\n",
    "        ) \n",
    "        return qdrant_client\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_create_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str,\n",
    "    configuration: any\n",
    ") -> any:\n",
    "    try:\n",
    "        result = qdrant_client.create_collection(\n",
    "            collection_name = collection_name,\n",
    "            vectors_config = configuration\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_get_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(\n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_list_collections(\n",
    "    qdrant_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_list = []\n",
    "        for description in collections.collections:\n",
    "            collection_list.append(description.name)\n",
    "        return collection_list\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_upsert_points(\n",
    "    qdrant_client: qc, \n",
    "    collection_name: str,\n",
    "    points: any\n",
    ") -> any:\n",
    "    try:\n",
    "        results = qdrant_client.upsert(\n",
    "            collection_name = collection_name, \n",
    "            points = points\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_search_data(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    scroll_filter: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.scroll(\n",
    "            collection_name = collection_name,\n",
    "            scroll_filter = scroll_filter,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "def qdrant_search_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    query_vector: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.search(\n",
    "            collection_name = collection_name,\n",
    "            query_vector = query_vector,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str, \n",
    "    vectors: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        results = qdrant_client.delete_vectors(\n",
    "            collection_name = collection_name,\n",
    "            vectors = vectors\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing document: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a005608-d938-4cbc-9f63-72796dc7848b",
   "metadata": {},
   "source": [
    "# Getting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acde4f66-6c16-47fa-b070-83ff480251e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_prefix = 'llm-rag'\n",
    "storage_structure = {}\n",
    "database_list = mongo_list_databases(\n",
    "    mongo_client = mongo_client\n",
    ")\n",
    "for database in database_list:\n",
    "    if database_prefix in database:\n",
    "        collection_list = mongo_list_collections(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database\n",
    "        )\n",
    "        storage_structure[database] = collection_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ab797c-1593-4a73-9e62-23549e1cf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "\n",
    "storage_documents = {}\n",
    "for database, collections in storage_structure.items():\n",
    "    if not database in storage_documents:\n",
    "        storage_documents[database] = {}\n",
    "    for collection in collections:\n",
    "        collection_documents = mongo_list_documents(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database,\n",
    "            collection_name = collection,\n",
    "            filter_query = {},\n",
    "            sorting_query = [\n",
    "                ('index', ASCENDING),\n",
    "                ('sub-index', ASCENDING)\n",
    "            ]\n",
    "        )\n",
    "        storage_documents[database][collection] = collection_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106409c-4a12-4168-b991-f9596a837d93",
   "metadata": {},
   "source": [
    "# Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6bfa8f2-38e7-4a9e-9902-43d2d4ff637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_generate_code_document_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    document_chunks = splitter.create_documents([document])\n",
    "    document_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return document_chunks\n",
    "\n",
    "def lanchain_generate_text_document_chunks(\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    document_chunks = splitter.create_documents([document])\n",
    "    document_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return document_chunks\n",
    "\n",
    "def langchain_generate_document_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    document_chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = document_chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9442fc86-e594-4ee0-9152-69fd6c92fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "def generate_document_vector_packet(\n",
    "    document: any,\n",
    "    configuration: any,\n",
    ") -> any:\n",
    "    document_type = document['type']\n",
    "    used_configuration = configuration[document_type]\n",
    "    \n",
    "    document_chunks = []\n",
    "    if document_type == 'code':\n",
    "        document_chunks = langchain_generate_code_document_chunks(\n",
    "            language = Language.PYTHON,\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    if document_type == 'markdown':\n",
    "        document_chunks = lanchain_generate_text_document_chunks(\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "        \n",
    "    vector_embedding = langchain_generate_document_chunk_embeddings(\n",
    "        model_name = used_configuration['model-name'],\n",
    "        document_chunks = document_chunks\n",
    "    )\n",
    "\n",
    "    packet = {\n",
    "        'chunks': document_chunks,\n",
    "        'embeddings': vector_embedding\n",
    "    }\n",
    "    \n",
    "    return packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d066c6-37c8-404c-9f7c-989c74dbf294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "qdrant_client = qdrant_setup_client(\n",
    "    api_key = 'qdrant_key',\n",
    "    address = '127.0.0.1', \n",
    "    port = '6333'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d85edbe6-3c92-4d96-b4a6-62ca8cf920e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Record(id='005732c9-4ce0-5230-bb85-96de0b7b6c1c', payload={'database': 'llm-rag-workflows', 'collection': 'demo-pipeline.ipynb', 'id': '671b31b2f12238512f2cafd1', 'chunk': 'if len(resp.history) == 0:', 'chunk_hash': '3c177a2a09134a89280e93580f24d2d9'}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id='b7416704-cf40-521d-865d-a4ba79fc56d4', payload={'database': 'llm-rag-workflows', 'collection': 'demo-pipeline.ipynb', 'id': '671b31b2f12238512f2cafd1', 'chunk': 'if len(resp.history) == 0:', 'chunk_hash': '3c177a2a09134a89280e93580f24d2d9'}, vector=None, shard_key=None, order_value=None)],\n",
       " None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "qdrant_client.scroll(\n",
    "    collection_name = 'llm-rag-workflows-embeddings',\n",
    "    scroll_filter = models.Filter(\n",
    "        must = [\n",
    "            models.FieldCondition(\n",
    "                key='chunk_hash',\n",
    "                match = models.MatchValue(value=\"3c177a2a09134a89280e93580f24d2d9\")\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    limit = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f28247-4dcb-4fc7-8fcc-1ccbfb2120e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "qdrant_client = qdrant_setup_client(\n",
    "    api_key = 'qdrant_key',\n",
    "    address = '127.0.0.1', \n",
    "    port = '6333'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a3c9c46-b083-4ebc-8450-644c8bb9c833",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_document_vector_packet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m document_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(document[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     49\u001b[0m document_type \u001b[38;5;241m=\u001b[39m document[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m vector_packet \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_document_vector_packet\u001b[49m(\n\u001b[1;32m     52\u001b[0m     document \u001b[38;5;241m=\u001b[39m document,\n\u001b[1;32m     53\u001b[0m     configuration \u001b[38;5;241m=\u001b[39m vector_configuration\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m document_chunks \u001b[38;5;241m=\u001b[39m vector_packet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     57\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m vector_packet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_document_vector_packet' is not defined"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import uuid\n",
    "import re\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "# for preventing duplicates\n",
    "def format_chunk(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    chunk = re.sub(r'[^\\w\\s]', '', document_chunk)\n",
    "    chunk = re.sub(r'\\s+', ' ', chunk) \n",
    "    chunk = chunk.strip()\n",
    "    chunk = chunk.lower()\n",
    "    # This helps to remove unique hashes for duplicates such as:\n",
    "    # task_id = task_id )\n",
    "    # task_id = task_id \n",
    "    # task_id = task_id )\n",
    "    return chunk\n",
    "\n",
    "def generate_chunk_hash(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    cleaned_chunk = format_chunk(\n",
    "        document_chunk = document_chunk\n",
    "    )\n",
    "    return hashlib.md5(cleaned_chunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "vector_configuration = {\n",
    "    'code': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'markdown': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    }\n",
    "}\n",
    "\n",
    "# \n",
    "for database, collections in storage_documents.items():\n",
    "    vector_collection_name = database + '-embeddings'\n",
    "    for collection, documents in collections.items():\n",
    "        for document in documents:\n",
    "            document_id = str(document['_id'])\n",
    "            document_type = document['type']\n",
    "            \n",
    "            vector_packet = generate_document_vector_packet(\n",
    "                document = document,\n",
    "                configuration = vector_configuration\n",
    "            )\n",
    "            \n",
    "            document_chunks = vector_packet['chunks']\n",
    "            document_embeddings = vector_packet['embeddings']\n",
    "            if 0 < len(document_embeddings):\n",
    "                vector_collections = qdrant_list_collections(\n",
    "                    qdrant_client = qdrant_client\n",
    "                )\n",
    "\n",
    "                if not vector_collection_name in vector_collections:\n",
    "                    vector_collection_configuration = VectorParams(\n",
    "                          size = len(document_embeddings[0]), \n",
    "                          distance = Distance.COSINE\n",
    "                    )\n",
    "                    collection_created = qdrant_create_collection(\n",
    "                        qdrant_client = qdrant_client,\n",
    "                        collection_name = vector_collection_name,\n",
    "                        configuration = vector_collection_configuration\n",
    "                    )\n",
    "\n",
    "                vector_points = []\n",
    "                vector_index = 0\n",
    "                added_hashes = []\n",
    "                for chunk in document_chunks:\n",
    "                    vector_id = document_id + '-' + str(vector_index + 1)\n",
    "                    vector_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, vector_id))\n",
    "\n",
    "                    chunk_hash = generate_chunk_hash(\n",
    "                        document_chunk = chunk\n",
    "                    )\n",
    "                   \n",
    "                    existing_chunks = qdrant_search_data(\n",
    "                        qdrant_client = qdrant_client,\n",
    "                        collection_name = vector_collection_name,\n",
    "                        scroll_filter = models.Filter(\n",
    "                            must = [\n",
    "                                models.FieldCondition(\n",
    "                                    key = 'chunk_hash',\n",
    "                                    match = models.MatchValue(\n",
    "                                        value = chunk_hash\n",
    "                                    )\n",
    "                                )\n",
    "                            ]\n",
    "                        ),\n",
    "                        limit = 1\n",
    "                    )\n",
    "                    # Removes duplicates\n",
    "                    if len(existing_chunks[0]) == 0:\n",
    "                        if not chunk_hash in added_hashes:\n",
    "                            given_vector = document_embeddings[vector_index]\n",
    "\n",
    "                            chunk_point = PointStruct(\n",
    "                                id = vector_uuid, \n",
    "                                vector = given_vector,\n",
    "                                payload = {\n",
    "                                    'database': database,\n",
    "                                    'collection': collection,\n",
    "                                    'document': document_id,\n",
    "                                    'type': document_type,\n",
    "                                    'chunk': chunk,\n",
    "                                    'chunk_hash': chunk_hash\n",
    "                                }\n",
    "                            )\n",
    "                            added_hashes.append(chunk_hash)\n",
    "                            vector_points.append(chunk_point)\n",
    "                    vector_index += 1\n",
    "\n",
    "                if 0 < len(vector_points):\n",
    "                    points_stored = qdrant_upsert_points(\n",
    "                        qdrant_client = qdrant_client, \n",
    "                        collection_name = vector_collection_name,\n",
    "                        points = vector_points\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b2cbe-4b74-4317-af57-2e08e030a273",
   "metadata": {},
   "source": [
    "## SpaCy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "971ef653-ef6a-410e-9973-003cee707f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_document_keywords(\n",
    "    document: str\n",
    "):\n",
    "    doc = nlp(document.lower())\n",
    "    \n",
    "    keywords = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop               \n",
    "        and not token.is_punct              \n",
    "        and not token.is_space              \n",
    "        and len(token) > 1                  \n",
    "    ]\n",
    "    \n",
    "    keywords = list(set(keywords))\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f1691-83b5-4ea8-9c94-3ca18a402421",
   "metadata": {},
   "source": [
    "## NLTK Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "703906a6-4dab-4ecb-8ee1-7cc62c060c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_document_keywords(\n",
    "    document: any\n",
    "):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = document.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = list(dict.fromkeys(tokens))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c488e6f-ea6a-48f1-a948-b86e61d88851",
   "metadata": {},
   "source": [
    "## Meili Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2a5e0c5-f27a-4f25-acfe-a8f1b0648bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meilisearch as ms\n",
    "\n",
    "def meili_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, ms.Connection)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "def meili_setup_client(\n",
    "    host: str, \n",
    "    api_key: str\n",
    ") -> any:\n",
    "    try:\n",
    "        meili_client = ms.Client(\n",
    "            url = host, \n",
    "            api_key = api_key\n",
    "        )\n",
    "        return meili_client \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_get_index( \n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_check_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        meili_client.get_index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def meili_remove_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        response = meili_client.index(\n",
    "            index_name = index_name\n",
    "        ).delete()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_list_indexes(\n",
    "    meili_client: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        indexes = meili_client.get_indexes()\n",
    "        return indexes\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_add_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    documents: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.add_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_set_filterable(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    attributes: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_filterable_attributes(attributes)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_search_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    query: any, \n",
    "    options: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.search(\n",
    "            query,\n",
    "            options\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_update_documents(\n",
    "    meili_client, \n",
    "    index_name, \n",
    "    documents\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_delete_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    ids: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.delete_documents(\n",
    "            document_ids = ids\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517757ce-24c3-4a65-937f-840c30f49e7c",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6844a3a-3063-4eee-927f-e4b5d952a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meili_client = meili_setup_client(\n",
    "    host = 'http://127.0.0.1:7700', \n",
    "    api_key = 'meili_key'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8a3d862-c240-45a4-afcb-69cd413a5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "for database, collections in storage_documents.items():\n",
    "    keyword_collection_name = database + '-keywords'\n",
    "    keyword_documents = []\n",
    "    for collection, documents in collections.items():\n",
    "        document_index = 0\n",
    "        for document in documents:\n",
    "            document_id = str(document['_id'])\n",
    "            document_data = document['data']\n",
    "            document_type = document['type']\n",
    "            \n",
    "            document_keywords = get_document_keywords(\n",
    "                document = document_data\n",
    "            )\n",
    "            \n",
    "            keyword_id = document_id + '-' + str(document_index + 1)\n",
    "            keyword_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, keyword_id))\n",
    "\n",
    "            payload = {\n",
    "                'id': keyword_uuid,\n",
    "                'database': database,\n",
    "                'collection': collection,\n",
    "                'document': document_id,\n",
    "                'type': document_type,\n",
    "                'keywords': document_keywords\n",
    "            }\n",
    "\n",
    "            keyword_documents.append(payload)\n",
    "        \n",
    "    stored = meili_add_documents(\n",
    "        meili_client = meili_client,\n",
    "        index_name = keyword_collection_name,\n",
    "        documents = keyword_documents\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f473e44-4433-4e9d-83b8-f08a01819534",
   "metadata": {},
   "source": [
    "## Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bf75035-f9ea-4ba5-9694-5340c3732a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prompt(\n",
    "    prompt: str\n",
    ") -> any:\n",
    "    prompt = prompt.lower()\n",
    "    prompt = re.sub(r'\\s+', ' ', prompt)\n",
    "    prompt = re.sub(r'[^\\w\\s]', '', prompt)\n",
    "    return prompt.strip()\n",
    "\n",
    "def generate_prompt_embedding_query(\n",
    "    model_name: str,\n",
    "    prompt: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    embedding = embedding_model.embed_documents(\n",
    "        texts = [prompt]\n",
    "    )\n",
    "    return embedding[0]\n",
    "\n",
    "def spacy_find_keywords(\n",
    "    text: str\n",
    "):\n",
    "    formatted = nlp(text.lower())\n",
    "    \n",
    "    keywords = [\n",
    "        token.lemma_ for token in formatted\n",
    "        if not token.is_stop               \n",
    "        and not token.is_punct              \n",
    "        and not token.is_space              \n",
    "        and len(token) > 1                  \n",
    "    ]\n",
    "    \n",
    "    keywords = list(set(keywords))\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def generate_prompt_keyword_query(\n",
    "    prompt: any\n",
    ") -> any:\n",
    "    keywords = spacy_find_keywords(\n",
    "        text = prompt\n",
    "    )\n",
    "    keyword_query = ' OR '.join([f'keywords = \"{keyword}\"' for keyword in keywords])\n",
    "    return keyword_query\n",
    "\n",
    "def calculate_keyword_score(\n",
    "    keyword_query: str,\n",
    "    keyword_list: any\n",
    ") -> any:\n",
    "    match = 0\n",
    "    asked_keywords = keyword_query.split('OR')\n",
    "    for asked_keyword in asked_keywords:\n",
    "        formatted = asked_keyword.replace('keywords =', '')\n",
    "        formatted = formatted.replace('\"', '')\n",
    "        formatted = formatted.replace(' ', '')\n",
    "        \n",
    "        if formatted in keyword_list:\n",
    "            match += 1\n",
    "            \n",
    "    query_length = len(asked_keywords)\n",
    "    keyword_length = len(keyword_list)\n",
    "\n",
    "    if match == 0:\n",
    "        return 0.0\n",
    "\n",
    "    normalized = match / ((query_length * keyword_length) ** 0.5)\n",
    "    return normalized\n",
    "    \n",
    "def vector_search_collection(\n",
    "    vector_client: any,\n",
    "    search_client: any,\n",
    "    prompt: str,\n",
    "    top_k: int\n",
    "):\n",
    "\n",
    "    cleaned_prompt = clean_prompt(\n",
    "        prompt = prompt\n",
    "    )\n",
    "\n",
    "    prompt_embedding_query = generate_prompt_embedding_query(\n",
    "        model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        prompt = cleaned_prompt\n",
    "    )\n",
    "\n",
    "    prompt_keyword_query = generate_prompt_keyword_query(\n",
    "        prompt = cleaned_prompt\n",
    "    )\n",
    "\n",
    "    qdrant_collections = [\n",
    "        'llm-rag-code-embeddings',\n",
    "        'llm-rag-workflows-embeddings'\n",
    "    ]\n",
    "    \n",
    "    recommeded_cases = []\n",
    "    for collection in qdrant_collections:\n",
    "        results = qdrant_search_vectors(\n",
    "            qdrant_client = vector_client,  \n",
    "            collection_name = collection,\n",
    "            query_vector = prompt_embedding_query,\n",
    "            limit = top_k\n",
    "        ) \n",
    "        \n",
    "        for result in results:\n",
    "            res_database = result.payload['database']\n",
    "            res_collection = result.payload['collection']\n",
    "            res_document = result.payload['document']\n",
    "            res_type = result.payload['type']\n",
    "            res_score = result.score\n",
    "            \n",
    "            res_case = {\n",
    "                'source': 'vector',\n",
    "                'database': res_database,\n",
    "                'collection': res_collection,\n",
    "                'document': res_document,\n",
    "                'type': res_type,\n",
    "                'score': res_score\n",
    "            }\n",
    "            \n",
    "            recommeded_cases.append(res_case)\n",
    "            \n",
    "    meili_collections = [\n",
    "        'llm-rag-code-keywords',\n",
    "        'llm-rag-workflows-keywords'\n",
    "    ]\n",
    "\n",
    "    recommeded_keyword_cases = []\n",
    "    for index in meili_collections:\n",
    "        results = meili_search_documents(\n",
    "            meili_client = search_client, \n",
    "            index_name = index, \n",
    "            query = \"\", \n",
    "            options = {\n",
    "                'filter': prompt_keyword_query,\n",
    "                'attributesToRetrieve': ['database','collection','document', 'keywords'],\n",
    "                'limit': top_k\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for result in results['hits']:\n",
    "            res_database = result['database']\n",
    "            res_collection = result['collection']\n",
    "            res_document = result['document']\n",
    "            res_keywords = result['keywords']\n",
    "            \n",
    "            res_score = calculate_keyword_score(\n",
    "                keyword_query = prompt_keyword_query,\n",
    "                keyword_list = res_keywords\n",
    "            )\n",
    "\n",
    "            res_case = {\n",
    "                'source': 'search',\n",
    "                'database': res_database,\n",
    "                'collection': res_collection,\n",
    "                'document': res_document,\n",
    "                'type': res_type,\n",
    "                'score': res_score\n",
    "            }\n",
    "\n",
    "            recommeded_cases.append(res_case)\n",
    "\n",
    "    return recommeded_cases\n",
    "\n",
    "def get_top_document_metadata(\n",
    "    collection: str,\n",
    "    alpha: float\n",
    ") -> any:\n",
    "    df = pd.DataFrame(collection)\n",
    "    ids_with_both = df.groupby('document')['source'].nunique()\n",
    "    ids_with_both = ids_with_both[ids_with_both > 1].index\n",
    "    filtered_df = df[df['document'].isin(ids_with_both)]\n",
    "\n",
    "    matched_documents = []\n",
    "    for index_i, row_i in filtered_df[filtered_df['source'] == 'vector'].iterrows():\n",
    "        vector_source = row_i['source']\n",
    "        vector_database = row_i['database']\n",
    "        vector_collection = row_i['collection']\n",
    "        vector_id = row_i['document']\n",
    "        vector_type = row_i['type']\n",
    "        vector_score = row_i['score']\n",
    "        \n",
    "        for index_j, row_j in filtered_df[filtered_df['source'] == 'search'].iterrows():\n",
    "            search_source = row_j['source']\n",
    "            search_database = row_j['database']\n",
    "            search_collection = row_j['collection']\n",
    "            search_id = row_j['document']\n",
    "            search_type = row_j['type']\n",
    "            search_score = row_j['score']\n",
    "            \n",
    "            if vector_database == search_database:\n",
    "                if vector_collection == search_collection:\n",
    "                    if vector_type == search_type:\n",
    "                        if vector_id == search_id:\n",
    "                            hybrid_score = vector_score * alpha + search_score * (1-alpha)\n",
    "    \n",
    "                            matched_documents.append({\n",
    "                                'source': 'hybrid',\n",
    "                                'database': search_database,\n",
    "                                'collection': search_collection,\n",
    "                                'document': search_id,\n",
    "                                'score': hybrid_score\n",
    "                            })\n",
    "    \n",
    "    match_df = pd.DataFrame(matched_documents)\n",
    "    print(match_df)\n",
    "    return match_df.nlargest(1, 'score').values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910d0edd-1b62-4756-b095-ab40c999cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = 'Can you generate a kfp training component for cloud-HPC pipeline?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59ef22ea-6c86-4bf4-b0ba-86994a1e4db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hit_collection = vector_search_collection(\n",
    "    vector_client = qdrant_client,\n",
    "    search_client = meili_client,\n",
    "    prompt = example_prompt,\n",
    "    top_k = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d54fe94-1901-4630-8414-9d5142d1b7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source           database           collection                  document  \\\n",
      "0  hybrid  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
      "1  hybrid  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
      "\n",
      "      score  \n",
      "0  0.329265  \n",
      "1  0.295031  \n"
     ]
    }
   ],
   "source": [
    "top_document_metadata = get_top_document_metadata(\n",
    "    collection = hit_collection,\n",
    "    alpha = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ca91d8d-b327-471e-9317-522520b7a851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hybrid',\n",
       " 'llm-rag-workflows',\n",
       " 'demo-pipeline.ipynb',\n",
       " '671b31b2f12238512f2cafd2',\n",
       " 0.32926466398892446]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_document_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b36e225-8eee-4837-bf75-37f1656de0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('671b31b2f12238512f2cafd2')}\n"
     ]
    }
   ],
   "source": [
    "from bson.objectid import ObjectId\n",
    "case_object_id = ObjectId(top_document_metadata[3])\n",
    "document_query = {'_id': case_object_id}\n",
    "print(document_query)\n",
    "rag_document = mongo_get_document(\n",
    "    mongo_client = mongo_client, \n",
    "    database_name = top_document_metadata[1], \n",
    "    collection_name = top_document_metadata[2], \n",
    "    filter_query = document_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75ca6ade-34d1-43ef-a90c-4339e0072010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('671b31b2f12238512f2cafd2'),\n",
       " 'index': 8,\n",
       " 'sub-index': 0,\n",
       " 'type': 'code',\n",
       " 'data': 'import kfp\\ncode dependencies\\nget_istio_auth_session\\nkfp.Client\\nglobal code\\nimport kfp\\nKUBEFLOW_ENDPOINT = \"http://localhost:8080\"\\nKUBEFLOW_USERNAME = \"user@example.com\"\\nKUBEFLOW_PASSWORD = \"12341234\"\\nauth_session = get_istio_auth_session(\\n    url=KUBEFLOW_ENDPOINT,\\n    username=KUBEFLOW_USERNAME,\\n    password=KUBEFLOW_PASSWORD\\n)\\nclient = kfp.Client(host=f\"{KUBEFLOW_ENDPOINT}/pipeline\", cookies=auth_session[\"session_cookie\"])\\n'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebaf71d-3f5d-4269-a0e1-83c9257b6c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7eaa3e-385e-4f98-b37a-70599b8c5fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfef8d7c-49df-4978-910a-dd7739f1a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(hit_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b99e11e8-8a14-42a5-97b3-13a42a11ed90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>database</th>\n",
       "      <th>collection</th>\n",
       "      <th>document</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-code</td>\n",
       "      <td>celery.py</td>\n",
       "      <td>671b31b2f12238512f2cafe4</td>\n",
       "      <td>code</td>\n",
       "      <td>0.196113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-code</td>\n",
       "      <td>celery.py</td>\n",
       "      <td>671b31b2f12238512f2cafe2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.183395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-code</td>\n",
       "      <td>celery.py</td>\n",
       "      <td>671b31b2f12238512f2cafe5</td>\n",
       "      <td>code</td>\n",
       "      <td>0.181468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-code</td>\n",
       "      <td>celery.py</td>\n",
       "      <td>671b31b2f12238512f2cafe4</td>\n",
       "      <td>code</td>\n",
       "      <td>0.163650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-code</td>\n",
       "      <td>celery.py</td>\n",
       "      <td>671b31b2f12238512f2cafe2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.145550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b1f12238512f2cafbc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.684614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.571491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafcb</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.522507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.503023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafdc</td>\n",
       "      <td>code</td>\n",
       "      <td>0.501056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b1f12238512f2cafbc</td>\n",
       "      <td>code</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd0</td>\n",
       "      <td>code</td>\n",
       "      <td>0.182574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafc0</td>\n",
       "      <td>code</td>\n",
       "      <td>0.255377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafc1</td>\n",
       "      <td>code</td>\n",
       "      <td>0.235702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source           database           collection                  document  \\\n",
       "0   vector       llm-rag-code            celery.py  671b31b2f12238512f2cafe4   \n",
       "1   vector       llm-rag-code            celery.py  671b31b2f12238512f2cafe2   \n",
       "2   vector       llm-rag-code            celery.py  671b31b2f12238512f2cafe5   \n",
       "3   vector       llm-rag-code            celery.py  671b31b2f12238512f2cafe4   \n",
       "4   vector       llm-rag-code            celery.py  671b31b2f12238512f2cafe2   \n",
       "5   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b1f12238512f2cafbc   \n",
       "6   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "7   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafcb   \n",
       "8   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "9   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafdc   \n",
       "10  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b1f12238512f2cafbc   \n",
       "11  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd0   \n",
       "12  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "13  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafc0   \n",
       "14  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafc1   \n",
       "\n",
       "        type     score  \n",
       "0       code  0.196113  \n",
       "1       code  0.183395  \n",
       "2       code  0.181468  \n",
       "3       code  0.163650  \n",
       "4       code  0.145550  \n",
       "5   markdown  0.684614  \n",
       "6       code  0.571491  \n",
       "7   markdown  0.522507  \n",
       "8       code  0.503023  \n",
       "9       code  0.501056  \n",
       "10      code  0.471405  \n",
       "11      code  0.182574  \n",
       "12      code  0.087039  \n",
       "13      code  0.255377  \n",
       "14      code  0.235702  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "299be691-e4a8-4b9a-9e9c-6eff165bf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_with_both = df.groupby('document')['source'].nunique()\n",
    "ids_with_both = ids_with_both[ids_with_both > 1].index\n",
    "filtered_df = df[df['document'].isin(ids_with_both)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e2793ac-af0f-4b26-a53a-0d3d1b94c6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>database</th>\n",
       "      <th>collection</th>\n",
       "      <th>document</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b1f12238512f2cafbc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.684614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.571491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.503023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b1f12238512f2cafbc</td>\n",
       "      <td>code</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source           database           collection                  document  \\\n",
       "5   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b1f12238512f2cafbc   \n",
       "6   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "8   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "10  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b1f12238512f2cafbc   \n",
       "12  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "\n",
       "        type     score  \n",
       "5   markdown  0.684614  \n",
       "6       code  0.571491  \n",
       "8       code  0.503023  \n",
       "10      code  0.471405  \n",
       "12      code  0.087039  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86560887-0537-4557-a740-6e889e50469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32926466398892446\n",
      "0.29503091398892445\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "matched_documents = []\n",
    "for index_i, row_i in filtered_df[filtered_df['source'] == 'vector'].iterrows():\n",
    "    vector_source = row_i['source']\n",
    "    vector_database = row_i['database']\n",
    "    vector_collection = row_i['collection']\n",
    "    vector_id = row_i['document']\n",
    "    vector_type = row_i['type']\n",
    "    vector_score = row_i['score']\n",
    "    \n",
    "    for index_j, row_j in filtered_df[filtered_df['source'] == 'search'].iterrows():\n",
    "        search_source = row_j['source']\n",
    "        search_database = row_j['database']\n",
    "        search_collection = row_j['collection']\n",
    "        search_id = row_j['document']\n",
    "        search_type = row_j['type']\n",
    "        search_score = row_j['score']\n",
    "        \n",
    "        if vector_database == search_database:\n",
    "            if vector_collection == search_collection:\n",
    "                if vector_type == search_type:\n",
    "                    if vector_id == search_id:\n",
    "                        hybrid_score = vector_score * alpha + search_score * (1-alpha)\n",
    "\n",
    "                        matched_documents.append({\n",
    "                            'source': 'hybrid',\n",
    "                            'database': search_database,\n",
    "                            'collection': search_collection,\n",
    "                            'document': search_id,\n",
    "                            'score': hybrid_score\n",
    "                        })\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5772f54-764c-46be-9098-96e2eac58974",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pivot_df \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(pivot_df)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#pivot_df = pivot_df.fillna(0)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#pivot_df['hybrid_score'] = (pivot_df['vector'] * 0.7) + (pivot_df['search'] * 0.3)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#pivot_df[['document','vector']]\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/pandas/core/frame.py:9339\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[0;34m(self, columns, index, values)\u001b[0m\n\u001b[1;32m   9332\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9333\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   9334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\n\u001b[1;32m   9335\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, columns, index\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default, values\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m   9336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9337\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[0;32m-> 9339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:570\u001b[0m, in \u001b[0;36mpivot\u001b[0;34m(data, columns, index, values)\u001b[0m\n\u001b[1;32m    566\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mindexed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns_listlike\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    572\u001b[0m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m    573\u001b[0m ]\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/pandas/core/series.py:4615\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[0;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4571\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[1;32m   4572\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4611\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[1;32m   4612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4613\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[0;32m-> 4615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:517\u001b[0m, in \u001b[0;36munstack\u001b[0;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m--> 517\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[1;32m    521\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:154\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[0;34m(self, index, level, constructor, sort)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[1;32m    147\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    150\u001b[0m         PerformanceWarning,\n\u001b[1;32m    151\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:210\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[0;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "#pivot_df = filtered_df.pivot(index = 'document', columns = 'source', values = 'score')\n",
    "\n",
    "#print(pivot_df)\n",
    "\n",
    "#pivot_df = pivot_df.fillna(0)\n",
    "#pivot_df['hybrid_score'] = (pivot_df['vector'] * 0.7) + (pivot_df['search'] * 0.3)\n",
    "#pivot_df[['document','vector']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85f278ea-3f84-44ed-afa1-ed30869a4b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>database</th>\n",
       "      <th>collection</th>\n",
       "      <th>document</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b1f12238512f2cafbc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>0.684614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.571491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vector</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.503023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b1f12238512f2cafbc</td>\n",
       "      <td>code</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>search</td>\n",
       "      <td>llm-rag-workflows</td>\n",
       "      <td>demo-pipeline.ipynb</td>\n",
       "      <td>671b31b2f12238512f2cafd2</td>\n",
       "      <td>code</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source           database           collection                  document  \\\n",
       "5   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b1f12238512f2cafbc   \n",
       "6   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "8   vector  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "10  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b1f12238512f2cafbc   \n",
       "12  search  llm-rag-workflows  demo-pipeline.ipynb  671b31b2f12238512f2cafd2   \n",
       "\n",
       "        type     score  \n",
       "5   markdown  0.684614  \n",
       "6       code  0.571491  \n",
       "8       code  0.503023  \n",
       "10      code  0.471405  \n",
       "12      code  0.087039  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a01a9141-e30b-48f0-99c1-433d69af7ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'vector',\n",
       "  'database': 'llm-rag-code',\n",
       "  'collection': 'celery.py',\n",
       "  'document': '671b31b2f12238512f2cafe4',\n",
       "  'type': 'code',\n",
       "  'score': 0.19611284},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-code',\n",
       "  'collection': 'celery.py',\n",
       "  'document': '671b31b2f12238512f2cafe2',\n",
       "  'type': 'code',\n",
       "  'score': 0.18339549},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-code',\n",
       "  'collection': 'celery.py',\n",
       "  'document': '671b31b2f12238512f2cafe5',\n",
       "  'type': 'code',\n",
       "  'score': 0.18146801},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-code',\n",
       "  'collection': 'celery.py',\n",
       "  'document': '671b31b2f12238512f2cafe4',\n",
       "  'type': 'code',\n",
       "  'score': 0.16364963},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-code',\n",
       "  'collection': 'celery.py',\n",
       "  'document': '671b31b2f12238512f2cafe2',\n",
       "  'type': 'code',\n",
       "  'score': 0.14554955},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b1f12238512f2cafbc',\n",
       "  'type': 'markdown',\n",
       "  'score': 0.6846139},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafd2',\n",
       "  'type': 'code',\n",
       "  'score': 0.5714905},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafcb',\n",
       "  'type': 'markdown',\n",
       "  'score': 0.5225067},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafd2',\n",
       "  'type': 'code',\n",
       "  'score': 0.503023},\n",
       " {'source': 'vector',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafdc',\n",
       "  'type': 'code',\n",
       "  'score': 0.50105625},\n",
       " {'source': 'search',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b1f12238512f2cafbc',\n",
       "  'type': 'code',\n",
       "  'score': 0.47140452079103173},\n",
       " {'source': 'search',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafd0',\n",
       "  'type': 'code',\n",
       "  'score': 0.18257418583505536},\n",
       " {'source': 'search',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafd2',\n",
       "  'type': 'code',\n",
       "  'score': 0.08703882797784893},\n",
       " {'source': 'search',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafc0',\n",
       "  'type': 'code',\n",
       "  'score': 0.2553769592276246},\n",
       " {'source': 'search',\n",
       "  'database': 'llm-rag-workflows',\n",
       "  'collection': 'demo-pipeline.ipynb',\n",
       "  'document': '671b31b2f12238512f2cafc1',\n",
       "  'type': 'code',\n",
       "  'score': 0.23570226039551587}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e45d5-da34-421d-a38d-aa8a81a39eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_multiple_keywords(keywords):\n",
    "    # Create a filter for multiple keywords using OR\n",
    "    filter_expression = ' OR '.join([f'keywords = \"{keyword}\"' for keyword in keywords])\n",
    "    results = index.search(\"\", {\n",
    "        'filters': filter_expression,\n",
    "        'attributesToRetrieve': ['id', 'title', 'keywords']\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "140e915d-d1fb-4d4a-9bb8-62283d814cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=28, index_uid='llm-rag-workflows-keywords', status='enqueued', type='settingsUpdate', enqueued_at=datetime.datetime(2024, 10, 28, 10, 27, 49, 585572))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_set_filterable(\n",
    "    meili_client = meili_client, \n",
    "    index_name = 'llm-rag-workflows-keywords', \n",
    "    attributes = ['keywords']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67198d-6fc2-4bd8-b982-5326af4cb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    \n",
    "    \n",
    "    # Step 1: Perform Vector Search in Qdrant\n",
    "    qdrant_results = qdrant_client.search(\n",
    "        collection_name=qdrant_index,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Extract Qdrant document IDs and vector scores\n",
    "    qdrant_docs = [\n",
    "        {\n",
    "            \"id\": result.payload[\"id\"],\n",
    "            \"vector_score\": result.score\n",
    "        } for result in qdrant_results\n",
    "    ]\n",
    "    qdrant_ids = [doc[\"id\"] for doc in qdrant_docs]\n",
    "\n",
    "    # Step 2: Perform Keyword Search in MeiliSearch with Filter on Qdrant Results\n",
    "    meili_results = meili_client.index(meili_index).search(\n",
    "        prompt,\n",
    "        {\n",
    "            \"filter\": f\"id IN [{', '.join(map(str, qdrant_ids))}]\",\n",
    "            \"limit\": top_k\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Extract MeiliSearch document IDs and keyword scores\n",
    "    meili_docs = [\n",
    "        {\n",
    "            \"id\": hit[\"id\"],\n",
    "            \"content\": hit,  # Retrieve the full document for final output\n",
    "            \"keyword_score\": hit[\"_rankingScore\"]  # MeiliSearch-specific score field\n",
    "        } for hit in meili_results[\"hits\"]\n",
    "    ]\n",
    "\n",
    "    # Step 3: Merge and Score Results Using Weighted Scoring\n",
    "    # Create a dictionary for combined scoring, merging both Qdrant and MeiliSearch results\n",
    "    combined_results = {}\n",
    "    for doc in qdrant_docs:\n",
    "        combined_results[doc[\"id\"]] = {\n",
    "            \"vector_score\": doc[\"vector_score\"],\n",
    "            \"keyword_score\": 0,  # Will be updated if present in MeiliSearch\n",
    "            \"content\": None\n",
    "        }\n",
    "    for doc in meili_docs:\n",
    "        if doc[\"id\"] in combined_results:\n",
    "            combined_results[doc[\"id\"]][\"keyword_score\"] = doc[\"keyword_score\"]\n",
    "            combined_results[doc[\"id\"]][\"content\"] = doc[\"content\"]\n",
    "\n",
    "    # Calculate the final score using the weighted sum of vector and keyword scores\n",
    "    for doc_id, scores in combined_results.items():\n",
    "        scores[\"combined_score\"] = alpha * scores[\"vector_score\"] + (1 - alpha) * scores[\"keyword_score\"]\n",
    "\n",
    "    # Step 4: Sort by Combined Score and Return Results\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "\n",
    "    # Return the top results with their content\n",
    "    return [doc[\"content\"] for doc in sorted_results if doc[\"content\"]]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a57eaf7d-fd2b-4272-bb39-c6feff6f71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = meili_client.index('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4b2d3886-a0aa-4890-8c06-4b988c8d2dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=7, index_uid='test', status='enqueued', type='documentAdditionOrUpdate', enqueued_at=datetime.datetime(2024, 10, 25, 12, 4, 54, 993824))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.add_documents(keyword_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f47b0044-86c0-4b7d-b09a-387213407dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=8, index_uid='test', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 10, 25, 12, 10, 17, 391692))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('test').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e34b6c9-aa7b-46bd-bfcb-4ba5b18e4118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=23, index_uid='llm-rag-workflows-keywords', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 10, 28, 9, 4, 12, 919528))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('llm-rag-workflows-keywords').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb6d43e4-3f24-438b-8db1-03458d86c8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=24, index_uid='llm-rag-code-keywords', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 10, 28, 9, 4, 15, 88227))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('llm-rag-code-keywords').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e14c6e-d9a8-4a3b-b9f0-1f8b3f729ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
