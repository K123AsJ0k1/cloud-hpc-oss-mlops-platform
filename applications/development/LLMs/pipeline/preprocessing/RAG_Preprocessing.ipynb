{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993ee419-c78f-4913-835a-a99f502f9558",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251cd8d3-b660-4562-8021-1b0d92ae025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Created and works\n",
    "def set_formatted_user(\n",
    "    user: str   \n",
    ") -> any:\n",
    "    return re.sub(r'[^a-z0-9]+', '-', user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf9931-d782-4018-a581-a04d7a042acc",
   "metadata": {},
   "source": [
    "## SWIFT Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dab875-a998-4f3f-8ebb-a2392ce1719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import Config,RepositoryEnv\n",
    "\n",
    "from keystoneauth1 import loading, session\n",
    "from keystoneauth1.identity import v3\n",
    "from keystoneclient.v3 import client as keystone_client\n",
    "\n",
    "import swiftclient as sc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec9c60a-9bb8-4050-a9db-dc486d762254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works\n",
    "def is_swift_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return isinstance(storage_client, sc.Connection)\n",
    "# Works\n",
    "def swift_setup_client(\n",
    "    pre_auth_url: str,\n",
    "    pre_auth_token: str,\n",
    "    user_domain_name: str,\n",
    "    project_domain_name: str,\n",
    "    project_name: str,\n",
    "    auth_version: str\n",
    ") -> any:\n",
    "    swift_client = sc.Connection(\n",
    "        preauthurl = pre_auth_url,\n",
    "        preauthtoken = pre_auth_token,\n",
    "        os_options = {\n",
    "            'user_domain_name': user_domain_name,\n",
    "            'project_domain_name': project_domain_name,\n",
    "            'project_name': project_name\n",
    "        },\n",
    "        auth_version = auth_version\n",
    "    )\n",
    "    return swift_client\n",
    "# Works\n",
    "def swift_create_bucket(\n",
    "    swift_client: any,\n",
    "    bucket_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        swift_client.put_container(\n",
    "            container = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "# Works\n",
    "def swift_check_bucket(\n",
    "    swift_client: any,\n",
    "    bucket_name:str\n",
    ") -> any:\n",
    "    try:\n",
    "        bucket_info = swift_client.get_container(\n",
    "            container = bucket_name\n",
    "        )\n",
    "        bucket_metadata = bucket_info[0]\n",
    "        list_of_objects = bucket_info[1]\n",
    "        return {'metadata': bucket_metadata, 'objects': list_of_objects}\n",
    "    except Exception as e:\n",
    "        return {} \n",
    "# Refactored\n",
    "def swift_delete_bucket(\n",
    "    swift_client: any,\n",
    "    bucket_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        swift_client.delete_container(\n",
    "            container = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "# Created\n",
    "def swift_list_buckets(\n",
    "    swift_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        account_buckets = swift_client.get_account()[1]\n",
    "        buckets = {}\n",
    "        for bucket in account_buckets:\n",
    "            bucket_name = bucket['name']\n",
    "            bucket_count = bucket['count']\n",
    "            bucket_size = bucket['bytes']\n",
    "            buckets[bucket_name] = {\n",
    "                'amount': bucket_count,\n",
    "                'size': bucket_size\n",
    "            }\n",
    "        return buckets\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "# Works\n",
    "def swift_create_object(\n",
    "    swift_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    object_data: any,\n",
    "    object_metadata: any\n",
    ") -> bool: \n",
    "    # This should be updated to handle 5 GB objects\n",
    "    # It also should handle metadata\n",
    "    try:\n",
    "        swift_client.put_object(\n",
    "            container = bucket_name,\n",
    "            obj = object_path,\n",
    "            contents = object_data,\n",
    "            headers = object_metadata\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "# Works\n",
    "def swift_check_object(\n",
    "    swift_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> any: \n",
    "    try:\n",
    "        object_metadata = swift_client.head_object(\n",
    "            container = bucket_name,\n",
    "            obj = object_path\n",
    "        )       \n",
    "        return object_metadata\n",
    "    except Exception as e:\n",
    "        return {} \n",
    "# Refactored\n",
    "def swift_get_object(\n",
    "    swift_client:any,\n",
    "    bucket_name: str,\n",
    "    object_path: str\n",
    ") -> any:\n",
    "    # This should handle metadata\n",
    "    try:\n",
    "        response = swift_client.get_object(\n",
    "            container = bucket_name,\n",
    "            obj = object_path \n",
    "        )\n",
    "        object_info = response[0]\n",
    "        object_data = response[1]\n",
    "        return {'data': object_data, 'info': object_info}\n",
    "    except Exception as e:\n",
    "        return {}     \n",
    "# Refactored   \n",
    "def swift_remove_object(\n",
    "    swift_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    try:\n",
    "        swift_client.delete_object(\n",
    "            container = bucket_name, \n",
    "            obj = object_path\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "# Works\n",
    "def swift_update_object(\n",
    "    swift_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    object_data: any,\n",
    "    object_metadata: any\n",
    ") -> bool:  \n",
    "    remove = swift_remove_object(\n",
    "        swift_client = swift_client, \n",
    "        bucket_name = bucket_name, \n",
    "        object_path = object_path\n",
    "    )\n",
    "    if not remove:\n",
    "        return False\n",
    "    create = swift_create_object(\n",
    "        swift_client = swift_client, \n",
    "        bucket_name = bucket_name, \n",
    "        object_path = object_path, \n",
    "        object_data = object_data,\n",
    "        object_metadata = object_metadata\n",
    "    )\n",
    "    return create\n",
    "# Works\n",
    "def swift_create_or_update_object(\n",
    "    swift_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    object_data: any,\n",
    "    object_metadata: any\n",
    ") -> any:\n",
    "    bucket_info = swift_check_bucket(\n",
    "        swift_client = swift_client, \n",
    "        bucket_name = bucket_name\n",
    "    )\n",
    "    \n",
    "    if len(bucket_info) == 0:\n",
    "        creation_status = swift_create_bucket(\n",
    "            swift_client = swift_client, \n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        if not creation_status:\n",
    "            return False\n",
    "    \n",
    "    object_info = swift_check_object(\n",
    "        swift_client = swift_client, \n",
    "        bucket_name = bucket_name, \n",
    "        object_path = object_path\n",
    "    )\n",
    "    \n",
    "    if len(object_info) == 0:\n",
    "        return swift_create_object(\n",
    "            swift_client = swift_client, \n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            object_data = object_data,\n",
    "            object_metadata = object_metadata\n",
    "        )\n",
    "    else:\n",
    "        return swift_update_object(\n",
    "            swift_client = swift_client, \n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            object_data = object_data,\n",
    "            object_metadata = object_metadata\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd263a-107c-4708-8036-a1455d2a5bed",
   "metadata": {},
   "source": [
    "## Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad284aeb-6ab4-4c02-a402-010870636423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2-2\n",
    "\n",
    "# Refactored and Works\n",
    "def set_encoded_metadata(\n",
    "    used_client: str,\n",
    "    object_metadata: any\n",
    ") -> any:\n",
    "    encoded_metadata = {}\n",
    "    if used_client == 'swift':\n",
    "        key_initial = 'x-object-meta'\n",
    "        for key, value in object_metadata.items():\n",
    "            encoded_key = key_initial + '-' + key\n",
    "            if isinstance(value, list):\n",
    "                encoded_metadata[encoded_key] = 'list=' + ','.join(map(str, value))\n",
    "                continue\n",
    "            encoded_metadata[encoded_key] = str(value)\n",
    "    return encoded_metadata\n",
    "# Refactored and works\n",
    "def get_general_metadata(\n",
    "    used_client: str,\n",
    "    object_metadata: any\n",
    ") -> any:\n",
    "    general_metadata = {}\n",
    "    if used_client == 'swift':\n",
    "        key_initial = 'x-object-meta'\n",
    "        for key, value in object_metadata.items():\n",
    "            if not key_initial == key[:len(key_initial)]:\n",
    "                general_metadata[key] = value\n",
    "    return general_metadata\n",
    "# Refactored and works\n",
    "def get_decoded_metadata(\n",
    "    used_client: str,\n",
    "    object_metadata: any\n",
    ") -> any: \n",
    "    decoded_metadata = {}\n",
    "    if used_client == 'swift':\n",
    "        key_initial = 'x-object-meta'\n",
    "        for key, value in object_metadata.items():\n",
    "            if key_initial == key[:len(key_initial)]:\n",
    "                decoded_key = key[len(key_initial) + 1:]\n",
    "                if 'list=' in value:\n",
    "                    string_integers = value.split('=')[1]\n",
    "                    values = string_integers.split(',')\n",
    "                    if len(values) == 1 and values[0] == '':\n",
    "                        decoded_metadata[decoded_key] = []\n",
    "                    else:\n",
    "                        try:\n",
    "                            decoded_metadata[decoded_key] = list(map(int, values))\n",
    "                        except:\n",
    "                            decoded_metadata[decoded_key] = list(map(str, values))\n",
    "                    continue\n",
    "                if value.isnumeric():\n",
    "                    decoded_metadata[decoded_key] = int(value)\n",
    "                    continue\n",
    "                decoded_metadata[decoded_key] = value\n",
    "    return decoded_metadata\n",
    "# Refactored and works\n",
    "def set_bucket_names(\n",
    "    storage_parameters: any\n",
    ") -> any:\n",
    "    storage_names = []\n",
    "    bucket_prefix = storage_parameters['bucket-prefix']\n",
    "    ice_id = storage_parameters['ice-id']\n",
    "    user = storage_parameters['user']\n",
    "    storage_names.append(bucket_prefix + '-forwarder-' + ice_id)\n",
    "    storage_names.append(bucket_prefix + '-preprocessor-' + ice_id)\n",
    "    storage_names.append(bucket_prefix + '-submitter-' + ice_id + '-' + set_formatted_user(user = user))\n",
    "    storage_names.append(bucket_prefix + '-pipeline-' + ice_id + '-' + set_formatted_user(user = user))\n",
    "    storage_names.append(bucket_prefix + '-experiment-' + ice_id + '-' + set_formatted_user(user = user))\n",
    "    return storage_names\n",
    "# Refactored and works\n",
    "def setup_storage_client(\n",
    "    storage_parameters: any\n",
    ") -> any:\n",
    "    storage_client = None\n",
    "    if storage_parameters['used-client'] == 'swift':\n",
    "        storage_client = swift_setup_client(\n",
    "            pre_auth_url = storage_parameters['pre-auth-url'],\n",
    "            pre_auth_token = storage_parameters['pre-auth-token'],\n",
    "            user_domain_name = storage_parameters['user-domain-name'],\n",
    "            project_domain_name = storage_parameters['project-domain-name'],\n",
    "            project_name = storage_parameters['project-name'],\n",
    "            auth_version = storage_parameters['auth-version']\n",
    "        )\n",
    "    return storage_client\n",
    "# Refactored and works\n",
    "def check_object_metadata(\n",
    "    storage_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> any: \n",
    "    object_metadata = {\n",
    "        'general-meta': {},\n",
    "        'custom-meta': {}\n",
    "    }\n",
    "    if is_swift_client(storage_client = storage_client):\n",
    "        all_metadata = swift_check_object(\n",
    "           swift_client = storage_client,\n",
    "           bucket_name = bucket_name,\n",
    "           object_path = object_path\n",
    "        ) \n",
    "\n",
    "        general_metadata = {}\n",
    "        custom_metadata = {}\n",
    "        if not len(all_metadata) == 0:\n",
    "            general_metadata = get_general_metadata(\n",
    "                used_client = 'swift',\n",
    "                object_metadata = all_metadata\n",
    "            )\n",
    "            custom_metadata = get_decoded_metadata(\n",
    "                used_client = 'swift',\n",
    "                object_metadata = all_metadata\n",
    "            )\n",
    "\n",
    "        object_metadata['general-meta'] = general_metadata\n",
    "        object_metadata['custom-meta'] = custom_metadata\n",
    "\n",
    "    return object_metadata\n",
    "# Refactored and works\n",
    "def get_object_content(\n",
    "    storage_client: any,\n",
    "    bucket_name: str,\n",
    "    object_path: str\n",
    ") -> any:\n",
    "    object_content = {}\n",
    "    if is_swift_client(storage_client = storage_client):\n",
    "        fetched_object = swift_get_object(\n",
    "            swift_client = storage_client,\n",
    "            bucket_name = bucket_name,\n",
    "            object_path = object_path\n",
    "        )\n",
    "        object_content['data'] = pickle.loads(fetched_object['data'])\n",
    "        object_content['general-meta'] = get_general_metadata(\n",
    "            used_client = 'swift',\n",
    "            object_metadata = fetched_object['info']\n",
    "        )\n",
    "        object_content['custom-meta'] = get_decoded_metadata(\n",
    "            used_client = 'swift',\n",
    "            object_metadata = fetched_object['info']\n",
    "        )\n",
    "    return object_content\n",
    "# Refactored    \n",
    "def remove_object(\n",
    "    storage_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    removed = False\n",
    "    if is_swift_client(storage_client = storage_client):\n",
    "        removed = swift_remove_object(\n",
    "            swift_client = storage_client,\n",
    "            bucket_name = bucket_name,\n",
    "            object_path = object_path\n",
    "        )\n",
    "    return removed\n",
    "# Refactored and works\n",
    "def create_or_update_object(\n",
    "    storage_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    object_data: any,\n",
    "    object_metadata: any\n",
    ") -> any:\n",
    "    success = False\n",
    "    if is_swift_client(storage_client = storage_client):\n",
    "        formatted_data = pickle.dumps(object_data)\n",
    "        formatted_metadata = set_encoded_metadata(\n",
    "            used_client = 'swift',\n",
    "            object_metadata = object_metadata\n",
    "        )\n",
    "\n",
    "        success = swift_create_or_update_object(\n",
    "            swift_client = storage_client,\n",
    "            bucket_name = bucket_name,\n",
    "            object_path = object_path,\n",
    "            object_data = formatted_data,\n",
    "            object_metadata = formatted_metadata\n",
    "        )\n",
    "    return success\n",
    "# Created and works\n",
    "def format_bucket_metadata(\n",
    "    used_client: str,\n",
    "    bucket_metadata: any\n",
    ") -> any:\n",
    "    formatted_metadata = {}\n",
    "    if used_client == 'swift':\n",
    "        relevant_values = {\n",
    "            'x-container-object-count': 'object-count',\n",
    "            'x-container-bytes-used-actual': 'used-bytes',\n",
    "            'last-modified': 'date',\n",
    "            'content-type': 'type'\n",
    "        }\n",
    "        formatted_metadata = {}\n",
    "        for key,value in bucket_metadata.items():\n",
    "            if key in relevant_values:\n",
    "                formatted_key = relevant_values[key]\n",
    "                formatted_metadata[formatted_key] = value\n",
    "    return formatted_metadata\n",
    "# Created and works\n",
    "def format_bucket_objects(\n",
    "    used_client: str,\n",
    "    bucket_objects: any\n",
    ") -> any:\n",
    "    formatted_objects = {}\n",
    "    if used_client == 'swift':\n",
    "        for bucket_object in bucket_objects:\n",
    "            formatted_object_metadata = {\n",
    "                'hash': 'id',\n",
    "                'bytes': 'used-bytes',\n",
    "                'last_modified': 'date'\n",
    "            }\n",
    "            object_key = None\n",
    "            object_metadata = {}\n",
    "            for key, value in bucket_object.items():\n",
    "                if key == 'name':\n",
    "                    object_key = value\n",
    "                if key in formatted_object_metadata:\n",
    "                    formatted_key = formatted_object_metadata[key]\n",
    "                    object_metadata[formatted_key] = value\n",
    "            formatted_objects[object_key] = object_metadata\n",
    "    return formatted_objects\n",
    "# Created and works\n",
    "def format_bucket_info(\n",
    "    used_client: str,\n",
    "    bucket_info: any\n",
    ") -> any:\n",
    "    bucket_metadata = {}\n",
    "    bucket_objects = {}\n",
    "    if used_client == 'swift':\n",
    "        bucket_metadata = format_bucket_metadata(\n",
    "            used_client = used_client,\n",
    "            bucket_metadata = bucket_info['metadata']\n",
    "        )\n",
    "        bucket_objects = format_bucket_objects(\n",
    "            used_client = used_client,\n",
    "            bucket_objects = bucket_info['objects']\n",
    "        )\n",
    "    return {'metadata': bucket_metadata, 'objects': bucket_objects} \n",
    "# Created and works\n",
    "def get_bucket_info(\n",
    "    storage_client: any,\n",
    "    bucket_name: str\n",
    ") -> any:\n",
    "    bucket_info = {}\n",
    "    if is_swift_client(storage_client = storage_client):\n",
    "        unformatted_bucket_info = swift_check_bucket(\n",
    "            swift_client = storage_client,\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        bucket_info = format_bucket_info(\n",
    "            used_client = 'swift',\n",
    "            bucket_info = unformatted_bucket_info\n",
    "        )\n",
    "    return bucket_info\n",
    "# Created and works\n",
    "def format_container_info(\n",
    "    used_client: str,\n",
    "    container_info: any\n",
    ") -> any:\n",
    "    formatted_container_info = {}\n",
    "    if used_client == 'swift':\n",
    "        for bucket in container_info:\n",
    "            bucket_name = bucket['name']\n",
    "            bucket_count = bucket['count']\n",
    "            bucket_size = bucket['bytes']\n",
    "            formatted_container_info[bucket_name] = {\n",
    "                'amount': bucket_count,\n",
    "                'size': bucket_size\n",
    "            }\n",
    "    return formatted_container_info\n",
    "# Created and works\n",
    "def get_container_info( \n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    container_info = {}\n",
    "    if is_swift_client(storage_client = storage_client):\n",
    "        unformatted_container_info = swift_list_buckets(\n",
    "            swift_client = storage_client \n",
    "        )\n",
    "        container_info = format_container_info(\n",
    "            used_client = 'swift',\n",
    "            container_info = unformatted_container_info\n",
    "        )\n",
    "    return container_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c0470-eb29-436c-ad3c-76c9a9f35661",
   "metadata": {},
   "source": [
    "## Object Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333662ed-317e-4b72-aa6b-5539e8041ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2-3\n",
    "\n",
    "# Created and works\n",
    "def set_object_path(\n",
    "    object_name: str,\n",
    "    path_replacers: any,\n",
    "    path_names: any\n",
    "):\n",
    "    object_paths = {\n",
    "        'root': 'name',\n",
    "        'code': 'CODE/name',\n",
    "        'slurm': 'CODE/SLURM/name',\n",
    "        'ray': 'CODE/RAY/name',\n",
    "        'data': 'DATA/name',\n",
    "        'artifacts': 'ARTIFACTS/name',\n",
    "        'time': 'TIMES/name'\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "    path_split = object_paths[object_name].split('/')\n",
    "    for name in path_split:\n",
    "        if name in path_replacers:\n",
    "            replacer = path_replacers[name]\n",
    "            if 0 < len(replacer):\n",
    "                path_split[i] = replacer\n",
    "        i = i + 1\n",
    "    \n",
    "    if not len(path_names) == 0:\n",
    "        path_split.extend(path_names)\n",
    "\n",
    "    object_path = '/'.join(path_split)\n",
    "    print('Used object path:' + str(object_path))\n",
    "    return object_path\n",
    "# created and works\n",
    "def setup_storage(\n",
    "    storage_parameters: any\n",
    ") -> any:\n",
    "    storage_client = setup_storage_client(\n",
    "        storage_parameters = storage_parameters\n",
    "    ) \n",
    "    \n",
    "    storage_name = set_bucket_names(\n",
    "       storage_parameters = storage_parameters\n",
    "    )\n",
    "    \n",
    "    return storage_client, storage_name\n",
    "# Created and works\n",
    "def check_object(\n",
    "    storage_client: any,\n",
    "    bucket_name: str,\n",
    "    object_name: str,\n",
    "    path_replacers: any,\n",
    "    path_names: any\n",
    ") -> bool:\n",
    "    object_path = set_object_path(\n",
    "        object_name = object_name,\n",
    "        path_replacers = path_replacers,\n",
    "        path_names = path_names\n",
    "    )\n",
    "    # Consider making these functions object storage agnostic\n",
    "    object_metadata = check_object_metadata(\n",
    "        storage_client = storage_client,\n",
    "        bucket_name = bucket_name,\n",
    "        object_path = object_path\n",
    "    )\n",
    "    object_metadata['path'] = object_path\n",
    "    return object_metadata\n",
    "# Created and works\n",
    "def get_object(\n",
    "    storage_client: any,\n",
    "    bucket_name: str,\n",
    "    object_name: str,\n",
    "    path_replacers: any,\n",
    "    path_names: any\n",
    ") -> any:\n",
    "    checked_object = check_object(\n",
    "        storage_client = storage_client,\n",
    "        bucket_name = bucket_name,\n",
    "        object_name = object_name,\n",
    "        path_replacers = path_replacers,\n",
    "        path_names = path_names\n",
    "    )\n",
    "\n",
    "    object_data = None\n",
    "    if not len(checked_object['general-meta']) == 0:\n",
    "        # Consider making these functions object storage agnostic\n",
    "        object_data = get_object_content(\n",
    "            storage_client = storage_client,\n",
    "            bucket_name = bucket_name,\n",
    "            object_path = checked_object['path']\n",
    "        )\n",
    "\n",
    "    return object_data\n",
    "# Created and Works\n",
    "def set_object(\n",
    "    storage_client: any,\n",
    "    bucket_name: str,\n",
    "    object_name: str,\n",
    "    path_replacers: any,\n",
    "    path_names: any,\n",
    "    overwrite: bool,\n",
    "    object_data: any,\n",
    "    object_metadata: any\n",
    "):\n",
    "    checked_object = check_object(\n",
    "        storage_client = storage_client,\n",
    "        bucket_name = bucket_name,\n",
    "        object_name = object_name,\n",
    "        path_replacers = path_replacers,\n",
    "        path_names = path_names\n",
    "    )\n",
    "    \n",
    "    perform = True\n",
    "    if not len(checked_object['general-meta']) == 0 and not overwrite:\n",
    "        perform = False\n",
    "    \n",
    "    if perform:\n",
    "        create_or_update_object(\n",
    "            storage_client = storage_client,\n",
    "            bucket_name = bucket_name,\n",
    "            object_path = checked_object['path'],\n",
    "            object_data = object_data,\n",
    "            object_metadata = object_metadata\n",
    "        )\n",
    "# Created and works\n",
    "def check_bucket(\n",
    "    storage_client: any,\n",
    "    bucket_name: str\n",
    ") -> any:\n",
    "    return get_bucket_info(\n",
    "        storage_client = storage_client,\n",
    "        bucket_name = bucket_name\n",
    "    )\n",
    "# Created and works\n",
    "def check_buckets(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return get_container_info( \n",
    "        storage_client = storage_client\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7f925-2329-4c12-aa5c-a91fc6e746ca",
   "metadata": {},
   "source": [
    "## Metadata Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62a9605-233f-459b-b3c8-7b814e121a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created and works\n",
    "def general_object_metadata():\n",
    "    general_object_metadata = {\n",
    "        'version': 1\n",
    "    }\n",
    "    return general_object_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27b38b-9671-4e57-b8fa-23f55022e9e1",
   "metadata": {},
   "source": [
    "## Access Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f53cdae-f0fe-403f-a1b9-c186dadfb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storage_parameters(\n",
    "    env_path: str,\n",
    "    auth_url: str,\n",
    "    pre_auth_url: str,\n",
    "    auth_version: str,\n",
    "    bucket_prefix: str,\n",
    "    ice_id: str,\n",
    "    user: str\n",
    "):\n",
    "    env_config = Config(RepositoryEnv(env_path))\n",
    "    swift_auth_url = auth_url\n",
    "    swift_user = env_config.get('CSC_USERNAME')\n",
    "    swift_key = env_config.get('CSC_PASSWORD')\n",
    "    swift_project_name = env_config.get('CSC_PROJECT_NAME')\n",
    "    swift_user_domain_name = env_config.get('CSC_USER_DOMAIN_NAME')\n",
    "    swift_project_domain_name = env_config.get('CSC_USER_DOMAIN_NAME')\n",
    "\n",
    "    loader = loading.get_plugin_loader('password')\n",
    "    auth = loader.load_from_options(\n",
    "        auth_url = swift_auth_url,\n",
    "        username = swift_user,\n",
    "        password = swift_key,\n",
    "        project_name = swift_project_name,\n",
    "        user_domain_name = swift_user_domain_name,\n",
    "        project_domain_name = swift_project_domain_name\n",
    "    )\n",
    "\n",
    "    keystone_session = session.Session(\n",
    "        auth = auth\n",
    "    )\n",
    "    swift_token = keystone_session.get_token()\n",
    "\n",
    "    swift_pre_auth_url = pre_auth_url\n",
    "    swift_auth_version = auth_version\n",
    "\n",
    "    storage_parameters = {\n",
    "        'bucket-prefix': bucket_prefix,\n",
    "        'ice-id': ice_id,\n",
    "        'user': user,\n",
    "        'used-client': 'swift',\n",
    "        'pre-auth-url': str(swift_pre_auth_url),\n",
    "        'pre-auth-token': str(swift_token),\n",
    "        'user-domain-name': str(swift_user_domain_name),\n",
    "        'project-domain-name': str(swift_project_domain_name),\n",
    "        'project-name': str(swift_project_name),\n",
    "        'auth-version': str(swift_auth_version)\n",
    "    }\n",
    "\n",
    "    return storage_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823600f4-af67-42e9-9d26-a5b110d014bb",
   "metadata": {},
   "source": [
    "## Gaining Storage Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90dc85c-08ff-4e1d-994e-037d60afdc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_absolute_path = '/home/sfniila/.ssh/.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f480d29-3a8a-46e4-a8ba-66212a58477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_parameters = get_storage_parameters(\n",
    "    env_path = env_absolute_path,\n",
    "    auth_url = 'https://pouta.csc.fi:5001/v3',\n",
    "    pre_auth_url = 'https://a3s.fi:443/swift/v1/AUTH_6698ff90e6704a74930c33d6b66f1b5b',\n",
    "    auth_version = '3',\n",
    "    bucket_prefix = 'integration',\n",
    "    ice_id = 's0-c0-u1',\n",
    "    user = 'user@example.com'\n",
    ")\n",
    "\n",
    "storage_client, storage_names = setup_storage(\n",
    "    storage_parameters = storage_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea24f27-389d-474c-b047-66460b84346d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['integration-forwarder-s0-c0-u1',\n",
       " 'integration-preprocessor-s0-c0-u1',\n",
       " 'integration-submitter-s0-c0-u1-user-example-com',\n",
       " 'integration-pipeline-s0-c0-u1-user-example-com',\n",
       " 'integration-experiment-s0-c0-u1-user-example-com']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6ce27-a931-43ea-a045-d3c6ba7e32fc",
   "metadata": {},
   "source": [
    "## Tree Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fdeacaf8-1ab3-4eb5-afad-81c331477943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "def tree_extract_imports(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(tree_extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def tree_extract_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(tree_extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "\n",
    "def tree_extract_code_and_dependencies(\n",
    "    node: any,\n",
    "    code_text: str\n",
    ") -> any:\n",
    "    code = {}\n",
    "    if not node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        code = {\n",
    "            'name': 'global',\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        }\n",
    "    return code\n",
    "\n",
    "def tree_extract_functions_and_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(tree_extract_functions_and_dependencies(child, code_text))\n",
    "    return functions\n",
    "\n",
    "def tree_get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def tree_get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions\n",
    "\n",
    "def tree_create_code_document(\n",
    "    code_imports: any,\n",
    "    code_functions: any,\n",
    "    function_item: any\n",
    ") -> any:\n",
    "    used_imports = tree_get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = tree_get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': function_item['name'],\n",
    "        'dependencies': function_item['dependencies'],\n",
    "        'code': function_item['code']\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "     \n",
    "def tree_format_code_document(\n",
    "    code_document: any\n",
    ") -> any:\n",
    "    formatted_document = ''\n",
    "    for doc_import in code_document['imports']:\n",
    "        formatted_document += doc_import + '\\n'\n",
    "\n",
    "    for doc_functions in code_document['functions']:\n",
    "        formatted_document += doc_functions + '\\n'\n",
    "\n",
    "    formatted_document += 'code dependencies\\n'\n",
    "\n",
    "    for doc_dependency in code_document['dependencies']:\n",
    "        formatted_document += doc_dependency + '\\n'\n",
    "    \n",
    "    for line in code_document['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        doc_code = re.sub(r'#.*','', line)\n",
    "        if not bool(doc_code.strip()):\n",
    "            continue\n",
    "        formatted_document += doc_code + '\\n'    \n",
    "    return formatted_document\n",
    "\n",
    "def tree_create_python_code_and_function_documents(\n",
    "    code_document: any\n",
    "):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "   \n",
    "    tree = parser.parse(\n",
    "        bytes(\n",
    "            code_document,\n",
    "            \"utf8\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    code_imports = tree_extract_imports(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_global = tree_extract_code_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #print(code_global)\n",
    "\n",
    "    code_functions = tree_extract_functions_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    initial_documents = []\n",
    "\n",
    "    for item in code_global:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    for item in code_functions:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    formatted_documents = []\n",
    "    for document in initial_documents:\n",
    "        formatted_document = tree_format_code_document(\n",
    "            code_document = document\n",
    "        )\n",
    "        formatted_documents.append(formatted_document)\n",
    "    return formatted_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bc176-8393-4444-ba17-bc8c75d512e0",
   "metadata": {},
   "source": [
    "## Mongo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683a6bfa-d688-4769-b9e3-cbb28fca3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient as mc\n",
    "\n",
    "def mongo_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return isinstance(storage_client, mc.Connection)\n",
    "\n",
    "def mongo_setup_client(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    address: str,\n",
    "    port: str\n",
    ") -> any:\n",
    "    connection_prefix = 'mongodb://(username):(password)@(address):(port)/'\n",
    "    connection_address = connection_prefix.replace('(username)', username)\n",
    "    connection_address = connection_address.replace('(password)', password)\n",
    "    connection_address = connection_address.replace('(address)', address)\n",
    "    connection_address = connection_address.replace('(port)', port)\n",
    "    mongo_client = mc(\n",
    "        host = connection_address\n",
    "    )\n",
    "    return mongo_client\n",
    "\n",
    "def mongo_get_database(\n",
    "    mongo_client: any,\n",
    "    database_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        return database\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_check_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database_exists = database_name in mongo_client.list_database_names()\n",
    "        return database_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_list_databases(\n",
    "    mongo_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        databases = mongo_client.list_database_names()\n",
    "        return databases\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        mongo_client.drop_database(database_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_get_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collection = database[collection_name]\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def mongo_check_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        collection_exists = collection_name in database.list_collection_names()\n",
    "        return collection_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_update_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_many(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_list_collections(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collections = database.list_collection_names()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try: \n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        database.drop_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_create_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    document: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.insert_one(document)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_get_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    "):\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        document = collection.find_one(filter_query)\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        return None \n",
    "    \n",
    "def mongo_list_documents(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        documents = list(collection.find(filter_query))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_update_document(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_one(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_remove_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    ") -> bool:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.delete_one(filter_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d977eb7-1e0c-43c5-9ef7-daee436bd10f",
   "metadata": {},
   "source": [
    "## Qdrant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5e0869-7b02-4eee-8bf4-78ee088ec216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient as qc\n",
    "\n",
    "def qdrant_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, qc.Connection)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_setup_client(\n",
    "    api_key: str,\n",
    "    address: str, \n",
    "    port: str\n",
    ") -> any:\n",
    "    try:\n",
    "        qdrant_client = qc(\n",
    "            host = address,\n",
    "            port = int(port),\n",
    "            api_key = api_key,\n",
    "            https = False\n",
    "        ) \n",
    "        return qdrant_client\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_create_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str,\n",
    "    configuration: any\n",
    ") -> any:\n",
    "    try:\n",
    "        result = qdrant_client.create_collection(\n",
    "            collection_name = collection_name,\n",
    "            vectors_config = configuration\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_get_collections(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(\n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_list_collections(\n",
    "    qdrant_client: any, \n",
    "    database_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_upsert_points(\n",
    "    qdrant_client: qc, \n",
    "    collection_name: str,\n",
    "    points: any\n",
    ") -> any:\n",
    "    try:\n",
    "        results = qdrant_client.upsert(\n",
    "            collection_name = collection_name, \n",
    "            points = points\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_search_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    query_vector: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.search(\n",
    "            collection_name = collection_name,\n",
    "            query_vector = query_vector,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str, \n",
    "    vectors: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        results = qdrant_client.delete_vectors(\n",
    "            collection_name = collection_name,\n",
    "            vectors = vectors\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing document: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f1161-cc2d-4177-94bf-4011f79cda21",
   "metadata": {},
   "source": [
    "## Meili Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6cd8dc-6015-401a-81d0-4a71b92d2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meilisearch as ms\n",
    "\n",
    "def meili_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, ms.Connection)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "def meili_setup_client(\n",
    "    host: str, \n",
    "    api_key: str\n",
    ") -> any:\n",
    "    try:\n",
    "        meili_client = ms.Client(\n",
    "            url = host, \n",
    "            api_key = api_key\n",
    "        )\n",
    "        return meili_client \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_get_index( \n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_check_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        meili_client.get_index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def meili_remove_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        response = meili_client.index(\n",
    "            index_name = index_name\n",
    "        ).delete()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_list_indexes(\n",
    "    meili_client: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        indexes = meili_client.get_indexes()\n",
    "        return indexes\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_add_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    documents: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.add_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_search_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    query: any, \n",
    "    options: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.search(\n",
    "            query = query, \n",
    "            options = options\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_update_documents(\n",
    "    meili_client, \n",
    "    index_name, \n",
    "    documents\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_delete_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    ids: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.delete_documents(\n",
    "            document_ids = ids\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed4f06-6893-401a-9b55-45f600743542",
   "metadata": {},
   "source": [
    "## Langchain functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e53f09-8abd-423d-847e-001e276630b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def langchain_generate_code_document_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    document_chunks = splitter.create_documents([document])\n",
    "    document_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return document_chunks\n",
    "\n",
    "def langchain_generate_document_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    document_chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = document_chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd5dd6-6c30-4b19-a53c-15622807c7fe",
   "metadata": {},
   "source": [
    "## NLTK Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5749cc-9d93-4f19-ab57-6d31f82a017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_code_document_keywords(\n",
    "    code_document: any\n",
    "):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    code_text = code_document.lower()\n",
    "    tokens = word_tokenize(code_text)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = list(dict.fromkeys(tokens))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f2958-ce1d-4ce3-8731-3f6a5b01a36b",
   "metadata": {},
   "source": [
    "## Document Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b1f5032-42bf-4516-9dcd-f24282e7e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "import nbformat\n",
    "import requests\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_document(\n",
    "    document_url: str,\n",
    "    document_type: str\n",
    ") -> any:\n",
    "    document = None\n",
    "    response = requests.get(\n",
    "        url = document_url\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        if document_type == 'text':\n",
    "            document = response.text\n",
    "        if document_type == 'json':\n",
    "            document = json.loads(response.text)\n",
    "        # handle html later\n",
    "    return document\n",
    "\n",
    "def scrape_documents(\n",
    "    url_list: any,\n",
    "    timeout: int\n",
    ") -> any:\n",
    "    documents = []\n",
    "\n",
    "    text_files = [\n",
    "        'py',\n",
    "        'md',\n",
    "        'yaml',\n",
    "        'sh'\n",
    "    ]\n",
    "\n",
    "    json_files = [\n",
    "        'ipynb'\n",
    "    ]\n",
    "    index = 0\n",
    "    for url in url_list:\n",
    "        document = None\n",
    "        url_split = url.split('/')\n",
    "        if 'github' in url_split[2]:\n",
    "            if 'raw' in url_split[2]:\n",
    "                file_end = url_split[-1].split('.')[-1]\n",
    "                if file_end in text_files:\n",
    "                    document = get_document(\n",
    "                        document_url = url,\n",
    "                        document_type = 'text' \n",
    "                    )\n",
    "                if file_end in json_files:\n",
    "                    document = get_document(\n",
    "                        document_url = url,\n",
    "                        document_type = 'json' \n",
    "                    )\n",
    "        documents.append(document)\n",
    "        index = index + 1\n",
    "        if index < len(url_list):\n",
    "            time.sleep(timeout)\n",
    "    return documents\n",
    "\n",
    "def extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_document: any\n",
    "): \n",
    "    notebook_documents = {\n",
    "        'markdown': [],\n",
    "        'code': []\n",
    "    }\n",
    "\n",
    "    notebook = nbformat.from_dict(notebook_document)\n",
    "\n",
    "    index = 0\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            notebook_documents['markdown'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "        if cell.cell_type == 'code':\n",
    "            notebook_documents['code'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "    \n",
    "    return notebook_documents\n",
    "\n",
    "def parse_markdown_into_text(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    text = soup.get_text()\n",
    "    text = text.rstrip('\\n')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefc0c40-9861-4189-a132-5f2692eb4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_urls = [\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/tutorials/demo_notebooks/demo_pipeline/demo-pipeline.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/experiments/article/cloud-hpc/Cloud-HPC-FMNIST-Experiment.ipynb',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/backend/functions/platforms/celery.py',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/deployment/monitoring/kustomization.yaml',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/development/GPUs/gpu-125-test.yaml',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/development/LLMs/deployment/compose/cpu-stack.yaml',\n",
    "    'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/gpu-setup.sh'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133f1e27-f68f-4dc3-afd5-8c735ec4b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_documents = scrape_documents(\n",
    "    url_list = wanted_urls,\n",
    "    timeout = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "804ac7b8-22e7-4c38-82c1-44e32204d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "def tree_extract_imports(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(tree_extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def tree_extract_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(tree_extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "\n",
    "def tree_extract_code_and_dependencies(\n",
    "    node: any,\n",
    "    code_text: str\n",
    ") -> any:\n",
    "    codes = []\n",
    "    if not node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name')\n",
    "        if name is None:\n",
    "            code = code_text[start_byte:end_byte].decode('utf8')\n",
    "            if not 'def' in code:\n",
    "                dependencies = tree_extract_dependencies(node, code_text)\n",
    "                codes.append({\n",
    "                    'name': 'global',\n",
    "                    'code': code,\n",
    "                    'dependencies': dependencies\n",
    "                })\n",
    "    return codes\n",
    "\n",
    "def tree_extract_functions_and_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(tree_extract_functions_and_dependencies(child, code_text))\n",
    "    return functions\n",
    "\n",
    "def tree_get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def tree_get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions\n",
    "\n",
    "def tree_create_code_document(\n",
    "    code_imports: any,\n",
    "    code_functions: any,\n",
    "    function_item: any\n",
    ") -> any:\n",
    "    used_imports = tree_get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = tree_get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': function_item['name'],\n",
    "        'dependencies': function_item['dependencies'],\n",
    "        'code': function_item['code']\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "     \n",
    "def tree_format_code_document(\n",
    "    code_document: any\n",
    ") -> any:\n",
    "    formatted_document = ''\n",
    "    for doc_import in code_document['imports']:\n",
    "        formatted_document += doc_import + '\\n'\n",
    "\n",
    "    for doc_functions in code_document['functions']:\n",
    "        formatted_document += doc_functions + '\\n'\n",
    "\n",
    "    if 0 < len(code_document['dependencies']):\n",
    "        formatted_document += 'code dependencies\\n'\n",
    "\n",
    "        for doc_dependency in code_document['dependencies']:\n",
    "            formatted_document += doc_dependency + '\\n'\n",
    "\n",
    "    if code_document['name'] == 'global':\n",
    "        formatted_document += code_document['name'] + ' code\\n'\n",
    "    else:\n",
    "        formatted_document += 'function ' + code_document['name'] + ' code\\n'\n",
    "    \n",
    "    for line in code_document['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        doc_code = re.sub(r'#.*','', line)\n",
    "        if not bool(doc_code.strip()):\n",
    "            continue\n",
    "        formatted_document += doc_code + '\\n'    \n",
    "    return formatted_document\n",
    "\n",
    "def tree_create_python_code_and_function_documents(\n",
    "    code_document: any\n",
    "):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "   \n",
    "    tree = parser.parse(\n",
    "        bytes(\n",
    "            code_document,\n",
    "            \"utf8\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    code_imports = tree_extract_imports(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_global = tree_extract_code_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_functions = tree_extract_functions_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    initial_documents = []\n",
    "    for item in code_global:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    for item in code_functions:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    formatted_documents = []\n",
    "    seen_functions = []\n",
    "    for document in initial_documents:\n",
    "        if not document['name'] == 'global':\n",
    "            if document['name'] in seen_functions:\n",
    "                continue\n",
    "        \n",
    "        print(document['name'])\n",
    "        formatted_document = tree_format_code_document(\n",
    "            code_document = document\n",
    "        )\n",
    "\n",
    "        formatted_documents.append(formatted_document)\n",
    "        seen_functions.append(document['name'])\n",
    "    return formatted_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5478f5c7-72c7-4cd8-bac0-97088e069c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_notebook_documents(\n",
    "    notebook_document: any\n",
    "):\n",
    "    notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "        notebook_document = notebook_document\n",
    "    )\n",
    "\n",
    "    markdown_document = ''\n",
    "    markdown_ids = []\n",
    "    for block in notebook_documents['markdown']:\n",
    "        joined_text = ''.join(block['data'])\n",
    "        markdown_text = parse_markdown_into_text(\n",
    "            markdown_document = joined_text\n",
    "        )\n",
    "        markdown_document += markdown_text + '\\n\\n'\n",
    "        markdown_ids.append(block['id'])\n",
    "    \n",
    "    code_documents = []\n",
    "    code_ids = []\n",
    "    for block in notebook_documents['code']:\n",
    "        joined_code = ''.join(block['data'])\n",
    "        block_code_documents = tree_create_python_code_and_function_documents(\n",
    "            code_document = joined_code\n",
    "        )\n",
    "        \n",
    "        if len(block_code_documents) == 0:\n",
    "            #print(block['data'])\n",
    "            code_documents.extend(block['data'])\n",
    "        else:\n",
    "            code_documents.extend(block_code_documents)\n",
    "        code_ids.append(block['id'])\n",
    "    \n",
    "    formatted_documents = {\n",
    "        'markdown': markdown_document,\n",
    "        'code': code_documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b8dcdc7f-8ad2-45d6-a03e-504fee248fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "import nbformat\n",
    "import requests\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_document(\n",
    "    document_url: str,\n",
    "    document_type: str\n",
    ") -> any:\n",
    "    document = None\n",
    "    response = requests.get(\n",
    "        url = document_url\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        if document_type == 'text':\n",
    "            document = response.text\n",
    "        if document_type == 'json':\n",
    "            document = json.loads(response.text)\n",
    "        # handle html later\n",
    "    return document\n",
    "\n",
    "def scrape_documents(\n",
    "    url_list: any,\n",
    "    timeout: int\n",
    ") -> any:\n",
    "    documents = []\n",
    "\n",
    "    text_files = [\n",
    "        'py',\n",
    "        'md',\n",
    "        'yaml',\n",
    "        'sh'\n",
    "    ]\n",
    "\n",
    "    json_files = [\n",
    "        'ipynb'\n",
    "    ]\n",
    "    index = 0\n",
    "    for url in url_list:\n",
    "        document = None\n",
    "        url_split = url.split('/')\n",
    "        if 'github' in url_split[2]:\n",
    "            if 'raw' in url_split[2]:\n",
    "                file_end = url_split[-1].split('.')[-1]\n",
    "                if file_end in text_files:\n",
    "                    document = get_document(\n",
    "                        document_url = url,\n",
    "                        document_type = 'text' \n",
    "                    )\n",
    "                if file_end in json_files:\n",
    "                    document = get_document(\n",
    "                        document_url = url,\n",
    "                        document_type = 'json' \n",
    "                    )\n",
    "        documents.append(document)\n",
    "        index = index + 1\n",
    "        if index < len(url_list):\n",
    "            time.sleep(timeout)\n",
    "    return documents\n",
    "\n",
    "def extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_document: any\n",
    "): \n",
    "    notebook_documents = {\n",
    "        'markdown': [],\n",
    "        'code': []\n",
    "    }\n",
    "\n",
    "    notebook = nbformat.from_dict(notebook_document)\n",
    "\n",
    "    index = 0\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            notebook_documents['markdown'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "        if cell.cell_type == 'code':\n",
    "            notebook_documents['code'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "    \n",
    "    return notebook_documents\n",
    "    \n",
    "def parse_markdown_into_text(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    text = soup.get_text()\n",
    "    text = text.rstrip('\\n')\n",
    "    return text\n",
    "\n",
    "def create_notebook_documents(\n",
    "    notebook_document: any\n",
    "):\n",
    "    notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "        notebook_document = notebook_document\n",
    "    )\n",
    "\n",
    "    markdown_document = ''\n",
    "    markdown_ids = []\n",
    "    for block in notebook_documents['markdown']:\n",
    "        joined_text = ''.join(block['data'])\n",
    "        markdown_text = parse_markdown_into_text(\n",
    "            markdown_text = joined_text\n",
    "        )\n",
    "        markdown_document += markdown_text + '\\n\\n'\n",
    "        markdown_ids.append(block['id'])\n",
    "    # There can be duplicate functions\n",
    "    # These needs to be removed by preprocessing\n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    code_ids = []\n",
    "    for block in notebook_documents['code']:\n",
    "        joined_code = ''.join(block['data'])\n",
    "        block_code_documents = tree_create_python_code_and_function_documents(\n",
    "            code_document = joined_code\n",
    "        )\n",
    "\n",
    "        code_doc_index = 0\n",
    "        for code_doc in block_code_documents:\n",
    "            row_split = code_doc.split('\\n')\n",
    "            for row in row_split:\n",
    "                if 'function' in row and 'code' in row:\n",
    "                    function_name = row.split(' ')[1]\n",
    "                    if not function_name in seen_function_names:\n",
    "                        seen_function_names.append(function_name)\n",
    "                    else:\n",
    "                        print(function_name)\n",
    "                        del block_code_documents[code_doc_index]\n",
    "            code_doc_index += 1\n",
    "        \n",
    "        if 0 < len(block_code_documents):\n",
    "            code_documents.extend(block_code_documents)\n",
    "            code_ids.append(block['id'])\n",
    "    \n",
    "    formatted_documents = {\n",
    "        'markdown': markdown_document,\n",
    "        'code': code_documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8571d39b-7b02-4fc1-8f0d-b6f5ece1c96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_istio_auth_session\n"
     ]
    }
   ],
   "source": [
    "formatted_notebook_documents = create_notebook_documents(\n",
    "    notebook_document = scraped_documents[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4ad17b25-5ae6-4b48-ab4d-8716e2d3e70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'code dependencies\\n%%bash\\npip install kfp~=1.8.14\\n'\n"
     ]
    }
   ],
   "source": [
    "print(repr(formatted_notebook_documents['code'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1001f52c-dfb3-486d-998e-cefaa67d899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'warnings.filterwarnings(\"ignore\")\\n'\n"
     ]
    }
   ],
   "source": [
    "print(repr(formatted_notebook_documents['code'][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2ad68031-bb3e-475f-9182-a2ebee639f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global code\n",
      "%%bash\n",
      "pip install kfp~=1.8.14\n",
      "\n",
      "\n",
      "import warnings\n",
      "code dependencies\n",
      "warnings.filterwarnings\n",
      "global code\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "import kfp\n",
      "import kfp.dsl as dsl\n",
      "from kfp.aws import use_aws_secret\n",
      "from kfp.v2.dsl import (\n",
      "    component,\n",
      "    Input,\n",
      "    Output,\n",
      "    Dataset,\n",
      "    Metrics,\n",
      "    Artifact,\n",
      "    Model\n",
      ")\n",
      "\n",
      "\n",
      "import requests\n",
      "from urllib.parse import urlsplit\n",
      "import re\n",
      "code dependencies\n",
      "requests.Session\n",
      "s.get\n",
      "RuntimeError\n",
      "len\n",
      "urlsplit\n",
      "re.search\n",
      "redirect_url_obj._replace\n",
      "re.sub\n",
      "re.search\n",
      "redirect_url_obj.geturl\n",
      "s.get\n",
      "redirect_url_obj.geturl\n",
      "RuntimeError\n",
      "redirect_url_obj.geturl\n",
      "s.post\n",
      "len\n",
      "RuntimeError\n",
      "\"; \".join\n",
      "function get_istio_auth_session code\n",
      "def get_istio_auth_session(url: str, username: str, password: str) -> dict:\n",
      "    \"\"\"\n",
      "    Determine if the specified URL is secured by Dex and try to obtain a session cookie.\n",
      "    WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\n",
      "             (we default default to using `staticPasswords` if both are enabled)\n",
      "    :param url: Kubeflow server URL, including protocol\n",
      "    :param username: Dex `staticPasswords` or `LDAP` username\n",
      "    :param password: Dex `staticPasswords` or `LDAP` password\n",
      "    :return: auth session information\n",
      "    \"\"\"\n",
      "    auth_session = {\n",
      "        \"endpoint_url\": url,    \n",
      "        \"redirect_url\": None,   \n",
      "        \"dex_login_url\": None,  \n",
      "        \"is_secured\": None,     \n",
      "        \"session_cookie\": None  \n",
      "    }\n",
      "    with requests.Session() as s:\n",
      "        resp = s.get(url, allow_redirects=True)\n",
      "        if resp.status_code != 200:\n",
      "            raise RuntimeError(\n",
      "                f\"HTTP status code '{resp.status_code}' for GET against: {url}\"\n",
      "            )\n",
      "        auth_session[\"redirect_url\"] = resp.url\n",
      "        if len(resp.history) == 0:\n",
      "            auth_session[\"is_secured\"] = False\n",
      "            return auth_session\n",
      "        else:\n",
      "            auth_session[\"is_secured\"] = True\n",
      "        redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\n",
      "        if re.search(r\"/auth$\", redirect_url_obj.path):\n",
      "            redirect_url_obj = redirect_url_obj._replace(\n",
      "                path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\n",
      "            )\n",
      "        if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\n",
      "            auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\n",
      "        else:\n",
      "            resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\n",
      "            if resp.status_code != 200:\n",
      "                raise RuntimeError(\n",
      "                    f\"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}\"\n",
      "                )\n",
      "            auth_session[\"dex_login_url\"] = resp.url\n",
      "        resp = s.post(\n",
      "            auth_session[\"dex_login_url\"],\n",
      "            data={\"login\": username, \"password\": password},\n",
      "            allow_redirects=True\n",
      "        )\n",
      "        if len(resp.history) == 0:\n",
      "            raise RuntimeError(\n",
      "                f\"Login credentials were probably invalid - \"\n",
      "                f\"No redirect after POST to: {auth_session['dex_login_url']}\"\n",
      "            )\n",
      "        auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
      "    return auth_session\n",
      "\n",
      "\n",
      "import kfp\n",
      "code dependencies\n",
      "get_istio_auth_session\n",
      "kfp.Client\n",
      "global code\n",
      "import kfp\n",
      "KUBEFLOW_ENDPOINT = \"http://localhost:8080\"\n",
      "KUBEFLOW_USERNAME = \"user@example.com\"\n",
      "KUBEFLOW_PASSWORD = \"12341234\"\n",
      "auth_session = get_istio_auth_session(\n",
      "    url=KUBEFLOW_ENDPOINT,\n",
      "    username=KUBEFLOW_USERNAME,\n",
      "    password=KUBEFLOW_PASSWORD\n",
      ")\n",
      "client = kfp.Client(host=f\"{KUBEFLOW_ENDPOINT}/pipeline\", cookies=auth_session[\"session_cookie\"])\n",
      "\n",
      "\n",
      "code dependencies\n",
      "pd.read_csv\n",
      "df.to_csv\n",
      "function pull_data code\n",
      "def pull_data(url: str, data: Output[Dataset]):\n",
      "    \"\"\"\n",
      "    Pull data component.\n",
      "    \"\"\"\n",
      "    import pandas as pd\n",
      "    df = pd.read_csv(url, sep=\";\")\n",
      "    df.to_csv(data.path, index=None)\n",
      "\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import pickle\n",
      "code dependencies\n",
      "pd.read_csv\n",
      "train_test_split\n",
      "StandardScaler\n",
      "train.drop\n",
      "scaler.fit_transform\n",
      "train.drop\n",
      "test.drop\n",
      "scaler.transform\n",
      "test.drop\n",
      "open\n",
      "pickle.dump\n",
      "train.to_csv\n",
      "test.to_csv\n",
      "function preprocess code\n",
      "def preprocess(\n",
      "    data: Input[Dataset],\n",
      "    scaler_out: Output[Artifact],\n",
      "    train_set: Output[Dataset],\n",
      "    test_set: Output[Dataset],\n",
      "    target: str = \"quality\",\n",
      "):\n",
      "    \"\"\"\n",
      "    Preprocess component.\n",
      "    \"\"\"\n",
      "    import pandas as pd\n",
      "    import pickle\n",
      "    from sklearn.model_selection import train_test_split\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "    data = pd.read_csv(data.path)\n",
      "    train, test = train_test_split(data)\n",
      "    scaler = StandardScaler()\n",
      "    train[train.drop(target, axis=1).columns] = scaler.fit_transform(train.drop(target, axis=1))\n",
      "    test[test.drop(target, axis=1).columns] = scaler.transform(test.drop(target, axis=1))\n",
      "    with open(scaler_out.path, 'wb') as fp:\n",
      "        pickle.dump(scaler, fp, pickle.HIGHEST_PROTOCOL)\n",
      "    train.to_csv(train_set.path, index=None)\n",
      "    test.to_csv(test_set.path, index=None)\n",
      "\n",
      "\n",
      "from typing import NamedTuple\n",
      "import logging\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "import mlflow\n",
      "from sklearn.linear_model import ElasticNet\n",
      "import pickle\n",
      "from collections import namedtuple\n",
      "from ice import eval_metrics\n",
      "code dependencies\n",
      "NamedTuple\n",
      "logging.basicConfig\n",
      "logging.getLogger\n",
      "np.sqrt\n",
      "mean_squared_error\n",
      "mean_absolute_error\n",
      "r2_score\n",
      "pd.read_csv\n",
      "pd.read_csv\n",
      "train.drop\n",
      "test.drop\n",
      "logger.info\n",
      "mlflow.set_tracking_uri\n",
      "logger.info\n",
      "mlflow.set_experiment\n",
      "mlflow.start_run\n",
      "logger.info\n",
      "ElasticNet\n",
      "logger.info\n",
      "model.fit\n",
      "logger.info\n",
      "model.predict\n",
      "eval_metrics\n",
      "logger.info\n",
      "logger.info\n",
      "logger.info\n",
      "logger.info\n",
      "logger.info\n",
      "mlflow.log_param\n",
      "mlflow.log_param\n",
      "mlflow.log_metric\n",
      "mlflow.log_metric\n",
      "mlflow.log_metric\n",
      "logger.info\n",
      "mlflow.sklearn.log_model\n",
      "logger.info\n",
      "np.save\n",
      "mlflow.log_artifact\n",
      "logging.info\n",
      "open\n",
      "pickle.dump\n",
      "namedtuple\n",
      "output\n",
      "mlflow.get_artifact_uri\n",
      "function train code\n",
      "def train(\n",
      "    train_set: Input[Dataset],\n",
      "    test_set: Input[Dataset],\n",
      "    saved_model: Output[Model],\n",
      "    mlflow_experiment_name: str,\n",
      "    mlflow_tracking_uri: str,\n",
      "    mlflow_s3_endpoint_url: str,\n",
      "    model_name: str,\n",
      "    alpha: float,\n",
      "    l1_ratio: float,\n",
      "    target: str = \"quality\",\n",
      ") -> NamedTuple(\"Output\", [('storage_uri', str), ('run_id', str),]):\n",
      "    \"\"\"\n",
      "    Train component.\n",
      "    \"\"\"\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    from sklearn.linear_model import ElasticNet\n",
      "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
      "    import mlflow\n",
      "    import mlflow.sklearn\n",
      "    import os\n",
      "    import logging\n",
      "    import pickle\n",
      "    from collections import namedtuple\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "    def eval_metrics(actual, pred):\n",
      "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
      "        mae = mean_absolute_error(actual, pred)\n",
      "        r2 = r2_score(actual, pred)\n",
      "        return rmse, mae, r2\n",
      "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = mlflow_s3_endpoint_url\n",
      "    train = pd.read_csv(train_set.path)\n",
      "    test = pd.read_csv(test_set.path)\n",
      "    train_x = train.drop([target], axis=1)\n",
      "    test_x = test.drop([target], axis=1)\n",
      "    train_y = train[[target]]\n",
      "    test_y = test[[target]]\n",
      "    logger.info(f\"Using MLflow tracking URI: {mlflow_tracking_uri}\")\n",
      "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
      "    logger.info(f\"Using MLflow experiment: {mlflow_experiment_name}\")\n",
      "    mlflow.set_experiment(mlflow_experiment_name)\n",
      "    with mlflow.start_run() as run:\n",
      "        run_id = run.info.run_id\n",
      "        logger.info(f\"Run ID: {run_id}\")\n",
      "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
      "        logger.info(\"Fitting model...\")\n",
      "        model.fit(train_x, train_y)\n",
      "        logger.info(\"Predicting...\")\n",
      "        predicted_qualities = model.predict(test_x)\n",
      "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
      "        logger.info(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
      "        logger.info(\"  RMSE: %s\" % rmse)\n",
      "        logger.info(\"  MAE: %s\" % mae)\n",
      "        logger.info(\"  R2: %s\" % r2)\n",
      "        logger.info(\"Logging parameters to MLflow\")\n",
      "        mlflow.log_param(\"alpha\", alpha)\n",
      "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
      "        mlflow.log_metric(\"rmse\", rmse)\n",
      "        mlflow.log_metric(\"r2\", r2)\n",
      "        mlflow.log_metric(\"mae\", mae)\n",
      "        logger.info(\"Logging trained model\")\n",
      "        mlflow.sklearn.log_model(\n",
      "            model,\n",
      "            model_name,\n",
      "            registered_model_name=\"ElasticnetWineModel\",\n",
      "            serialization_format=\"pickle\"\n",
      "        )\n",
      "        logger.info(\"Logging predictions artifact to MLflow\")\n",
      "        np.save(\"predictions.npy\", predicted_qualities)\n",
      "        mlflow.log_artifact(\n",
      "        local_path=\"predictions.npy\", artifact_path=\"predicted_qualities/\"\n",
      "        )\n",
      "        logging.info(f\"Saving model to: {saved_model.path}\")\n",
      "        with open(saved_model.path, 'wb') as fp:\n",
      "            pickle.dump(model, fp, pickle.HIGHEST_PROTOCOL)\n",
      "        output = namedtuple('Output', ['storage_uri', 'run_id'])\n",
      "        return output(mlflow.get_artifact_uri(), run_id)\n",
      "\n",
      "\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "code dependencies\n",
      "np.sqrt\n",
      "mean_squared_error\n",
      "mean_absolute_error\n",
      "r2_score\n",
      "function eval_metrics code\n",
      "def eval_metrics(actual, pred):\n",
      "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
      "        mae = mean_absolute_error(actual, pred)\n",
      "        r2 = r2_score(actual, pred)\n",
      "        return rmse, mae, r2\n",
      "\n",
      "\n",
      "import logging\n",
      "from mlflow.tracking import MlflowClient\n",
      "code dependencies\n",
      "logging.basicConfig\n",
      "logging.getLogger\n",
      "MlflowClient\n",
      "client.get_run\n",
      "logger.info\n",
      "threshold_metrics.items\n",
      "logger.error\n",
      "function evaluate code\n",
      "def evaluate(\n",
      "    run_id: str,\n",
      "    mlflow_tracking_uri: str,\n",
      "    threshold_metrics: dict\n",
      ") -> bool:\n",
      "    \"\"\"\n",
      "    Evaluate component: Compares metrics from training with given thresholds.\n",
      "    Args:\n",
      "        run_id (string):  MLflow run ID\n",
      "        mlflow_tracking_uri (string): MLflow tracking URI\n",
      "        threshold_metrics (dict): Minimum threshold values for each metric\n",
      "    Returns:\n",
      "        Bool indicating whether evaluation passed or failed.\n",
      "    \"\"\"\n",
      "    from mlflow.tracking import MlflowClient\n",
      "    import logging\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "    client = MlflowClient(tracking_uri=mlflow_tracking_uri)\n",
      "    info = client.get_run(run_id)\n",
      "    training_metrics = info.data.metrics\n",
      "    logger.info(f\"Training metrics: {training_metrics}\")\n",
      "    for key, value in threshold_metrics.items():\n",
      "        if key not in training_metrics or training_metrics[key] > value:\n",
      "            logger.error(f\"Metric {key} failed. Evaluation not passed!\")\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "\n",
      "import logging\n",
      "from kserve import utils\n",
      "from kserve import V1beta1InferenceService\n",
      "from kubernetes import client\n",
      "from kserve import V1beta1InferenceServiceSpec\n",
      "from kserve import V1beta1PredictorSpec\n",
      "from kserve import V1beta1SKLearnSpec\n",
      "from kubernetes.client import V1ResourceRequirements\n",
      "from kserve import KServeClient\n",
      "code dependencies\n",
      "logging.basicConfig\n",
      "logging.getLogger\n",
      "logger.info\n",
      "utils.get_default_target_namespace\n",
      "V1beta1InferenceService\n",
      "client.V1ObjectMeta\n",
      "V1beta1InferenceServiceSpec\n",
      "V1beta1PredictorSpec\n",
      "V1beta1SKLearnSpec\n",
      "V1ResourceRequirements\n",
      "KServeClient\n",
      "KServe.create\n",
      "function deploy_model code\n",
      "def deploy_model(model_name: str, storage_uri: str):\n",
      "    \"\"\"\n",
      "    Deploy the model as an inference service with Kserve.\n",
      "    \"\"\"\n",
      "    import logging\n",
      "    from kubernetes import client\n",
      "    from kserve import KServeClient\n",
      "    from kserve import constants\n",
      "    from kserve import utils\n",
      "    from kserve import V1beta1InferenceService\n",
      "    from kserve import V1beta1InferenceServiceSpec\n",
      "    from kserve import V1beta1PredictorSpec\n",
      "    from kserve import V1beta1SKLearnSpec\n",
      "    from kubernetes.client import V1ResourceRequirements\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "    model_uri = f\"{storage_uri}/{model_name}\"\n",
      "    logger.info(f\"MODEL URI: {model_uri}\")\n",
      "    namespace = utils.get_default_target_namespace()\n",
      "    kserve_version='v1beta1'\n",
      "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
      "    isvc = V1beta1InferenceService(\n",
      "        api_version = api_version,\n",
      "        kind = constants.KSERVE_KIND,\n",
      "        metadata = client.V1ObjectMeta(\n",
      "            name = model_name,\n",
      "            namespace = namespace,\n",
      "            annotations = {'sidecar.istio.io/inject':'false'}\n",
      "        ),\n",
      "        spec = V1beta1InferenceServiceSpec(\n",
      "            predictor=V1beta1PredictorSpec(\n",
      "                service_account_name=\"kserve-sa\",\n",
      "                min_replicas=1,\n",
      "                max_replicas = 1,\n",
      "                sklearn=V1beta1SKLearnSpec(\n",
      "                    storage_uri=model_uri,\n",
      "                    resources=V1ResourceRequirements(\n",
      "                        requests={\"cpu\": \"100m\", \"memory\": \"512Mi\"},\n",
      "                        limits={\"cpu\": \"300m\", \"memory\": \"512Mi\"}\n",
      "                    )\n",
      "                ),\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "    KServe = KServeClient()\n",
      "    KServe.create(isvc)\n",
      "\n",
      "\n",
      "import logging\n",
      "import requests\n",
      "from urllib.parse import urlsplit\n",
      "import re\n",
      "from kserve import utils\n",
      "import pickle\n",
      "from kserve import KServeClient\n",
      "from ice import get_istio_auth_session\n",
      "code dependencies\n",
      "logging.basicConfig\n",
      "logging.getLogger\n",
      "requests.Session\n",
      "s.get\n",
      "RuntimeError\n",
      "len\n",
      "urlsplit\n",
      "re.search\n",
      "redirect_url_obj._replace\n",
      "re.sub\n",
      "re.search\n",
      "redirect_url_obj.geturl\n",
      "s.get\n",
      "redirect_url_obj.geturl\n",
      "RuntimeError\n",
      "redirect_url_obj.geturl\n",
      "s.post\n",
      "len\n",
      "RuntimeError\n",
      "\"; \".join\n",
      "get_istio_auth_session\n",
      "auth_session[\"session_cookie\"].replace\n",
      "print\n",
      "utils.get_default_target_namespace\n",
      "logger.info\n",
      "open\n",
      "pickle.load\n",
      "logger.info\n",
      "scaler.transform\n",
      "KServeClient\n",
      "KServe.get\n",
      "KServe.get\n",
      "logger.info\n",
      "logger.info\n",
      "logger.info\n",
      "input_sample.tolist\n",
      "requests.post\n",
      "RuntimeError\n",
      "response.json\n",
      "logger.info\n",
      "response.json\n",
      "function inference code\n",
      "def inference(\n",
      "    model_name: str,\n",
      "    scaler_in: Input[Artifact]\n",
      "):\n",
      "    \"\"\"\n",
      "    Test inference.\n",
      "    \"\"\"\n",
      "    from kserve import KServeClient\n",
      "    import requests\n",
      "    import pickle\n",
      "    import logging\n",
      "    from kserve import utils\n",
      "    from urllib.parse import urlsplit\n",
      "    import re\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "    def get_istio_auth_session(url: str, username: str, password: str) -> dict:\n",
      "        \"\"\"\n",
      "        Determine if the specified URL is secured by Dex and try to obtain a session cookie.\n",
      "        WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\n",
      "                 (we default default to using `staticPasswords` if both are enabled)\n",
      "        :param url: Kubeflow server URL, including protocol\n",
      "        :param username: Dex `staticPasswords` or `LDAP` username\n",
      "        :param password: Dex `staticPasswords` or `LDAP` password\n",
      "        :return: auth session information\n",
      "        \"\"\"\n",
      "        auth_session = {\n",
      "            \"endpoint_url\": url,    \n",
      "            \"redirect_url\": None,   \n",
      "            \"dex_login_url\": None,  \n",
      "            \"is_secured\": None,     \n",
      "            \"session_cookie\": None  \n",
      "        }\n",
      "        with requests.Session() as s:\n",
      "            resp = s.get(url, allow_redirects=True)\n",
      "            if resp.status_code != 200:\n",
      "                raise RuntimeError(\n",
      "                    f\"HTTP status code '{resp.status_code}' for GET against: {url}\"\n",
      "                )\n",
      "            auth_session[\"redirect_url\"] = resp.url\n",
      "            if len(resp.history) == 0:\n",
      "                auth_session[\"is_secured\"] = False\n",
      "                return auth_session\n",
      "            else:\n",
      "                auth_session[\"is_secured\"] = True\n",
      "            redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\n",
      "            if re.search(r\"/auth$\", redirect_url_obj.path):\n",
      "                redirect_url_obj = redirect_url_obj._replace(\n",
      "                    path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\n",
      "                )\n",
      "            if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\n",
      "                auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\n",
      "            else:\n",
      "                resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\n",
      "                if resp.status_code != 200:\n",
      "                    raise RuntimeError(\n",
      "                        f\"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}\"\n",
      "                    )\n",
      "                auth_session[\"dex_login_url\"] = resp.url\n",
      "            resp = s.post(\n",
      "                auth_session[\"dex_login_url\"],\n",
      "                data={\"login\": username, \"password\": password},\n",
      "                allow_redirects=True\n",
      "            )\n",
      "            if len(resp.history) == 0:\n",
      "                raise RuntimeError(\n",
      "                    f\"Login credentials were probably invalid - \"\n",
      "                    f\"No redirect after POST to: {auth_session['dex_login_url']}\"\n",
      "                )\n",
      "            auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
      "        return auth_session\n",
      "    KUBEFLOW_ENDPOINT = \"http://istio-ingressgateway.istio-system.svc.cluster.local:80\"\n",
      "    KUBEFLOW_USERNAME = \"user@example.com\"\n",
      "    KUBEFLOW_PASSWORD = \"12341234\"\n",
      "    auth_session = get_istio_auth_session(\n",
      "    url=KUBEFLOW_ENDPOINT,\n",
      "    username=KUBEFLOW_USERNAME,\n",
      "    password=KUBEFLOW_PASSWORD,\n",
      "    )\n",
      "    TOKEN = auth_session[\"session_cookie\"].replace(\"authservice_session=\", \"\")\n",
      "    print(\"Token:\", TOKEN)\n",
      "    namespace = utils.get_default_target_namespace()\n",
      "    input_sample = [[5.6, 0.54, 0.04, 1.7, 0.049, 5, 13, 0.9942, 3.72, 0.58, 11.4],\n",
      "                    [11.3, 0.34, 0.45, 2, 0.082, 6, 15, 0.9988, 2.94, 0.66, 9.2]]\n",
      "    logger.info(f\"Loading standard scaler from: {scaler_in.path}\")\n",
      "    with open(scaler_in.path, 'rb') as fp:\n",
      "        scaler = pickle.load(fp)\n",
      "    logger.info(f\"Standardizing sample: {scaler_in.path}\")\n",
      "    input_sample = scaler.transform(input_sample)\n",
      "    KServe = KServeClient()\n",
      "    KServe.get(model_name, namespace=namespace, watch=True, timeout_seconds=120)\n",
      "    inference_service = KServe.get(model_name, namespace=namespace)\n",
      "    logger.info(f\"inference_service: {inference_service}\")\n",
      "    is_url = f\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/v1/models/{model_name}:predict\"\n",
      "    header = {\"Host\": f\"{model_name}.{namespace}.example.com\"}\n",
      "    logger.info(f\"\\nInference service status:\\n{inference_service['status']}\")\n",
      "    logger.info(f\"\\nInference service URL:\\n{is_url}\\n\")\n",
      "    inference_input = {\n",
      "        'instances': input_sample.tolist()\n",
      "    }\n",
      "    response = requests.post(\n",
      "        is_url,\n",
      "        json=inference_input,\n",
      "        headers=header,\n",
      "        cookies={\"authservice_session\": TOKEN}\n",
      "    )\n",
      "    if response.status_code != 200:\n",
      "        raise RuntimeError(f\"HTTP status code '{response.status_code}': {response.json()}\")\n",
      "    logger.info(f\"\\nPrediction response:\\n{response.json()}\\n\")\n",
      "\n",
      "\n",
      "code dependencies\n",
      "pull_data\n",
      "preprocess\n",
      "train\n",
      "train_task.apply\n",
      "use_aws_secret\n",
      "evaluate\n",
      "dsl.Condition\n",
      "deploy_model\n",
      "inference\n",
      "inference_task.after\n",
      "function pipeline code\n",
      "def pipeline(\n",
      "    url: str,\n",
      "    target: str,\n",
      "    mlflow_experiment_name: str,\n",
      "    mlflow_tracking_uri: str,\n",
      "    mlflow_s3_endpoint_url: str,\n",
      "    model_name: str,\n",
      "    alpha: float,\n",
      "    l1_ratio: float,\n",
      "    threshold_metrics: dict,\n",
      "):\n",
      "    pull_task = pull_data(url=url)\n",
      "    preprocess_task = preprocess(data=pull_task.outputs[\"data\"])\n",
      "    train_task = train(\n",
      "        train_set=preprocess_task.outputs[\"train_set\"],\n",
      "        test_set=preprocess_task.outputs[\"test_set\"],\n",
      "        target=target,\n",
      "        mlflow_experiment_name=mlflow_experiment_name,\n",
      "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
      "        mlflow_s3_endpoint_url=mlflow_s3_endpoint_url,\n",
      "        model_name=model_name,\n",
      "        alpha=alpha,\n",
      "        l1_ratio=l1_ratio\n",
      "    )\n",
      "    train_task.apply(use_aws_secret(secret_name=\"aws-secret\"))\n",
      "    evaluate_trask = evaluate(\n",
      "        run_id=train_task.outputs[\"run_id\"],\n",
      "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
      "        threshold_metrics=threshold_metrics\n",
      "    )\n",
      "    eval_passed = evaluate_trask.output\n",
      "    with dsl.Condition(eval_passed == \"true\"):\n",
      "        deploy_model_task = deploy_model(\n",
      "            model_name=model_name,\n",
      "            storage_uri=train_task.outputs[\"storage_uri\"],\n",
      "        )\n",
      "        inference_task = inference(\n",
      "            model_name=model_name,\n",
      "            scaler_in=preprocess_task.outputs[\"scaler_out\"]\n",
      "        )\n",
      "        inference_task.after(deploy_model_task)\n",
      "\n",
      "\n",
      "global code\n",
      "eval_threshold_metrics = {'rmse': 0.9, 'r2': 0.3, 'mae': 0.8}\n",
      "arguments = {\n",
      "    \"url\": \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
      "    \"target\": \"quality\",\n",
      "    \"mlflow_tracking_uri\": \"http://mlflow.mlflow.svc.cluster.local:5000\",\n",
      "    \"mlflow_s3_endpoint_url\": \"http://mlflow-minio-service.mlflow.svc.cluster.local:9000\",\n",
      "    \"mlflow_experiment_name\": \"demo-notebook\",\n",
      "    \"model_name\": \"wine-quality\",\n",
      "    \"alpha\": 0.5,\n",
      "    \"l1_ratio\": 0.5,\n",
      "    \"threshold_metrics\": eval_threshold_metrics\n",
      "}\n",
      "\n",
      "\n",
      "code dependencies\n",
      "client.create_run_from_pipeline_func\n",
      "global code\n",
      "run_name = \"demo-run\"\n",
      "experiment_name = \"demo-experiment\"\n",
      "client.create_run_from_pipeline_func(\n",
      "    pipeline_func=pipeline,\n",
      "    run_name=run_name,\n",
      "    experiment_name=experiment_name,\n",
      "    arguments=arguments,\n",
      "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
      "    enable_caching=False,\n",
      "    namespace=\"kubeflow-user-example-com\"\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for code in formatted_notebook_documents['code']:\n",
    "    print(code)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be6bc67-b49e-43ce-8935-13f5d4cf52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_document = scraped_documents[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be60e924-090e-4c31-829a-b7e1bc0340dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'markdown': [{'id': 0, 'data': ['# Demo KFP pipeline']},\n",
       "  {'id': 1, 'data': ['Install requirements:']},\n",
       "  {'id': 3, 'data': ['Imports:']},\n",
       "  {'id': 5,\n",
       "   'data': ['## 1. Connect to client\\n',\n",
       "    '\\n',\n",
       "    \"The default way of accessing Kubeflow is via port-forward. This enables you to get started quickly without imposing any requirements on your environment. Run the following to port-forward Istio's Ingress-Gateway to local port `8080`:\\n\",\n",
       "    '\\n',\n",
       "    '```sh\\n',\n",
       "    'kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\\n',\n",
       "    '```']},\n",
       "  {'id': 8,\n",
       "   'data': ['## 2. Components\\n',\n",
       "    '\\n',\n",
       "    'There are different ways to define components in KFP. Here, we use the **@component** decorator to define the components as Python function-based components.\\n',\n",
       "    '\\n',\n",
       "    'The **@component** annotation converts the function into a factory function that creates pipeline steps that execute this function. This example also specifies the base container image to run you component in.']},\n",
       "  {'id': 9, 'data': ['Pull data component:']},\n",
       "  {'id': 11, 'data': ['Preprocess component:']},\n",
       "  {'id': 13, 'data': ['Train component:']},\n",
       "  {'id': 15, 'data': ['Evaluate component:']},\n",
       "  {'id': 17, 'data': ['Deploy model component:']},\n",
       "  {'id': 19, 'data': ['Inference component:']},\n",
       "  {'id': 21, 'data': ['## 3. Pipeline\\n', '\\n', 'Pipeline definition:']},\n",
       "  {'id': 23, 'data': ['Pipeline arguments:']},\n",
       "  {'id': 25, 'data': ['## 4. Submit run']},\n",
       "  {'id': 27, 'data': ['## 5. Check run']},\n",
       "  {'id': 28,\n",
       "   'data': ['### Kubeflow Pipelines UI\\n',\n",
       "    '\\n',\n",
       "    \"The default way of accessing Kubeflow is via port-forward. This enables you to get started quickly without imposing any requirements on your environment. Run the following to port-forward Istio's Ingress-Gateway to local port `8080`:\\n\",\n",
       "    '\\n',\n",
       "    '```sh\\n',\n",
       "    'kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\\n',\n",
       "    '```\\n',\n",
       "    '\\n',\n",
       "    'After running the command, you can access the Kubeflow Central Dashboard by doing the following:\\n',\n",
       "    '\\n',\n",
       "    '1. Open your browser and visit [http://localhost:8080/](http://localhost:8080/). You should get the Dex login screen.\\n',\n",
       "    \"2. Login with the default user's credential. The default email address is `user@example.com` and the default password is `12341234`.\"]},\n",
       "  {'id': 29,\n",
       "   'data': ['### MLFlow UI\\n',\n",
       "    '\\n',\n",
       "    'To access MLFlow UI, open a terminal and forward a local port to MLFlow server:\\n',\n",
       "    '\\n',\n",
       "    '<br>\\n',\n",
       "    '\\n',\n",
       "    '```bash\\n',\n",
       "    'kubectl -n mlflow port-forward svc/mlflow 5000:5000\\n',\n",
       "    '```\\n',\n",
       "    '\\n',\n",
       "    '<br>\\n',\n",
       "    '\\n',\n",
       "    \"Now MLFlow's UI should be reachable at [`http://localhost:5000`](http://localhost:5000).\"]},\n",
       "  {'id': 30,\n",
       "   'data': ['## 6. Check deployed model\\n',\n",
       "    '\\n',\n",
       "    '```bash\\n',\n",
       "    '# get inference services\\n',\n",
       "    'kubectl -n kubeflow-user-example-com get inferenceservice\\n',\n",
       "    '\\n',\n",
       "    '# get deployed model pods\\n',\n",
       "    'kubectl -n kubeflow-user-example-com get pods\\n',\n",
       "    '\\n',\n",
       "    '# delete inference service\\n',\n",
       "    'kubectl -n kubeflow-user-example-com delete inferenceservice wine-quality\\n',\n",
       "    '```\\n',\n",
       "    '<br>\\n',\n",
       "    '\\n',\n",
       "    'If something goes wrong, check the logs with:\\n',\n",
       "    '\\n',\n",
       "    '<br>\\n',\n",
       "    '\\n',\n",
       "    '```bash\\n',\n",
       "    'kubectl logs -n kubeflow-user-example-com <pod-name> kserve-container\\n',\n",
       "    '\\n',\n",
       "    'kubectl logs -n kubeflow-user-example-com <pod-name> queue-proxy\\n',\n",
       "    '\\n',\n",
       "    'kubectl logs -n kubeflow-user-example-com <pod-name> storage-initializer\\n',\n",
       "    '```\\n']},\n",
       "  {'id': 31,\n",
       "   'data': ['## 7. Troubleshooting\\n',\n",
       "    '\\n',\n",
       "    \"If the inference isn't working, try to patch the knative-serving config-domain:\\n\",\n",
       "    '\\n',\n",
       "    '\\n',\n",
       "    '\\n',\n",
       "    '```bash\\n',\n",
       "    'kubectl patch cm config-domain --patch \\'{\"data\":{\"example.com\":\"\"}}\\' -n knative-serving\\n',\n",
       "    '```']}],\n",
       " 'code': [{'id': 2, 'data': ['%%bash\\n', '\\n', 'pip install kfp~=1.8.14']},\n",
       "  {'id': 4,\n",
       "   'data': ['import warnings\\n',\n",
       "    'warnings.filterwarnings(\"ignore\")\\n',\n",
       "    '\\n',\n",
       "    'import kfp\\n',\n",
       "    'import kfp.dsl as dsl\\n',\n",
       "    'from kfp.aws import use_aws_secret\\n',\n",
       "    'from kfp.v2.dsl import (\\n',\n",
       "    '    component,\\n',\n",
       "    '    Input,\\n',\n",
       "    '    Output,\\n',\n",
       "    '    Dataset,\\n',\n",
       "    '    Metrics,\\n',\n",
       "    '    Artifact,\\n',\n",
       "    '    Model\\n',\n",
       "    ')']},\n",
       "  {'id': 6,\n",
       "   'data': ['import re\\n',\n",
       "    'import requests\\n',\n",
       "    'from urllib.parse import urlsplit\\n',\n",
       "    '\\n',\n",
       "    'def get_istio_auth_session(url: str, username: str, password: str) -> dict:\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Determine if the specified URL is secured by Dex and try to obtain a session cookie.\\n',\n",
       "    '    WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\\n',\n",
       "    '             (we default default to using `staticPasswords` if both are enabled)\\n',\n",
       "    '\\n',\n",
       "    '    :param url: Kubeflow server URL, including protocol\\n',\n",
       "    '    :param username: Dex `staticPasswords` or `LDAP` username\\n',\n",
       "    '    :param password: Dex `staticPasswords` or `LDAP` password\\n',\n",
       "    '    :return: auth session information\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    # define the default return object\\n',\n",
       "    '    auth_session = {\\n',\n",
       "    '        \"endpoint_url\": url,    # KF endpoint URL\\n',\n",
       "    '        \"redirect_url\": None,   # KF redirect URL, if applicable\\n',\n",
       "    '        \"dex_login_url\": None,  # Dex login URL (for POST of credentials)\\n',\n",
       "    '        \"is_secured\": None,     # True if KF endpoint is secured\\n',\n",
       "    '        \"session_cookie\": None  # Resulting session cookies in the form \"key1=value1; key2=value2\"\\n',\n",
       "    '    }\\n',\n",
       "    '\\n',\n",
       "    '    # use a persistent session (for cookies)\\n',\n",
       "    '    with requests.Session() as s:\\n',\n",
       "    '\\n',\n",
       "    '        ################\\n',\n",
       "    '        # Determine if Endpoint is Secured\\n',\n",
       "    '        ################\\n',\n",
       "    '        resp = s.get(url, allow_redirects=True)\\n',\n",
       "    '        if resp.status_code != 200:\\n',\n",
       "    '            raise RuntimeError(\\n',\n",
       "    '                f\"HTTP status code \\'{resp.status_code}\\' for GET against: {url}\"\\n',\n",
       "    '            )\\n',\n",
       "    '\\n',\n",
       "    '        auth_session[\"redirect_url\"] = resp.url\\n',\n",
       "    '\\n',\n",
       "    '        # if we were NOT redirected, then the endpoint is UNSECURED\\n',\n",
       "    '        if len(resp.history) == 0:\\n',\n",
       "    '            auth_session[\"is_secured\"] = False\\n',\n",
       "    '            return auth_session\\n',\n",
       "    '        else:\\n',\n",
       "    '            auth_session[\"is_secured\"] = True\\n',\n",
       "    '\\n',\n",
       "    '        ################\\n',\n",
       "    '        # Get Dex Login URL\\n',\n",
       "    '        ################\\n',\n",
       "    '        redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\\n',\n",
       "    '\\n',\n",
       "    '        # if we are at `/auth?=xxxx` path, we need to select an auth type\\n',\n",
       "    '        if re.search(r\"/auth$\", redirect_url_obj.path):\\n',\n",
       "    '\\n',\n",
       "    '            #######\\n',\n",
       "    '            # TIP: choose the default auth type by including ONE of the following\\n',\n",
       "    '            #######\\n',\n",
       "    '\\n',\n",
       "    '            # OPTION 1: set \"staticPasswords\" as default auth type\\n',\n",
       "    '            redirect_url_obj = redirect_url_obj._replace(\\n',\n",
       "    '                path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\\n',\n",
       "    '            )\\n',\n",
       "    '            # OPTION 2: set \"ldap\" as default auth type\\n',\n",
       "    '            # redirect_url_obj = redirect_url_obj._replace(\\n',\n",
       "    '            #     path=re.sub(r\"/auth$\", \"/auth/ldap\", redirect_url_obj.path)\\n',\n",
       "    '            # )\\n',\n",
       "    '\\n',\n",
       "    '        # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST)\\n',\n",
       "    '        if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\\n',\n",
       "    '            auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\\n',\n",
       "    '\\n',\n",
       "    '        # else, we need to be redirected to the actual login page\\n',\n",
       "    '        else:\\n',\n",
       "    '            # this GET should redirect us to the `/auth/xxxx/login` path\\n',\n",
       "    '            resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\\n',\n",
       "    '            if resp.status_code != 200:\\n',\n",
       "    '                raise RuntimeError(\\n',\n",
       "    '                    f\"HTTP status code \\'{resp.status_code}\\' for GET against: {redirect_url_obj.geturl()}\"\\n',\n",
       "    '                )\\n',\n",
       "    '\\n',\n",
       "    '            # set the login url\\n',\n",
       "    '            auth_session[\"dex_login_url\"] = resp.url\\n',\n",
       "    '\\n',\n",
       "    '        ################\\n',\n",
       "    '        # Attempt Dex Login\\n',\n",
       "    '        ################\\n',\n",
       "    '        resp = s.post(\\n',\n",
       "    '            auth_session[\"dex_login_url\"],\\n',\n",
       "    '            data={\"login\": username, \"password\": password},\\n',\n",
       "    '            allow_redirects=True\\n',\n",
       "    '        )\\n',\n",
       "    '        if len(resp.history) == 0:\\n',\n",
       "    '            raise RuntimeError(\\n',\n",
       "    '                f\"Login credentials were probably invalid - \"\\n',\n",
       "    '                f\"No redirect after POST to: {auth_session[\\'dex_login_url\\']}\"\\n',\n",
       "    '            )\\n',\n",
       "    '\\n',\n",
       "    '        # store the session cookies in a \"key1=value1; key2=value2\" string\\n',\n",
       "    '        auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\\n',\n",
       "    '\\n',\n",
       "    '    return auth_session']},\n",
       "  {'id': 7,\n",
       "   'data': ['import kfp\\n',\n",
       "    '\\n',\n",
       "    'KUBEFLOW_ENDPOINT = \"http://localhost:8080\"\\n',\n",
       "    'KUBEFLOW_USERNAME = \"user@example.com\"\\n',\n",
       "    'KUBEFLOW_PASSWORD = \"12341234\"\\n',\n",
       "    '\\n',\n",
       "    'auth_session = get_istio_auth_session(\\n',\n",
       "    '    url=KUBEFLOW_ENDPOINT,\\n',\n",
       "    '    username=KUBEFLOW_USERNAME,\\n',\n",
       "    '    password=KUBEFLOW_PASSWORD\\n',\n",
       "    ')\\n',\n",
       "    '\\n',\n",
       "    'client = kfp.Client(host=f\"{KUBEFLOW_ENDPOINT}/pipeline\", cookies=auth_session[\"session_cookie\"])\\n',\n",
       "    '# print(client.list_experiments())']},\n",
       "  {'id': 10,\n",
       "   'data': ['@component(\\n',\n",
       "    '    base_image=\"python:3.10\",\\n',\n",
       "    '    packages_to_install=[\"pandas~=1.4.2\"],\\n',\n",
       "    \"    output_component_file='components/pull_data_component.yaml',\\n\",\n",
       "    ')\\n',\n",
       "    'def pull_data(url: str, data: Output[Dataset]):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Pull data component.\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    import pandas as pd\\n',\n",
       "    '\\n',\n",
       "    '    df = pd.read_csv(url, sep=\";\")\\n',\n",
       "    '    df.to_csv(data.path, index=None)']},\n",
       "  {'id': 12,\n",
       "   'data': ['@component(\\n',\n",
       "    '    base_image=\"python:3.10\",\\n',\n",
       "    '    packages_to_install=[\"pandas~=1.4.2\", \"scikit-learn~=1.0.2\"],\\n',\n",
       "    \"    output_component_file='components/preprocess_component.yaml',\\n\",\n",
       "    ')\\n',\n",
       "    'def preprocess(\\n',\n",
       "    '    data: Input[Dataset],\\n',\n",
       "    '    scaler_out: Output[Artifact],\\n',\n",
       "    '    train_set: Output[Dataset],\\n',\n",
       "    '    test_set: Output[Dataset],\\n',\n",
       "    '    target: str = \"quality\",\\n',\n",
       "    '):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Preprocess component.\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    import pandas as pd\\n',\n",
       "    '    import pickle\\n',\n",
       "    '    from sklearn.model_selection import train_test_split\\n',\n",
       "    '    from sklearn.preprocessing import StandardScaler\\n',\n",
       "    '\\n',\n",
       "    '    data = pd.read_csv(data.path)\\n',\n",
       "    '\\n',\n",
       "    '    # Split the data into training and test sets. (0.75, 0.25) split.\\n',\n",
       "    '    train, test = train_test_split(data)\\n',\n",
       "    '\\n',\n",
       "    '    scaler = StandardScaler()\\n',\n",
       "    '\\n',\n",
       "    '    train[train.drop(target, axis=1).columns] = scaler.fit_transform(train.drop(target, axis=1))\\n',\n",
       "    '    test[test.drop(target, axis=1).columns] = scaler.transform(test.drop(target, axis=1))\\n',\n",
       "    '\\n',\n",
       "    \"    with open(scaler_out.path, 'wb') as fp:\\n\",\n",
       "    '        pickle.dump(scaler, fp, pickle.HIGHEST_PROTOCOL)\\n',\n",
       "    '\\n',\n",
       "    '    train.to_csv(train_set.path, index=None)\\n',\n",
       "    '    test.to_csv(test_set.path, index=None)']},\n",
       "  {'id': 14,\n",
       "   'data': ['from typing import NamedTuple\\n',\n",
       "    '\\n',\n",
       "    '@component(\\n',\n",
       "    '    base_image=\"python:3.10\",\\n',\n",
       "    '    packages_to_install=[\"numpy\", \"pandas~=1.4.2\", \"scikit-learn~=1.0.2\", \"mlflow~=2.4.1\", \"boto3~=1.21.0\"],\\n',\n",
       "    \"    output_component_file='components/train_component.yaml',\\n\",\n",
       "    ')\\n',\n",
       "    'def train(\\n',\n",
       "    '    train_set: Input[Dataset],\\n',\n",
       "    '    test_set: Input[Dataset],\\n',\n",
       "    '    saved_model: Output[Model],\\n',\n",
       "    '    mlflow_experiment_name: str,\\n',\n",
       "    '    mlflow_tracking_uri: str,\\n',\n",
       "    '    mlflow_s3_endpoint_url: str,\\n',\n",
       "    '    model_name: str,\\n',\n",
       "    '    alpha: float,\\n',\n",
       "    '    l1_ratio: float,\\n',\n",
       "    '    target: str = \"quality\",\\n',\n",
       "    ') -> NamedTuple(\"Output\", [(\\'storage_uri\\', str), (\\'run_id\\', str),]):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Train component.\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    import numpy as np\\n',\n",
       "    '    import pandas as pd\\n',\n",
       "    '    from sklearn.linear_model import ElasticNet\\n',\n",
       "    '    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\n',\n",
       "    '    import mlflow\\n',\n",
       "    '    import mlflow.sklearn\\n',\n",
       "    '    import os\\n',\n",
       "    '    import logging\\n',\n",
       "    '    import pickle\\n',\n",
       "    '    from collections import namedtuple\\n',\n",
       "    '\\n',\n",
       "    '    logging.basicConfig(level=logging.INFO)\\n',\n",
       "    '    logger = logging.getLogger(__name__)\\n',\n",
       "    '\\n',\n",
       "    '    def eval_metrics(actual, pred):\\n',\n",
       "    '        rmse = np.sqrt(mean_squared_error(actual, pred))\\n',\n",
       "    '        mae = mean_absolute_error(actual, pred)\\n',\n",
       "    '        r2 = r2_score(actual, pred)\\n',\n",
       "    '        return rmse, mae, r2\\n',\n",
       "    '\\n',\n",
       "    \"    os.environ['MLFLOW_S3_ENDPOINT_URL'] = mlflow_s3_endpoint_url\\n\",\n",
       "    '\\n',\n",
       "    '    # load data\\n',\n",
       "    '    train = pd.read_csv(train_set.path)\\n',\n",
       "    '    test = pd.read_csv(test_set.path)\\n',\n",
       "    '\\n',\n",
       "    '    # The predicted column is \"quality\" which is a scalar from [3, 9]\\n',\n",
       "    '    train_x = train.drop([target], axis=1)\\n',\n",
       "    '    test_x = test.drop([target], axis=1)\\n',\n",
       "    '    train_y = train[[target]]\\n',\n",
       "    '    test_y = test[[target]]\\n',\n",
       "    '\\n',\n",
       "    '    logger.info(f\"Using MLflow tracking URI: {mlflow_tracking_uri}\")\\n',\n",
       "    '    mlflow.set_tracking_uri(mlflow_tracking_uri)\\n',\n",
       "    '\\n',\n",
       "    '    logger.info(f\"Using MLflow experiment: {mlflow_experiment_name}\")\\n',\n",
       "    '    mlflow.set_experiment(mlflow_experiment_name)\\n',\n",
       "    '\\n',\n",
       "    '    with mlflow.start_run() as run:\\n',\n",
       "    '\\n',\n",
       "    '        run_id = run.info.run_id\\n',\n",
       "    '        logger.info(f\"Run ID: {run_id}\")\\n',\n",
       "    '\\n',\n",
       "    '        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\\n',\n",
       "    '\\n',\n",
       "    '        logger.info(\"Fitting model...\")\\n',\n",
       "    '        model.fit(train_x, train_y)\\n',\n",
       "    '\\n',\n",
       "    '        logger.info(\"Predicting...\")\\n',\n",
       "    '        predicted_qualities = model.predict(test_x)\\n',\n",
       "    '\\n',\n",
       "    '        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\\n',\n",
       "    '\\n',\n",
       "    '        logger.info(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\\n',\n",
       "    '        logger.info(\"  RMSE: %s\" % rmse)\\n',\n",
       "    '        logger.info(\"  MAE: %s\" % mae)\\n',\n",
       "    '        logger.info(\"  R2: %s\" % r2)\\n',\n",
       "    '\\n',\n",
       "    '        logger.info(\"Logging parameters to MLflow\")\\n',\n",
       "    '        mlflow.log_param(\"alpha\", alpha)\\n',\n",
       "    '        mlflow.log_param(\"l1_ratio\", l1_ratio)\\n',\n",
       "    '        mlflow.log_metric(\"rmse\", rmse)\\n',\n",
       "    '        mlflow.log_metric(\"r2\", r2)\\n',\n",
       "    '        mlflow.log_metric(\"mae\", mae)\\n',\n",
       "    '\\n',\n",
       "    '        # save model to mlflow\\n',\n",
       "    '        logger.info(\"Logging trained model\")\\n',\n",
       "    '        mlflow.sklearn.log_model(\\n',\n",
       "    '            model,\\n',\n",
       "    '            model_name,\\n',\n",
       "    '            registered_model_name=\"ElasticnetWineModel\",\\n',\n",
       "    '            serialization_format=\"pickle\"\\n',\n",
       "    '        )\\n',\n",
       "    '\\n',\n",
       "    '        logger.info(\"Logging predictions artifact to MLflow\")\\n',\n",
       "    '        np.save(\"predictions.npy\", predicted_qualities)\\n',\n",
       "    '        mlflow.log_artifact(\\n',\n",
       "    '        local_path=\"predictions.npy\", artifact_path=\"predicted_qualities/\"\\n',\n",
       "    '        )\\n',\n",
       "    '\\n',\n",
       "    '        # save model as KFP artifact\\n',\n",
       "    '        logging.info(f\"Saving model to: {saved_model.path}\")\\n',\n",
       "    \"        with open(saved_model.path, 'wb') as fp:\\n\",\n",
       "    '            pickle.dump(model, fp, pickle.HIGHEST_PROTOCOL)\\n',\n",
       "    '\\n',\n",
       "    '        # prepare output\\n',\n",
       "    \"        output = namedtuple('Output', ['storage_uri', 'run_id'])\\n\",\n",
       "    '\\n',\n",
       "    '        # return str(mlflow.get_artifact_uri())\\n',\n",
       "    '        return output(mlflow.get_artifact_uri(), run_id)']},\n",
       "  {'id': 16,\n",
       "   'data': ['@component(\\n',\n",
       "    '    base_image=\"python:3.10\",\\n',\n",
       "    '    packages_to_install=[\"numpy\", \"mlflow~=2.4.1\"],\\n',\n",
       "    \"    output_component_file='components/evaluate_component.yaml',\\n\",\n",
       "    ')\\n',\n",
       "    'def evaluate(\\n',\n",
       "    '    run_id: str,\\n',\n",
       "    '    mlflow_tracking_uri: str,\\n',\n",
       "    '    threshold_metrics: dict\\n',\n",
       "    ') -> bool:\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Evaluate component: Compares metrics from training with given thresholds.\\n',\n",
       "    '\\n',\n",
       "    '    Args:\\n',\n",
       "    '        run_id (string):  MLflow run ID\\n',\n",
       "    '        mlflow_tracking_uri (string): MLflow tracking URI\\n',\n",
       "    '        threshold_metrics (dict): Minimum threshold values for each metric\\n',\n",
       "    '    Returns:\\n',\n",
       "    '        Bool indicating whether evaluation passed or failed.\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    from mlflow.tracking import MlflowClient\\n',\n",
       "    '    import logging\\n',\n",
       "    '\\n',\n",
       "    '    logging.basicConfig(level=logging.INFO)\\n',\n",
       "    '    logger = logging.getLogger(__name__)\\n',\n",
       "    '\\n',\n",
       "    '    client = MlflowClient(tracking_uri=mlflow_tracking_uri)\\n',\n",
       "    '    info = client.get_run(run_id)\\n',\n",
       "    '    training_metrics = info.data.metrics\\n',\n",
       "    '\\n',\n",
       "    '    logger.info(f\"Training metrics: {training_metrics}\")\\n',\n",
       "    '\\n',\n",
       "    '    # compare the evaluation metrics with the defined thresholds\\n',\n",
       "    '    for key, value in threshold_metrics.items():\\n',\n",
       "    '        if key not in training_metrics or training_metrics[key] > value:\\n',\n",
       "    '            logger.error(f\"Metric {key} failed. Evaluation not passed!\")\\n',\n",
       "    '            return False\\n',\n",
       "    '    return True']},\n",
       "  {'id': 18,\n",
       "   'data': ['@component(\\n',\n",
       "    '    base_image=\"python:3.9\",\\n',\n",
       "    '    packages_to_install=[\"kserve==0.11.0\"],\\n',\n",
       "    \"    output_component_file='components/deploy_model_component.yaml',\\n\",\n",
       "    ')\\n',\n",
       "    'def deploy_model(model_name: str, storage_uri: str):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Deploy the model as an inference service with Kserve.\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    import logging\\n',\n",
       "    '    from kubernetes import client\\n',\n",
       "    '    from kserve import KServeClient\\n',\n",
       "    '    from kserve import constants\\n',\n",
       "    '    from kserve import utils\\n',\n",
       "    '    from kserve import V1beta1InferenceService\\n',\n",
       "    '    from kserve import V1beta1InferenceServiceSpec\\n',\n",
       "    '    from kserve import V1beta1PredictorSpec\\n',\n",
       "    '    from kserve import V1beta1SKLearnSpec\\n',\n",
       "    '    from kubernetes.client import V1ResourceRequirements\\n',\n",
       "    '\\n',\n",
       "    '    logging.basicConfig(level=logging.INFO)\\n',\n",
       "    '    logger = logging.getLogger(__name__)\\n',\n",
       "    '\\n',\n",
       "    '    model_uri = f\"{storage_uri}/{model_name}\"\\n',\n",
       "    '    logger.info(f\"MODEL URI: {model_uri}\")\\n',\n",
       "    '\\n',\n",
       "    '    namespace = utils.get_default_target_namespace()\\n',\n",
       "    \"    kserve_version='v1beta1'\\n\",\n",
       "    \"    api_version = constants.KSERVE_GROUP + '/' + kserve_version\\n\",\n",
       "    '\\n',\n",
       "    '    isvc = V1beta1InferenceService(\\n',\n",
       "    '        api_version = api_version,\\n',\n",
       "    '        kind = constants.KSERVE_KIND,\\n',\n",
       "    '        metadata = client.V1ObjectMeta(\\n',\n",
       "    '            name = model_name,\\n',\n",
       "    '            namespace = namespace,\\n',\n",
       "    \"            annotations = {'sidecar.istio.io/inject':'false'}\\n\",\n",
       "    '        ),\\n',\n",
       "    '        spec = V1beta1InferenceServiceSpec(\\n',\n",
       "    '            predictor=V1beta1PredictorSpec(\\n',\n",
       "    '                service_account_name=\"kserve-sa\",\\n',\n",
       "    '                min_replicas=1,\\n',\n",
       "    '                max_replicas = 1,\\n',\n",
       "    '                sklearn=V1beta1SKLearnSpec(\\n',\n",
       "    '                    storage_uri=model_uri,\\n',\n",
       "    '                    resources=V1ResourceRequirements(\\n',\n",
       "    '                        requests={\"cpu\": \"100m\", \"memory\": \"512Mi\"},\\n',\n",
       "    '                        limits={\"cpu\": \"300m\", \"memory\": \"512Mi\"}\\n',\n",
       "    '                    )\\n',\n",
       "    '                ),\\n',\n",
       "    '            )\\n',\n",
       "    '        )\\n',\n",
       "    '    )\\n',\n",
       "    '    KServe = KServeClient()\\n',\n",
       "    '    KServe.create(isvc)']},\n",
       "  {'id': 20,\n",
       "   'data': ['@component(\\n',\n",
       "    '    base_image=\"python:3.9\",  # kserve on python 3.10 comes with a dependency that fails to get installed\\n',\n",
       "    '    packages_to_install=[\"kserve==0.11.0\", \"scikit-learn~=1.0.2\"],\\n',\n",
       "    \"    output_component_file='components/inference_component.yaml',\\n\",\n",
       "    ')\\n',\n",
       "    'def inference(\\n',\n",
       "    '    model_name: str,\\n',\n",
       "    '    scaler_in: Input[Artifact]\\n',\n",
       "    '):\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    Test inference.\\n',\n",
       "    '    \"\"\"\\n',\n",
       "    '    from kserve import KServeClient\\n',\n",
       "    '    import requests\\n',\n",
       "    '    import pickle\\n',\n",
       "    '    import logging\\n',\n",
       "    '    from kserve import utils\\n',\n",
       "    '    from urllib.parse import urlsplit\\n',\n",
       "    '    import re\\n',\n",
       "    '    \\n',\n",
       "    '    logging.basicConfig(level=logging.INFO)\\n',\n",
       "    '    logger = logging.getLogger(__name__)\\n',\n",
       "    '    \\n',\n",
       "    '    def get_istio_auth_session(url: str, username: str, password: str) -> dict:\\n',\n",
       "    '        \"\"\"\\n',\n",
       "    '        Determine if the specified URL is secured by Dex and try to obtain a session cookie.\\n',\n",
       "    '        WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\\n',\n",
       "    '                 (we default default to using `staticPasswords` if both are enabled)\\n',\n",
       "    '    \\n',\n",
       "    '        :param url: Kubeflow server URL, including protocol\\n',\n",
       "    '        :param username: Dex `staticPasswords` or `LDAP` username\\n',\n",
       "    '        :param password: Dex `staticPasswords` or `LDAP` password\\n',\n",
       "    '        :return: auth session information\\n',\n",
       "    '        \"\"\"\\n',\n",
       "    '        # define the default return object\\n',\n",
       "    '        auth_session = {\\n',\n",
       "    '            \"endpoint_url\": url,    # KF endpoint URL\\n',\n",
       "    '            \"redirect_url\": None,   # KF redirect URL, if applicable\\n',\n",
       "    '            \"dex_login_url\": None,  # Dex login URL (for POST of credentials)\\n',\n",
       "    '            \"is_secured\": None,     # True if KF endpoint is secured\\n',\n",
       "    '            \"session_cookie\": None  # Resulting session cookies in the form \"key1=value1; key2=value2\"\\n',\n",
       "    '        }\\n',\n",
       "    '    \\n',\n",
       "    '        # use a persistent session (for cookies)\\n',\n",
       "    '        with requests.Session() as s:\\n',\n",
       "    '    \\n',\n",
       "    '            ################\\n',\n",
       "    '            # Determine if Endpoint is Secured\\n',\n",
       "    '            ################\\n',\n",
       "    '            resp = s.get(url, allow_redirects=True)\\n',\n",
       "    '            if resp.status_code != 200:\\n',\n",
       "    '                raise RuntimeError(\\n',\n",
       "    '                    f\"HTTP status code \\'{resp.status_code}\\' for GET against: {url}\"\\n',\n",
       "    '                )\\n',\n",
       "    '    \\n',\n",
       "    '            auth_session[\"redirect_url\"] = resp.url\\n',\n",
       "    '    \\n',\n",
       "    '            # if we were NOT redirected, then the endpoint is UNSECURED\\n',\n",
       "    '            if len(resp.history) == 0:\\n',\n",
       "    '                auth_session[\"is_secured\"] = False\\n',\n",
       "    '                return auth_session\\n',\n",
       "    '            else:\\n',\n",
       "    '                auth_session[\"is_secured\"] = True\\n',\n",
       "    '    \\n',\n",
       "    '            ################\\n',\n",
       "    '            # Get Dex Login URL\\n',\n",
       "    '            ################\\n',\n",
       "    '            redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\\n',\n",
       "    '    \\n',\n",
       "    '            # if we are at `/auth?=xxxx` path, we need to select an auth type\\n',\n",
       "    '            if re.search(r\"/auth$\", redirect_url_obj.path):\\n',\n",
       "    '    \\n',\n",
       "    '                #######\\n',\n",
       "    '                # TIP: choose the default auth type by including ONE of the following\\n',\n",
       "    '                #######\\n',\n",
       "    '    \\n',\n",
       "    '                # OPTION 1: set \"staticPasswords\" as default auth type\\n',\n",
       "    '                redirect_url_obj = redirect_url_obj._replace(\\n',\n",
       "    '                    path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\\n',\n",
       "    '                )\\n',\n",
       "    '                # OPTION 2: set \"ldap\" as default auth type\\n',\n",
       "    '                # redirect_url_obj = redirect_url_obj._replace(\\n',\n",
       "    '                #     path=re.sub(r\"/auth$\", \"/auth/ldap\", redirect_url_obj.path)\\n',\n",
       "    '                # )\\n',\n",
       "    '    \\n',\n",
       "    '            # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST)\\n',\n",
       "    '            if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\\n',\n",
       "    '                auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\\n',\n",
       "    '    \\n',\n",
       "    '            # else, we need to be redirected to the actual login page\\n',\n",
       "    '            else:\\n',\n",
       "    '                # this GET should redirect us to the `/auth/xxxx/login` path\\n',\n",
       "    '                resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\\n',\n",
       "    '                if resp.status_code != 200:\\n',\n",
       "    '                    raise RuntimeError(\\n',\n",
       "    '                        f\"HTTP status code \\'{resp.status_code}\\' for GET against: {redirect_url_obj.geturl()}\"\\n',\n",
       "    '                    )\\n',\n",
       "    '    \\n',\n",
       "    '                # set the login url\\n',\n",
       "    '                auth_session[\"dex_login_url\"] = resp.url\\n',\n",
       "    '    \\n',\n",
       "    '            ################\\n',\n",
       "    '            # Attempt Dex Login\\n',\n",
       "    '            ################\\n',\n",
       "    '            resp = s.post(\\n',\n",
       "    '                auth_session[\"dex_login_url\"],\\n',\n",
       "    '                data={\"login\": username, \"password\": password},\\n',\n",
       "    '                allow_redirects=True\\n',\n",
       "    '            )\\n',\n",
       "    '            if len(resp.history) == 0:\\n',\n",
       "    '                raise RuntimeError(\\n',\n",
       "    '                    f\"Login credentials were probably invalid - \"\\n',\n",
       "    '                    f\"No redirect after POST to: {auth_session[\\'dex_login_url\\']}\"\\n',\n",
       "    '                )\\n',\n",
       "    '    \\n',\n",
       "    '            # store the session cookies in a \"key1=value1; key2=value2\" string\\n',\n",
       "    '            auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\\n',\n",
       "    '    \\n',\n",
       "    '        return auth_session\\n',\n",
       "    '    \\n',\n",
       "    '    KUBEFLOW_ENDPOINT = \"http://istio-ingressgateway.istio-system.svc.cluster.local:80\"\\n',\n",
       "    '    KUBEFLOW_USERNAME = \"user@example.com\"\\n',\n",
       "    '    KUBEFLOW_PASSWORD = \"12341234\"\\n',\n",
       "    '    \\n',\n",
       "    '    auth_session = get_istio_auth_session(\\n',\n",
       "    '    url=KUBEFLOW_ENDPOINT,\\n',\n",
       "    '    username=KUBEFLOW_USERNAME,\\n',\n",
       "    '    password=KUBEFLOW_PASSWORD,\\n',\n",
       "    '    )\\n',\n",
       "    '    TOKEN = auth_session[\"session_cookie\"].replace(\"authservice_session=\", \"\")\\n',\n",
       "    '    print(\"Token:\", TOKEN)\\n',\n",
       "    '\\n',\n",
       "    '    namespace = utils.get_default_target_namespace()\\n',\n",
       "    '\\n',\n",
       "    '    input_sample = [[5.6, 0.54, 0.04, 1.7, 0.049, 5, 13, 0.9942, 3.72, 0.58, 11.4],\\n',\n",
       "    '                    [11.3, 0.34, 0.45, 2, 0.082, 6, 15, 0.9988, 2.94, 0.66, 9.2]]\\n',\n",
       "    '\\n',\n",
       "    '    logger.info(f\"Loading standard scaler from: {scaler_in.path}\")\\n',\n",
       "    \"    with open(scaler_in.path, 'rb') as fp:\\n\",\n",
       "    '        scaler = pickle.load(fp)\\n',\n",
       "    '\\n',\n",
       "    '    logger.info(f\"Standardizing sample: {scaler_in.path}\")\\n',\n",
       "    '    input_sample = scaler.transform(input_sample)\\n',\n",
       "    '\\n',\n",
       "    '    # get inference service\\n',\n",
       "    '    KServe = KServeClient()\\n',\n",
       "    '\\n',\n",
       "    '    # wait for deployment to be ready\\n',\n",
       "    '    KServe.get(model_name, namespace=namespace, watch=True, timeout_seconds=120)\\n',\n",
       "    '\\n',\n",
       "    '    inference_service = KServe.get(model_name, namespace=namespace)\\n',\n",
       "    '    logger.info(f\"inference_service: {inference_service}\")\\n',\n",
       "    '\\n',\n",
       "    '    is_url = f\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/v1/models/{model_name}:predict\"\\n',\n",
       "    '    header = {\"Host\": f\"{model_name}.{namespace}.example.com\"}\\n',\n",
       "    '    \\n',\n",
       "    '    logger.info(f\"\\\\nInference service status:\\\\n{inference_service[\\'status\\']}\")\\n',\n",
       "    '    logger.info(f\"\\\\nInference service URL:\\\\n{is_url}\\\\n\")\\n',\n",
       "    '\\n',\n",
       "    '    inference_input = {\\n',\n",
       "    \"        'instances': input_sample.tolist()\\n\",\n",
       "    '    }\\n',\n",
       "    '    response = requests.post(\\n',\n",
       "    '        is_url,\\n',\n",
       "    '        json=inference_input,\\n',\n",
       "    '        headers=header,\\n',\n",
       "    '        cookies={\"authservice_session\": TOKEN}\\n',\n",
       "    '        \\n',\n",
       "    '    )\\n',\n",
       "    '    if response.status_code != 200:\\n',\n",
       "    '        raise RuntimeError(f\"HTTP status code \\'{response.status_code}\\': {response.json()}\")\\n',\n",
       "    '    \\n',\n",
       "    '    logger.info(f\"\\\\nPrediction response:\\\\n{response.json()}\\\\n\")']},\n",
       "  {'id': 22,\n",
       "   'data': ['@dsl.pipeline(\\n',\n",
       "    \"      name='demo-pipeline',\\n\",\n",
       "    \"      description='An example pipeline that performs addition calculations.',\\n\",\n",
       "    ')\\n',\n",
       "    'def pipeline(\\n',\n",
       "    '    url: str,\\n',\n",
       "    '    target: str,\\n',\n",
       "    '    mlflow_experiment_name: str,\\n',\n",
       "    '    mlflow_tracking_uri: str,\\n',\n",
       "    '    mlflow_s3_endpoint_url: str,\\n',\n",
       "    '    model_name: str,\\n',\n",
       "    '    alpha: float,\\n',\n",
       "    '    l1_ratio: float,\\n',\n",
       "    '    threshold_metrics: dict,\\n',\n",
       "    '):\\n',\n",
       "    '    pull_task = pull_data(url=url)\\n',\n",
       "    '\\n',\n",
       "    '    preprocess_task = preprocess(data=pull_task.outputs[\"data\"])\\n',\n",
       "    '\\n',\n",
       "    '    train_task = train(\\n',\n",
       "    '        train_set=preprocess_task.outputs[\"train_set\"],\\n',\n",
       "    '        test_set=preprocess_task.outputs[\"test_set\"],\\n',\n",
       "    '        target=target,\\n',\n",
       "    '        mlflow_experiment_name=mlflow_experiment_name,\\n',\n",
       "    '        mlflow_tracking_uri=mlflow_tracking_uri,\\n',\n",
       "    '        mlflow_s3_endpoint_url=mlflow_s3_endpoint_url,\\n',\n",
       "    '        model_name=model_name,\\n',\n",
       "    '        alpha=alpha,\\n',\n",
       "    '        l1_ratio=l1_ratio\\n',\n",
       "    '    )\\n',\n",
       "    '    train_task.apply(use_aws_secret(secret_name=\"aws-secret\"))\\n',\n",
       "    '\\n',\n",
       "    '    evaluate_trask = evaluate(\\n',\n",
       "    '        run_id=train_task.outputs[\"run_id\"],\\n',\n",
       "    '        mlflow_tracking_uri=mlflow_tracking_uri,\\n',\n",
       "    '        threshold_metrics=threshold_metrics\\n',\n",
       "    '    )\\n',\n",
       "    '\\n',\n",
       "    '    eval_passed = evaluate_trask.output\\n',\n",
       "    '\\n',\n",
       "    '    with dsl.Condition(eval_passed == \"true\"):\\n',\n",
       "    '        deploy_model_task = deploy_model(\\n',\n",
       "    '            model_name=model_name,\\n',\n",
       "    '            storage_uri=train_task.outputs[\"storage_uri\"],\\n',\n",
       "    '        )\\n',\n",
       "    '\\n',\n",
       "    '        inference_task = inference(\\n',\n",
       "    '            model_name=model_name,\\n',\n",
       "    '            scaler_in=preprocess_task.outputs[\"scaler_out\"]\\n',\n",
       "    '        )\\n',\n",
       "    '        inference_task.after(deploy_model_task)']},\n",
       "  {'id': 24,\n",
       "   'data': ['# Specify pipeline argument values\\n',\n",
       "    '\\n',\n",
       "    \"eval_threshold_metrics = {'rmse': 0.9, 'r2': 0.3, 'mae': 0.8}\\n\",\n",
       "    '\\n',\n",
       "    'arguments = {\\n',\n",
       "    '    \"url\": \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\\n',\n",
       "    '    \"target\": \"quality\",\\n',\n",
       "    '    \"mlflow_tracking_uri\": \"http://mlflow.mlflow.svc.cluster.local:5000\",\\n',\n",
       "    '    \"mlflow_s3_endpoint_url\": \"http://mlflow-minio-service.mlflow.svc.cluster.local:9000\",\\n',\n",
       "    '    \"mlflow_experiment_name\": \"demo-notebook\",\\n',\n",
       "    '    \"model_name\": \"wine-quality\",\\n',\n",
       "    '    \"alpha\": 0.5,\\n',\n",
       "    '    \"l1_ratio\": 0.5,\\n',\n",
       "    '    \"threshold_metrics\": eval_threshold_metrics\\n',\n",
       "    '}']},\n",
       "  {'id': 26,\n",
       "   'data': ['run_name = \"demo-run\"\\n',\n",
       "    'experiment_name = \"demo-experiment\"\\n',\n",
       "    '\\n',\n",
       "    'client.create_run_from_pipeline_func(\\n',\n",
       "    '    pipeline_func=pipeline,\\n',\n",
       "    '    run_name=run_name,\\n',\n",
       "    '    experiment_name=experiment_name,\\n',\n",
       "    '    arguments=arguments,\\n',\n",
       "    '    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\\n',\n",
       "    '    enable_caching=False,\\n',\n",
       "    '    namespace=\"kubeflow-user-example-com\"\\n',\n",
       "    ')']}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db308a73-4128-4279-a394-658d650444de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Demo KFP pipeline']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_documents['markdown'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f656b81-ba69-4d35-a5a0-8b661ec5e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_markdown = parse_markdown_into_text(\n",
    "    markdown_document = notebook_documents['markdown'][4]['data']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2f6d601-cbd1-4c7f-9915-40ec853acb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Components\n",
      "There are different ways to define components in KFP. Here, we use the @component decorator to define the components as Python function-based components.\n",
      "The @component annotation converts the function into a factory function that creates pipeline steps that execute this function. This example also specifies the base container image to run you component in.\n"
     ]
    }
   ],
   "source": [
    "print(parsed_markdown) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add15ea-03dc-4163-ab58-951bfcf54fd0",
   "metadata": {},
   "source": [
    "## Document storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03638983-8217-4182-9fee-b85dfe3500c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_document_url = 'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/backend/functions/platforms/celery.py'\n",
    "document_content = get_document(\n",
    "    document_url = used_document_url\n",
    ")\n",
    "code_documents = parse_python_code_chunks(\n",
    "    document_code = document_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5504034-9af8-4c85-813b-2413882b7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = mongo_setup_client(\n",
    "    username = 'mongo123',\n",
    "    password = 'mongo456',\n",
    "    address = '127.0.0.1',\n",
    "    port = '27017'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ddd6cf6-2957-455f-a36d-45a4ebf0b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_database_name = 'llm-rag-code-functions'\n",
    "document_collection_name = 'celery-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb1e6f84-46f4-4a4a-9765-e09e4e7a95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "created_collection = []\n",
    "collection_ids = {}\n",
    "index = 1\n",
    "for code_document in code_documents:\n",
    "    result = mongo_create_document(\n",
    "        mongo_client = mongo_client,\n",
    "        database_name = document_database_name,\n",
    "        collection_name = document_collection_name,\n",
    "        document = {\n",
    "            'data': code_document\n",
    "        }\n",
    "    )\n",
    "    collection_ids[str(index)] = str(result.inserted_id)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "705efacf-b156-49d7-b4b5-e75155591a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '66d184973fc211d7909f628d',\n",
       " '2': '66d184973fc211d7909f628e',\n",
       " '3': '66d184973fc211d7909f628f',\n",
       " '4': '66d184973fc211d7909f6290',\n",
       " '5': '66d184973fc211d7909f6291',\n",
       " '6': '66d184973fc211d7909f6292',\n",
       " '7': '66d184973fc211d7909f6293',\n",
       " '8': '66d184973fc211d7909f6294',\n",
       " '9': '66d184973fc211d7909f6295'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d008bae-2b4d-4f60-80f3-c526978a3b88",
   "metadata": {},
   "source": [
    "## RAG Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "917f851b-f0e9-4510-9bb6-0db034d39707",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_documents = mongo_list_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    database_name = document_database_name,\n",
    "    collection_name = document_collection_name,\n",
    "    filter_query = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a10f9-1951-4e7a-9881-f58587f7d54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032169e-963a-493c-b9cf-58bc6d4c5339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae9ba4-3e05-423f-b514-3088091cbf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bcea1-61c8-469e-9179-ab5869b137d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b39490e9-7d85-406e-a3fd-0b758bdfec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document_id = str(fetched_documents[0]['_id'])\n",
    "example_document = fetched_documents[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8782ba8e-af76-41e4-ac19-f8564966b6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66d184973fc211d7909f628d\n"
     ]
    }
   ],
   "source": [
    "print(example_document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c3bc6d4-8496-4051-bc62-a4a6c8a5b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import shutil\n",
      "code dependencies\n",
      "os.path.abspath\n",
      "os.path.exists\n",
      "shutil.rmtree\n",
      "os.makedirs\n",
      "open\n",
      "def setup_celery_logging():\n",
      "    log_directory = os.path.abspath('logs')\n",
      "    if os.path.exists(log_directory):\n",
      "        shutil.rmtree(log_directory)\n",
      "    os.makedirs(log_directory, exist_ok=True)\n",
      "    log_path = log_directory + '/backend.log'\n",
      "    with open(log_path, 'w') as f:\n",
      "        pass\n",
      "    return log_path\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e29115-8051-4ac8-a70d-5b1e7ab86fe0",
   "metadata": {},
   "source": [
    "## Generating Document Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f54f8a5-4041-4602-ad12-996872ebf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4da936a9-62da-48c3-8e98-67ade7fa59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language = Language.PYTHON, \n",
    "    chunk_size = 50, \n",
    "    chunk_overlap = 0\n",
    ")\n",
    "document_chunks = python_splitter.create_documents([example_document])\n",
    "document_chunks = [doc.page_content for doc in document_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dccc65b3-1d94-4b18-bef0-71dc9ccb1d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import os\\nimport shutil\\ncode dependencies',\n",
       " 'os.path.abspath\\nos.path.exists\\nshutil.rmtree',\n",
       " 'os.makedirs\\nopen',\n",
       " 'def setup_celery_logging():',\n",
       " \"log_directory = os.path.abspath('logs')\",\n",
       " 'if os.path.exists(log_directory):',\n",
       " 'shutil.rmtree(log_directory)',\n",
       " 'os.makedirs(log_directory, exist_ok=True)',\n",
       " \"log_path = log_directory + '/backend.log'\",\n",
       " \"with open(log_path, 'w') as f:\\n        pass\",\n",
       " 'return log_path']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1c12cd9-bf2d-4e61-a893-d89230882444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "180bda7f-6f4c-48eb-93af-22fc1cf6a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_embeddings = embedding_model.embed_documents(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2589b8f-9fe1-478b-a637-3b895d67d994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d199f-88d6-4161-a9c9-354a5cf84382",
   "metadata": {},
   "source": [
    "## Storing Document Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2826cec0-3416-40ae-ae3c-280a5ea0b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_collection_name = document_database_name + '-vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d043593-26b4-4b13-9bac-9c874a44bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "qdrant_client = qdrant_setup_client(\n",
    "    api_key = 'qdrant_key',\n",
    "    address = '127.0.0.1', \n",
    "    port = '6333'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2037c113-2799-475a-aef3-c7b08f950d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "created = qdrant_create_collection(\n",
    "    qdrant_client = qdrant_client,\n",
    "    collection_name = vector_collection_name,\n",
    "    configuration = VectorParams(\n",
    "          size = len(example_embeddings[0]), \n",
    "          distance = Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ef82832-c286-4e3c-bce3-d0b17ba5a8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac871fc9-ceb7-418e-b40f-9df998762e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qdrant_client.models import PointStruct\n",
    "example_document_points = []\n",
    "index = 0\n",
    "for doc_chunk in document_chunks:\n",
    "    chunk_point = PointStruct(\n",
    "        id = index, \n",
    "        vector = example_embeddings[index],\n",
    "        payload = {\n",
    "            'database': document_database_name,\n",
    "            'collection': document_collection_name,\n",
    "            'id': example_document_id, \n",
    "            'data': doc_chunk\n",
    "        }\n",
    "    )\n",
    "    example_document_points.append(chunk_point)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "364757cc-443f-474e-925d-08c272c44923",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_stored = qdrant_upsert_points(\n",
    "    qdrant_client = qdrant_client, \n",
    "    collection_name = vector_collection_name,\n",
    "    points = example_document_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dbb5de10-b8e9-4ee9-a32e-96a17197fea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9f557-3826-41d7-a90f-eeeb153692b9",
   "metadata": {},
   "source": [
    "## Generating Document Key Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2712a2be-0c60-43fe-b231-41b1b81e9f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66d184973fc211d7909f628d\n"
     ]
    }
   ],
   "source": [
    "print(example_document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6bb24abc-9ad3-46a4-9bd4-3ef1fe30a764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import os\\nimport shutil\\ncode dependencies\\nos.path.abspath\\nos.path.exists\\nshutil.rmtree\\nos.makedirs\\nopen\\ndef setup_celery_logging():\\n    log_directory = os.path.abspath('logs')\\n    if os.path.exists(log_directory):\\n        shutil.rmtree(log_directory)\\n    os.makedirs(log_directory, exist_ok=True)\\n    log_path = log_directory + '/backend.log'\\n    with open(log_path, 'w') as f:\\n        pass\\n    return log_path\\n\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2b612867-3293-42b6-a466-39aa2b4418a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sfniila/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f9bd4cb-4ec0-41be-9d79-f4a58444fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b53343dd-4604-464d-819d-30a450ae4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', 'os', 'import', 'shutil', 'code', 'dependencies', 'os.path.abspath', 'os.path.exists', 'shutil.rmtree', 'os.makedirs', 'open', 'def', 'setup_celery_logging', 'log_directory', 'os.path.abspath', \"'logs\", 'if', 'os.path.exists', 'log_directory', 'shutil.rmtree', 'log_directory', 'os.makedirs', 'log_directory', 'exist_ok=True', 'log_path', 'log_directory', \"'/backend.log\", 'with', 'open', 'log_path', 'as', 'pass', 'return', 'log_path']\n"
     ]
    }
   ],
   "source": [
    "# Lower, tokenize and remove '('\n",
    "text = example_document.lower()\n",
    "tokens = word_tokenize(example_document)\n",
    "tokens = [token for token in tokens if len(token) > 1]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f7f61014-8ad8-4109-a6ba-a19cdb46ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', 'os', 'import', 'shutil', 'code', 'depend', 'os.path.abspath', 'os.path.exist', 'shutil.rmtre', 'os.makedir', 'open', 'def', 'setup_celery_log', 'log_directori', 'os.path.abspath', \"'log\", 'os.path.exist', 'log_directori', 'shutil.rmtre', 'log_directori', 'os.makedir', 'log_directori', 'exist_ok=tru', 'log_path', 'log_directori', \"'/backend.log\", 'open', 'log_path', 'pass', 'return', 'log_path']\n"
     ]
    }
   ],
   "source": [
    "# Stem tokens and remove stop words\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f0585e3d-51a7-4e5e-bb6f-f3eb275c5087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', 'os', 'shutil', 'code', 'depend', 'os.path.abspath', 'os.path.exist', 'shutil.rmtre', 'os.makedir', 'open', 'def', 'setup_celery_log', 'log_directori', \"'log\", 'exist_ok=tru', 'log_path', \"'/backend.log\", 'pass', 'return']\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates while keeping the order\n",
    "tokens = list(dict.fromkeys(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "09982581-bfda-424d-8228-af505992744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_keywords = [tokens]\n",
    "keywords_stored = []\n",
    "for document_tokens in document_keywords:\n",
    "    keywords_stored.append({\n",
    "        'database': document_database_name,\n",
    "        'collection': document_collection_name,\n",
    "        'id': example_document_id, \n",
    "        'data': document_tokens\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cded4234-0d88-43bc-918b-a37979f7bc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'database': 'llm-rag-code-functions',\n",
       "  'collection': 'celery-py',\n",
       "  'id': '66d184973fc211d7909f628d',\n",
       "  'data': ['import',\n",
       "   'os',\n",
       "   'shutil',\n",
       "   'code',\n",
       "   'depend',\n",
       "   'os.path.abspath',\n",
       "   'os.path.exist',\n",
       "   'shutil.rmtre',\n",
       "   'os.makedir',\n",
       "   'open',\n",
       "   'def',\n",
       "   'setup_celery_log',\n",
       "   'log_directori',\n",
       "   \"'log\",\n",
       "   'exist_ok=tru',\n",
       "   'log_path',\n",
       "   \"'/backend.log\",\n",
       "   'pass',\n",
       "   'return']}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_stored "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003a11a-7e82-4b90-9b39-6679550f798f",
   "metadata": {},
   "source": [
    "## Storing Document Key Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eefbc0a1-1240-402f-a60b-f669557c2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meili_client = meili_setup_client(\n",
    "    host = 'http://127.0.0.1:7700', \n",
    "    api_key = 'meili_key'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7e2811ef-d4e6-4ec2-9ca4-6eef514aa48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_index_name = document_database_name + '-keywords'\n",
    "#keyword_index_name = keyword_index_name.replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0c99b1f9-c0ff-4797-bd37-99d6b1c7c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored = meili_add_documents(\n",
    "    meili_client = meili_client,\n",
    "    index_name = keyword_index_name,\n",
    "    documents = keywords_stored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7cf922f7-30f7-4615-9aee-596a3ba49092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=2, index_uid='llm-rag-code-functions-keywords', status='enqueued', type='documentAdditionOrUpdate', enqueued_at=datetime.datetime(2024, 8, 30, 10, 1, 28, 842892))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82446f35-aea4-4f5e-83ed-4a0e2a4dcc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea72d33-28b4-4a4b-8a28-5da9a30887a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86153c09-4170-4bee-8680-b36a5c13afd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eecb08e-c89b-46c3-b056-a411b8451dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78c1ad43-629a-47c1-b7b3-7e76739b1444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport shutil\\ncode dependencies'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b06bb13-d746-4f92-8ea1-c53d84168174",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = qdrant_search_vectors(\n",
    "    qdrant_client = qdrant_client,  \n",
    "    collection_name = 'llm-rag-vectors',\n",
    "    query_vector = example_embeddings[0],\n",
    "    limit = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cfe4354f-3135-4115-b8a9-fefc3e8fa959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=1, version=0, score=1.0, payload={'data': 'import os\\nimport shutil\\ncode dependencies', 'order': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=2, version=0, score=0.32488102, payload={'data': 'os.path.abspath\\nos.path.exists\\nshutil.rmtree', 'order': 2}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3, version=0, score=0.2667464, payload={'data': 'os.makedirs\\nopen', 'order': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=7, version=0, score=0.22600092, payload={'data': 'shutil.rmtree(log_directory)', 'order': 7}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=8, version=0, score=0.20714475, payload={'data': 'os.makedirs(log_directory, exist_ok=True)', 'order': 8}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=6, version=0, score=0.16657859, payload={'data': 'if os.path.exists(log_directory):', 'order': 6}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=4, version=0, score=0.15950751, payload={'data': 'def setup_celery_logging():', 'order': 4}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=5, version=0, score=0.14061333, payload={'data': \"log_directory = os.path.abspath('logs')\", 'order': 5}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=10, version=0, score=0.1386426, payload={'data': \"with open(log_path, 'w') as f:\\n        pass\", 'order': 10}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=9, version=0, score=0.06527254, payload={'data': \"log_path = log_directory + '/backend.log'\", 'order': 9}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=11, version=0, score=0.012641182, payload={'data': 'return log_path', 'order': 11}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6e85450-d69a-4eaa-acb2-d461e49054f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'return log_path'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_text[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3525a301-acdd-4044-bd84-f292971939a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = qdrant_search_vectors(\n",
    "    qdrant_client = qdrant_client,  \n",
    "    collection_name = 'llm-rag-vectors',\n",
    "    query_vector = example_embeddings[-1],\n",
    "    limit = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d2002227-e880-43c5-ae46-e7b0d1d78585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=11, version=0, score=1.0000001, payload={'data': 'return log_path'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=9, version=0, score=0.77037394, payload={'data': \"log_path = log_directory + '/backend.log'\"}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=10, version=0, score=0.69867, payload={'data': \"with open(log_path, 'w') as f:\\n        pass\"}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352937ba-47b0-4c77-a94e-1facb831179d",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a810c0a-4443-40aa-9d09-495e7e456581",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_chunks = mongo_list_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    database_name = 'llm-rag-chunks',\n",
    "    collection_name = 'celery-py',\n",
    "    filter_query = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e09cd53-9566-49ac-988a-5e2e12e7e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_chunk = fetched_chunks[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd45e86-bf3d-4999-b98d-3477af015ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import shutil\n",
      "code dependencies\n",
      "os.path.abspath\n",
      "os.path.exists\n",
      "shutil.rmtree\n",
      "os.makedirs\n",
      "open\n",
      "def setup_celery_logging():\n",
      "    log_directory = os.path.abspath('logs')\n",
      "    if os.path.exists(log_directory):\n",
      "        shutil.rmtree(log_directory)\n",
      "    os.makedirs(log_directory, exist_ok=True)\n",
      "    log_path = log_directory + '/backend.log'\n",
      "    with open(log_path, 'w') as f:\n",
      "        pass\n",
      "    return log_path\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f872cf0-ef5e-4a6b-b350-2da8297ce058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bb36399-00e9-495b-b718-fc8aa11d981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language = Language.PYTHON, \n",
    "    chunk_size = 50, \n",
    "    chunk_overlap = 0\n",
    ")\n",
    "smaller_chunks = python_splitter.create_documents([example_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a00a91fa-67fe-4d6e-966f-fd24a1becfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport shutil\\ncode dependencies'),\n",
       " Document(page_content='os.path.abspath\\nos.path.exists\\nshutil.rmtree'),\n",
       " Document(page_content='os.makedirs\\nopen'),\n",
       " Document(page_content='def setup_celery_logging():'),\n",
       " Document(page_content=\"log_directory = os.path.abspath('logs')\"),\n",
       " Document(page_content='if os.path.exists(log_directory):'),\n",
       " Document(page_content='shutil.rmtree(log_directory)'),\n",
       " Document(page_content='os.makedirs(log_directory, exist_ok=True)'),\n",
       " Document(page_content=\"log_path = log_directory + '/backend.log'\"),\n",
       " Document(page_content=\"with open(log_path, 'w') as f:\\n        pass\"),\n",
       " Document(page_content='return log_path')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb387cf-ab51-4450-bd2b-5050561c04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "914e7758-2b73-474a-86f3-378a46a20c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103f5808-a057-431f-9e6e-bf3bb7c09b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    host = \"127.0.0.1\", \n",
    "    port = 6333\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95bdcbef-7f53-47b8-8c28-be344536352d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0badb232-5ebc-4beb-be40-962dca79b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "if not qdrant_client.collection_exists(\"llm-rag-vectors\"):\n",
    "   qdrant_client.create_collection(\n",
    "      collection_name=\"llm-rag-vectors\",\n",
    "      vectors_config=VectorParams(\n",
    "          size=100, \n",
    "          distance=Distance.COSINE\n",
    "      ),\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab784a6d-d1d0-414c-9284-24f71a57fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qdrant_client.models import PointStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff8bc5a-3f22-40b2-a8af-a674dec7ed08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.random.rand(100, 100)\n",
    "qdrant_client.upsert(\n",
    "   collection_name=\"llm-rag-vectors\",\n",
    "   points=[\n",
    "      PointStruct(\n",
    "            id=idx,\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"color\": \"red\", \"rand_number\": idx % 10}\n",
    "      )\n",
    "      for idx, vector in enumerate(vectors)\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d44c82f-e6c1-4599-848f-b201363283d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = np.random.rand(100)\n",
    "hits = qdrant_client.search(\n",
    "   collection_name=\"llm-rag-vectors\",\n",
    "   query_vector=query_vector,\n",
    "   limit=5  # Return 5 closest points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a185cb70-2486-4f8f-9458-ca3478a0b47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=41, version=0, score=0.81860214, payload={'color': 'red', 'rand_number': 1}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=0, version=0, score=0.8181431, payload={'color': 'red', 'rand_number': 0}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=33, version=0, score=0.81126696, payload={'color': 'red', 'rand_number': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=13, version=0, score=0.8024623, payload={'color': 'red', 'rand_number': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=67, version=0, score=0.7998194, payload={'color': 'red', 'rand_number': 7}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00603ea1-0480-4169-aa92-53fe8cb94be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Filter, FieldCondition, Range\n",
    "hits = qdrant_client.search(\n",
    "   collection_name=\"llm-rag-vectors\",\n",
    "   query_vector=query_vector,\n",
    "   query_filter=Filter(\n",
    "      must=[  # These conditions are required for search results\n",
    "            FieldCondition(\n",
    "               key='rand_number',  # Condition based on values of `rand_number` field.\n",
    "               range=Range(\n",
    "                  gte=3  # Select only those results where `rand_number` >= 3\n",
    "               )\n",
    "            )\n",
    "      ]\n",
    "   ),\n",
    "   limit=5  # Return 5 closest points\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5622a3b-4ca6-46c1-8e8f-c62d5813ea14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=33, version=0, score=0.81126696, payload={'color': 'red', 'rand_number': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=13, version=0, score=0.8024623, payload={'color': 'red', 'rand_number': 3}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=67, version=0, score=0.7998194, payload={'color': 'red', 'rand_number': 7}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=34, version=0, score=0.79686064, payload={'color': 'red', 'rand_number': 4}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=36, version=0, score=0.7856212, payload={'color': 'red', 'rand_number': 6}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f7d7b23a-fc5c-40c3-a8e0-3454a820c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "651ad224-6852-4756-90cf-b3617ae6422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_storage_client = pymongo.MongoClient(\n",
    "    'mongodb://mongo123:mongo456@127.0.0.1:27017/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ea419ac7-49ba-42b5-ac35-c050a6c324cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local']\n"
     ]
    }
   ],
   "source": [
    "print(document_storage_client.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "918a6060-363a-462d-a14b-c0cfedfce377",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = document_storage_client['llm-rag-chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "93eabe95-01ed-454c-ae81-069fed2ab133",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = database['celery-py'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "584cbe45-723e-4b24-9f73-d7ddd66e5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_index = 1\n",
    "created_collection = []\n",
    "for code_chunk in code_chunks:\n",
    "    created_collection.append(\n",
    "        {'data': code_chunk}\n",
    "    )\n",
    "    collection_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f661ad59-ec7e-47e6-9199-c874f81e8691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('66d06efb48c12d9454c2bbdc'), ObjectId('66d06efb48c12d9454c2bbdd'), ObjectId('66d06efb48c12d9454c2bbde'), ObjectId('66d06efb48c12d9454c2bbdf'), ObjectId('66d06efb48c12d9454c2bbe0'), ObjectId('66d06efb48c12d9454c2bbe1'), ObjectId('66d06efb48c12d9454c2bbe2'), ObjectId('66d06efb48c12d9454c2bbe3'), ObjectId('66d06efb48c12d9454c2bbe4')], acknowledged=True)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.insert_many(created_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2ad13a79-096c-49e0-921a-3a0c38044ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import shutil\n",
      "code dependencies\n",
      "os.path.abspath\n",
      "os.path.exists\n",
      "shutil.rmtree\n",
      "os.makedirs\n",
      "open\n",
      "def setup_celery_logging():\n",
      "    log_directory = os.path.abspath('logs')\n",
      "    if os.path.exists(log_directory):\n",
      "        shutil.rmtree(log_directory)\n",
      "    os.makedirs(log_directory, exist_ok=True)\n",
      "    log_path = log_directory + '/backend.log'\n",
      "    with open(log_path, 'w') as f:\n",
      "        pass\n",
      "    return log_path\n",
      "\n",
      "import os\n",
      "code dependencies\n",
      "os.path.abspath\n",
      "open\n",
      "listed_logs['logs'].append\n",
      "line.strip\n",
      "def get_celery_logs(): \n",
      "    log_path = os.path.abspath('logs/backend.log')\n",
      "    listed_logs = {'logs':[]}\n",
      "    with open(log_path, 'r') as f:\n",
      "        for line in f:\n",
      "            listed_logs['logs'].append(line.strip())\n",
      "    return listed_logs\n",
      "\n",
      "import os\n",
      "from celery import Celery\n",
      "code dependencies\n",
      "os.environ.get\n",
      "os.environ.get\n",
      "os.environ.get\n",
      "str\n",
      "str\n",
      "Celery\n",
      "def get_celery_instance():\n",
      "    redis_endpoint = os.environ.get('REDIS_ENDPOINT')\n",
      "    redis_port = os.environ.get('REDIS_PORT')\n",
      "    redis_db = os.environ.get('REDIS_DB')\n",
      "    name = 'tasks'\n",
      "    redis_connection = 'redis://' + redis_endpoint + ':' + str(redis_port) + '/' + str(redis_db)\n",
      "    celery_app = Celery(\n",
      "        main = name,\n",
      "        broker = redis_connection,\n",
      "        backend = redis_connection\n",
      "    )\n",
      "    celery_app.conf.broker_connection_retry_on_startup = True\n",
      "    return celery_app\n",
      "\n",
      "from celery.result import AsyncResult\n",
      "code dependencies\n",
      "AsyncResult\n",
      "def check_task_status(\n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    response = AsyncResult(\n",
      "        id = task_id, \n",
      "        app = celery_client\n",
      "    )\n",
      "    return response.state\n",
      "\n",
      "from celery.result import AsyncResult\n",
      "code dependencies\n",
      "AsyncResult\n",
      "def get_task_result(\n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    response = AsyncResult(\n",
      "        id = task_id, \n",
      "        app = celery_client\n",
      "    )\n",
      "    return response.result\n",
      "\n",
      "from ice import check_task_status\n",
      "from ice import get_task_result\n",
      "code dependencies\n",
      "check_task_status\n",
      "get_task_result\n",
      "def get_task( \n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    task_data = {\n",
      "        'status': '',\n",
      "        'result': None\n",
      "    }\n",
      "    task_status = check_task_status(\n",
      "        celery_client = celery_client,\n",
      "        task_id = task_id\n",
      "    )\n",
      "    task_data['status'] = task_status\n",
      "    if task_status == 'SUCCESS':\n",
      "        task_result = get_task_result(\n",
      "            celery_client = celery_client,\n",
      "            task_id = task_id\n",
      "        )\n",
      "        task_data['result'] = task_result\n",
      "    return task_data\n",
      "\n",
      "import time\n",
      "from ice import get_task\n",
      "code dependencies\n",
      "time.time\n",
      "time.time\n",
      "get_task\n",
      "time.sleep\n",
      "def await_task(\n",
      "    celery_client: any,\n",
      "    task_id: str,\n",
      "    timeout: int\n",
      ") -> any:\n",
      "    task_data = {}\n",
      "    start = time.time()\n",
      "    while time.time() - start <= timeout:\n",
      "        task_data = get_task(\n",
      "            celery_client = celery_client,\n",
      "            task_id = task_id\n",
      "        )\n",
      "        if not task_data['result'] is None:\n",
      "            break\n",
      "        time.sleep(2)\n",
      "    return task_data\n",
      "\n",
      "from celery import signature\n",
      "code dependencies\n",
      "len\n",
      "signature\n",
      "signature\n",
      "task.apply_async\n",
      "def get_signature_id(\n",
      "    task_name: str,\n",
      "    task_kwargs: any\n",
      ") -> str:\n",
      "    task = None\n",
      "    if 0 < len(task_kwargs):\n",
      "        task = signature(task_name, kwargs = task_kwargs)\n",
      "    else: \n",
      "        task = signature(task_name)\n",
      "    celery_task = task.apply_async()\n",
      "    return celery_task.id\n",
      "\n",
      "from ice import get_signature_id\n",
      "from ice import await_task\n",
      "code dependencies\n",
      "get_signature_id\n",
      "await_task\n",
      "def await_signature(\n",
      "    celery_client: any,\n",
      "    task_name: str,\n",
      "    task_kwargs: any,\n",
      "    timeout: int\n",
      ") -> any:\n",
      "    task_id = get_signature_id(\n",
      "        task_name = task_name,\n",
      "        task_kwargs = task_kwargs\n",
      "    )\n",
      "    return await_task(\n",
      "        celery_client = celery_client,\n",
      "        task_id = task_id,\n",
      "        timeout = timeout\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in collection.find():\n",
    "    print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aeabd86f-2a1b-47da-8ab9-0d2ac75b65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/applications/article/submitter/backend/functions/platforms/celery.py'\n",
    "response = requests.get(test_url)\n",
    "response.raise_for_status()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab64fe28-2f8c-4fd8-92b7-511fbd1fc9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "PY_LANGUAGE = Language(tspython.language())\n",
    "parser = Parser(PY_LANGUAGE)\n",
    "code = response.text\n",
    "tree = parser.parse(\n",
    "    bytes(code,\"utf8\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d03204-05fc-468c-9466-255634995e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def setup_celery_logging():\n",
      "    log_directory = os.path.abspath('logs')\n",
      "    \n",
      "    if os.path.exists(log_directory):\n",
      "        shutil.rmtree(log_directory)\n",
      "    \n",
      "    os.makedirs(log_directory, exist_ok=True)\n",
      "    log_path = log_directory + '/backend.log'\n",
      "    with open(log_path, 'w') as f:\n",
      "        pass\n",
      "\n",
      "    return log_path\n",
      "\n",
      "def get_celery_logs(): \n",
      "    log_path = os.path.abspath('logs/backend.log')\n",
      "    listed_logs = {'logs':[]}\n",
      "    with open(log_path, 'r') as f:\n",
      "        for line in f:\n",
      "            listed_logs['logs'].append(line.strip())\n",
      "    return listed_logs\n",
      "\n",
      "def get_celery_instance():\n",
      "    redis_endpoint = os.environ.get('REDIS_ENDPOINT')\n",
      "    redis_port = os.environ.get('REDIS_PORT')\n",
      "    redis_db = os.environ.get('REDIS_DB')\n",
      "    \n",
      "    name = 'tasks'\n",
      "    redis_connection = 'redis://' + redis_endpoint + ':' + str(redis_port) + '/' + str(redis_db)\n",
      "\n",
      "    celery_app = Celery(\n",
      "        main = name,\n",
      "        broker = redis_connection,\n",
      "        backend = redis_connection\n",
      "    )\n",
      "\n",
      "    celery_app.conf.broker_connection_retry_on_startup = True\n",
      "\n",
      "    return celery_app\n",
      "\n",
      "def check_task_status(\n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    response = AsyncResult(\n",
      "        id = task_id, \n",
      "        app = celery_client\n",
      "    )\n",
      "    return response.state\n",
      "\n",
      "def get_task_result(\n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    response = AsyncResult(\n",
      "        id = task_id, \n",
      "        app = celery_client\n",
      "    )\n",
      "    return response.result\n",
      "\n",
      "def get_task( \n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    task_data = {\n",
      "        'status': '',\n",
      "        'result': None\n",
      "    }\n",
      "    task_status = check_task_status(\n",
      "        celery_client = celery_client,\n",
      "        task_id = task_id\n",
      "    )\n",
      "    task_data['status'] = task_status\n",
      "    if task_status == 'SUCCESS':\n",
      "        task_result = get_task_result(\n",
      "            celery_client = celery_client,\n",
      "            task_id = task_id\n",
      "        )\n",
      "        task_data['result'] = task_result\n",
      "    return task_data\n",
      "\n",
      "def await_task(\n",
      "    celery_client: any,\n",
      "    task_id: str,\n",
      "    timeout: int\n",
      ") -> any:\n",
      "    task_data = {}\n",
      "    start = time.time()\n",
      "    # In the case of errors this\n",
      "    # makes wait the whole timeout\n",
      "    while time.time() - start <= timeout:\n",
      "        task_data = get_task(\n",
      "            celery_client = celery_client,\n",
      "            task_id = task_id\n",
      "        )\n",
      "        if not task_data['result'] is None:\n",
      "            break\n",
      "        time.sleep(2)\n",
      "    return task_data\n",
      "\n",
      "def get_signature_id(\n",
      "    task_name: str,\n",
      "    task_kwargs: any\n",
      ") -> str:\n",
      "    task = None\n",
      "    if 0 < len(task_kwargs):\n",
      "        task = signature(task_name, kwargs = task_kwargs)\n",
      "    else: \n",
      "        task = signature(task_name)\n",
      "    celery_task = task.apply_async()\n",
      "    return celery_task.id\n",
      "\n",
      "def await_signature(\n",
      "    celery_client: any,\n",
      "    task_name: str,\n",
      "    task_kwargs: any,\n",
      "    timeout: int\n",
      ") -> any:\n",
      "    task_id = get_signature_id(\n",
      "        task_name = task_name,\n",
      "        task_kwargs = task_kwargs\n",
      "    )\n",
      "    return await_task(\n",
      "        celery_client = celery_client,\n",
      "        task_id = task_id,\n",
      "        timeout = timeout\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_node = tree.root_node\n",
    "\n",
    "def extract_functions(node, code_text):\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        functions.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        functions.extend(extract_functions(child, code_text))\n",
    "    return functions\n",
    "\n",
    "functions = extract_functions(root_node, bytes(code, 'utf8'))\n",
    "\n",
    "for func in functions:\n",
    "    print(func)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc501596-2457-4433-b9b3-dda505ac9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imports(node, code_text):\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def extract_dependencies(node, code_text):\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "    \n",
    "def extract_functions_and_dependencies(node, code_text):\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(extract_functions_and_dependencies(child, code_text))\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73bbb6c3-8b59-4481-9e76-d9086429885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6385458e-0a85-42a8-8063-90e3a5029225",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_imports = extract_imports(root_node, bytes(code, 'utf8'))\n",
    "code_functions = extract_functions_and_dependencies(root_node, bytes(code, 'utf8'))\n",
    "\n",
    "document_chunks = []\n",
    "for item in code_functions:\n",
    "\n",
    "    used_imports = get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document_chunk = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': item['name'],\n",
    "        'dependencies': item['dependencies'],\n",
    "        'code': item['code']\n",
    "    }\n",
    "    \n",
    "    document_chunks.append(document_chunk)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "af215e98-367c-4e1c-b098-dea167886533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import shutil\n",
      "code dependencies\n",
      "os.path.abspath\n",
      "os.path.exists\n",
      "shutil.rmtree\n",
      "os.makedirs\n",
      "open\n",
      "def setup_celery_logging():\n",
      "    log_directory = os.path.abspath('logs')\n",
      "    if os.path.exists(log_directory):\n",
      "        shutil.rmtree(log_directory)\n",
      "    os.makedirs(log_directory, exist_ok=True)\n",
      "    log_path = log_directory + '/backend.log'\n",
      "    with open(log_path, 'w') as f:\n",
      "        pass\n",
      "    return log_path\n",
      "\n",
      "import os\n",
      "code dependencies\n",
      "os.path.abspath\n",
      "open\n",
      "listed_logs['logs'].append\n",
      "line.strip\n",
      "def get_celery_logs(): \n",
      "    log_path = os.path.abspath('logs/backend.log')\n",
      "    listed_logs = {'logs':[]}\n",
      "    with open(log_path, 'r') as f:\n",
      "        for line in f:\n",
      "            listed_logs['logs'].append(line.strip())\n",
      "    return listed_logs\n",
      "\n",
      "import os\n",
      "from celery import Celery\n",
      "code dependencies\n",
      "os.environ.get\n",
      "os.environ.get\n",
      "os.environ.get\n",
      "str\n",
      "str\n",
      "Celery\n",
      "def get_celery_instance():\n",
      "    redis_endpoint = os.environ.get('REDIS_ENDPOINT')\n",
      "    redis_port = os.environ.get('REDIS_PORT')\n",
      "    redis_db = os.environ.get('REDIS_DB')\n",
      "    name = 'tasks'\n",
      "    redis_connection = 'redis://' + redis_endpoint + ':' + str(redis_port) + '/' + str(redis_db)\n",
      "    celery_app = Celery(\n",
      "        main = name,\n",
      "        broker = redis_connection,\n",
      "        backend = redis_connection\n",
      "    )\n",
      "    celery_app.conf.broker_connection_retry_on_startup = True\n",
      "    return celery_app\n",
      "\n",
      "from celery.result import AsyncResult\n",
      "code dependencies\n",
      "AsyncResult\n",
      "def check_task_status(\n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    response = AsyncResult(\n",
      "        id = task_id, \n",
      "        app = celery_client\n",
      "    )\n",
      "    return response.state\n",
      "\n",
      "from celery.result import AsyncResult\n",
      "code dependencies\n",
      "AsyncResult\n",
      "def get_task_result(\n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    response = AsyncResult(\n",
      "        id = task_id, \n",
      "        app = celery_client\n",
      "    )\n",
      "    return response.result\n",
      "\n",
      "from ice import check_task_status\n",
      "from ice import get_task_result\n",
      "code dependencies\n",
      "check_task_status\n",
      "get_task_result\n",
      "def get_task( \n",
      "    celery_client: any,\n",
      "    task_id: str\n",
      ") -> any:\n",
      "    task_data = {\n",
      "        'status': '',\n",
      "        'result': None\n",
      "    }\n",
      "    task_status = check_task_status(\n",
      "        celery_client = celery_client,\n",
      "        task_id = task_id\n",
      "    )\n",
      "    task_data['status'] = task_status\n",
      "    if task_status == 'SUCCESS':\n",
      "        task_result = get_task_result(\n",
      "            celery_client = celery_client,\n",
      "            task_id = task_id\n",
      "        )\n",
      "        task_data['result'] = task_result\n",
      "    return task_data\n",
      "\n",
      "import time\n",
      "from ice import get_task\n",
      "code dependencies\n",
      "time.time\n",
      "time.time\n",
      "get_task\n",
      "time.sleep\n",
      "def await_task(\n",
      "    celery_client: any,\n",
      "    task_id: str,\n",
      "    timeout: int\n",
      ") -> any:\n",
      "    task_data = {}\n",
      "    start = time.time()\n",
      "    while time.time() - start <= timeout:\n",
      "        task_data = get_task(\n",
      "            celery_client = celery_client,\n",
      "            task_id = task_id\n",
      "        )\n",
      "        if not task_data['result'] is None:\n",
      "            break\n",
      "        time.sleep(2)\n",
      "    return task_data\n",
      "\n",
      "from celery import signature\n",
      "code dependencies\n",
      "len\n",
      "signature\n",
      "signature\n",
      "task.apply_async\n",
      "def get_signature_id(\n",
      "    task_name: str,\n",
      "    task_kwargs: any\n",
      ") -> str:\n",
      "    task = None\n",
      "    if 0 < len(task_kwargs):\n",
      "        task = signature(task_name, kwargs = task_kwargs)\n",
      "    else: \n",
      "        task = signature(task_name)\n",
      "    celery_task = task.apply_async()\n",
      "    return celery_task.id\n",
      "\n",
      "from ice import get_signature_id\n",
      "from ice import await_task\n",
      "code dependencies\n",
      "get_signature_id\n",
      "await_task\n",
      "def await_signature(\n",
      "    celery_client: any,\n",
      "    task_name: str,\n",
      "    task_kwargs: any,\n",
      "    timeout: int\n",
      ") -> any:\n",
      "    task_id = get_signature_id(\n",
      "        task_name = task_name,\n",
      "        task_kwargs = task_kwargs\n",
      "    )\n",
      "    return await_task(\n",
      "        celery_client = celery_client,\n",
      "        task_id = task_id,\n",
      "        timeout = timeout\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in document_chunks:\n",
    "    formatted_chunk = ''\n",
    "\n",
    "    for chunk_import in chunk['imports']:\n",
    "        formatted_chunk += chunk_import + '\\n'\n",
    "\n",
    "    for chunk_functions in chunk['functions']:\n",
    "        formatted_chunk += chunk_functions + '\\n'\n",
    "\n",
    "    formatted_chunk += 'code dependencies\\n'\n",
    "\n",
    "    for chunk_dependency in chunk['dependencies']:\n",
    "        formatted_chunk += chunk_dependency + '\\n'\n",
    "    \n",
    "    for line in chunk['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        parsed_code = re.sub(r'#.*','', line)\n",
    "        if not bool(parsed_code.strip()):\n",
    "            continue\n",
    "        formatted_chunk += parsed_code + '\\n'\n",
    "        \n",
    "    print(formatted_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9f27555-e45e-4f75-8027-6231bd067093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_used_imports(\n",
    "    code_imports: any,\n",
    "    relevant_imports: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in code_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in code_functions[0]['dependencies']:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca36b7d6-c4af-49d6-8a50-0a5a8719f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/README.md'\n",
    "response = requests.get(test_url)\n",
    "response.raise_for_status()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfa08ee6-b996-45b0-9b5c-39287ff05a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cloud-HPC integreated OSS MLOps Platform\n",
      "\n",
      "Welcome to the OSS MLOps Platform, a comprehensive suite designed to streamline your machine learning operations from experimentation to deployment. \n",
      "\n",
      "![logos.png](resources/img/logos.png)\n",
      "\n",
      "This fork provides documentation, applications and notebooks on how to enable the OSS platform run in a cloud virtual machine to utilize the Ray computing framework run in a supercomputer. \n",
      "\n",
      "The utilized and intended use enviroment is the CSC infrastructure ecosystem with the tested platforms being [CPouta](https://docs.csc.fi/cloud/pouta/) cloud platform, [Allas](https://docs.csc.fi/data/Allas/) object storage platform and [Mahti](https://docs.csc.fi/computing/) supercomputer platform.\n",
      "\n",
      "A more indepth explanation for the implemented thesis code and the initial ideas of the article code are found in a master's thesis ['On Integrating Cloud and High Performance Computing Enviroments in Machine Learning Operations'](https://helda.helsinki.fi/items/8b6cc75b-43a9-43e8-bd26-c8f1914cee34).\n",
      "\n",
      "## Overview of Project Structure\n",
      "\n",
      "- **Applications**\n",
      "  - [`Forwarder`](applications/article/forwarder): Self-implemented component that enables cloud-local interactions\n",
      "  - [`Submitter`](applications/article/submitter): Self-implemented component that enables local-hpc interactions\n",
      "  - [`Protype Forwarder`](applications/thesis/porter): A initial implementation of forwarder\n",
      "  - [`Prototype Submitter`](applications/thesis/porter): A initial implementation of submitter\n",
      "  \n",
      "- **Testing**\n",
      "  - [`Experiments`](experiments): Collection of notebooks for comparing Fashion MNIST scenarios\n",
      "  - [`Article experiments`](experiments/article): Scenarios used in a related article\n",
      "  - [`Thesis experiments`](experiments/thesis): Scenarios used in a related master's thesis\n",
      "  - [`Experiment Notes`](experiments/article/README.md): Information regarding article results and Cloud-HPC setup\n",
      "  \n",
      "- **Setup Scripts**\n",
      "  - [`setup.sh`](setup.sh): The primary script to install and configure the platform on your local machine.\n",
      "  - [`setup.md`](setup.md): Detailed documentation for platform setup and testing procedures.\n",
      "\n",
      "- **Deployment Resources**\n",
      "  - [`deployment/`](deployment): Contains Kubernetes deployment manifests and configurations for Infrastructure as Code (IaC) practices.\n",
      "\n",
      "- **Tutorials and Guides**\n",
      "  - [`tutorials/`](tutorials): A collection of resources to help you understand and utilize the platform effectively.\n",
      "    - [`local_deployment/`](tutorials/local_deployment): A comprehensive guide for local deployment, including configuration and testing instructions.\n",
      "    - [`gcp_quickstart/`](tutorials/gcp_quickstart): A guide for a quickstart deployment of the platform to GCP.\n",
      "    - [`gcp_deployment/`](tutorials/gcp_deployment): A guide for a production-ready deployment of the platform to GCP.\n",
      "    - [`demo_notebooks/`](tutorials/demo_notebooks): A set of Jupyter notebooks showcasing example ML pipelines.\n",
      "    - [`ray/`](tutorials/ray): A guide for setting up and using [Ray](https://docs.ray.io/en/latest/index.html).\n",
      "\n",
      "- **Testing Suite**\n",
      "  - [`tests/`](tests): A suite of tests designed to ensure the platform's integrity post-deployment.\n",
      "\n",
      "\n",
      "## Special Instructions for Mac Users\n",
      "\n",
      "> **Important Notice for Mac Users:** Ensure Docker Desktop is installed on your machine, not Rancher Desktop, to avoid conflicts during the `kubectl` installation process.\n",
      "If Rancher Desktop was previously installed, please uninstall it and switch to Docker Desktop. Update your Docker context with the following command:\n",
      "\n",
      "```bash\n",
      "docker context use default\n",
      "```\n",
      "\n",
      "Additionally, confirm that Xcode is installed correctly to prevent potential issues:\n",
      "\n",
      "```bash\n",
      "xcode-select --install\n",
      "```\n",
      "\n",
      "## Getting Started with a local setup\n",
      "\n",
      "To set up the platform locally, execute the [`setup.sh`](setup.sh) script. For a concise setup overview, refer to the [setup guide](setup.md), or for a more detailed approach, consult the [manual setup instructions](tutorials/local_deployment).\n",
      "\n",
      "## Exploring Demo Examples\n",
      "\n",
      "Dive into our demo examples to see the platform in action:\n",
      "\n",
      "- **Jupyter Notebooks (e2e)**:\n",
      "\n",
      "  - [Demo Wine quality ML pipeline.](tutorials/demo_notebooks/demo_pipeline)\n",
      "\n",
      "  - [Demo Fairness and energy monitoring pipeline.](tutorials/demo_notebooks/demo_fairness_and_energy_monitoring)\n",
      "  \n",
      "  - [Demo Ray-Kubeflow pipeline.](tutorials/ray/notebooks/ray_kubeflow.ipynb)\n",
      "\n",
      "\n",
      "- **Project Use-Cases (e2e)**:\n",
      "\n",
      "  - [Fashion-MNIST MLOPS pipeline](https://github.com/OSS-MLOPS-PLATFORM/demo-fmnist-mlops-pipeline)\n",
      "\n",
      "## High-Level Architecture Overview\n",
      "\n",
      "The following diagram illustrates the architectural design of the MLOps platform:\n",
      "\n",
      "![MLOps Platform Architecture](resources/img/mlops-platform-diagram.png)\n",
      "\n",
      "### Key Components\n",
      "\n",
      "- **Kind**: Simplifies local Kubernetes cluster setup.\n",
      "- **Kubernetes**: The backbone container orchestrator.\n",
      "- **MLFlow**: Manages experiment tracking and model registry.\n",
      "  - **PostgreSQL DB**: Stores metadata for parameters and metrics.\n",
      "  - **MinIO**: An artifact store for ML models.\n",
      "- **Kubeflow**: Orchestrates ML workflows.\n",
      "- **KServe**: Facilitates model deployment and serving.\n",
      "- **Prometheus & Grafana**: Provides monitoring solutions with advanced visualization capabilities.\n",
      "\n",
      "## Support & Feedback\n",
      "\n",
      "Join our Slack [oss-mlops-platform](https://join.slack.com/t/oss-mlops-platform/shared_invite/zt-28m00bllw-0zl2cuKILh6oa2dIwDN_DQ)\n",
      "workspace for issues, support requests or just discussing feedback.\n",
      "\n",
      "Alternatively, feel free to use GitHub Issues for bugs, tasks or ideas to be discussed.\n",
      "\n",
      "Contact people:\n",
      "\n",
      "Harry Souris - harry.souris@silo.ai\n",
      "\n",
      "Joaquin Rives - joaquin.rives@silo.ai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca76ef11-910f-467c-8c98-d28376b6ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://raw.githubusercontent.com/K123AsJ0k1/cloud-hpc-oss-mlops-platform/main/experiments/article/cloud-hpc/Cloud-HPC-FMNIST-Experiment.ipynb'\n",
    "response = requests.get(test_url)\n",
    "response.raise_for_status()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c98cd038-03d2-466f-b3ef-948292d3abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_node = nbformat.reads(\n",
    "    response.text, \n",
    "    as_version = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2baac103-77df-4700-b58e-6e356dfbfd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cloud-HPC FMNIST Experiment\n",
      "\n",
      "In this notebook we go over the necessery manual actions for submitting a training job in Mahti using CPouta, Allas and Mahti integreated OSS. If you want to try this notebook yourself, you need to already have the following:\n",
      "\n",
      "- MyCSC account\n",
      "- Project with billing units\n",
      "- Access to CPouta\n",
      "- Suitable network rules\n",
      "- SSH key for CPouta for local and bridge\n",
      "- Setup Cloud-HPC OSS\n",
      "- Access to Allas\n",
      "- Access to Mahti\n",
      "- SSH setup to Mahti\n",
      "\n",
      "To begin, please create a python virtual enviroment, install the packages and open this notebook with the following:\n",
      "\n",
      "```\n",
      "python3 -m venv exp_venv\n",
      "source exp_venv/bin/activate\n",
      "pip install -r exp_req.txt\n",
      "jupyter notebook\n",
      "```\n",
      "\n",
      "This notebook uses the following packages:\n",
      "- notebook\n",
      "- matplotlib\n",
      "- torch\n",
      "- torchvision\n",
      "- torchmetrics\n",
      "- python-decouple\n",
      "- keystoneauth1\n",
      "- python-swiftclient\n",
      "- kfp~=1.8.14\n",
      "## Data Analysis\n",
      "import torch\n",
      "from torchvision import datasets\n",
      "import torchvision.transforms as T\n",
      "\n",
      "image_labels = {\n",
      "    0: 'Top',\n",
      "    1: 'Trouser',\n",
      "    2: 'Pullover',\n",
      "    3: 'Dress',\n",
      "    4: 'Coat',\n",
      "    5: 'Sandal',\n",
      "    6: 'Shirt',\n",
      "    7: 'Sneaker',\n",
      "    8: 'Bag',\n",
      "    9: 'Ankle Boot',\n",
      "}\n",
      "\n",
      "regular_transform = T.Compose([\n",
      "    T.ToTensor()\n",
      "])\n",
      "\n",
      "source_train_data = datasets.FashionMNIST(\n",
      "    root = './data', \n",
      "    train = True, \n",
      "    download = True, \n",
      "    transform = regular_transform\n",
      ")\n",
      "\n",
      "source_test_data = datasets.FashionMNIST(\n",
      "    root = './data', \n",
      "    train = False, \n",
      "    download = True, \n",
      "    transform = regular_transform\n",
      ")\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def first_columnsXrows_images(\n",
      "    dataset: any,\n",
      "    labels: any,\n",
      "    columns: int,\n",
      "    rows: int\n",
      "):\n",
      "    figure = plt.figure(figsize = (10,10))\n",
      "    for i in range (1, columns * rows + 1):\n",
      "        image, label = dataset[i]\n",
      "        figure.add_subplot(\n",
      "            rows, \n",
      "            columns, \n",
      "            i\n",
      "        )\n",
      "        plt.title(labels[label])\n",
      "        plt.axis('off')\n",
      "        plt.imshow(\n",
      "            image.squeeze(), \n",
      "            cmap = 'gray'\n",
      "        )\n",
      "    plt.show()\n",
      "\n",
      "def class_amounts(\n",
      "    dataset: any,\n",
      "    labels: any\n",
      "):\n",
      "    class_amounts = torch.bincount(dataset.targets)\n",
      "    print('Class, amount:')\n",
      "    for i in range(len(labels)):\n",
      "        print(labels[i], class_amounts[i].item())\n",
      "first_columnsXrows_images(\n",
      "    dataset = source_train_data,\n",
      "    labels = image_labels,\n",
      "    columns = 3,\n",
      "    rows = 3\n",
      ")\n",
      "print('Train set has {} instances'.format(len(source_train_data)))\n",
      "class_amounts(\n",
      "    dataset = source_train_data,\n",
      "    labels = image_labels\n",
      ")\n",
      "first_columnsXrows_images(\n",
      "    dataset = source_test_data,\n",
      "    labels = image_labels,\n",
      "    columns = 3,\n",
      "    rows = 3\n",
      ")\n",
      "print('Test set has {} instances'.format(len(source_test_data)))\n",
      "class_amounts(\n",
      "    dataset = source_test_data,\n",
      "    labels = image_labels\n",
      ")\n",
      "## Preprocessing and training in Cloud-HPC\n",
      "## Boilerplate\n",
      "## Istio\n",
      "import re\n",
      "import requests\n",
      "from urllib.parse import urlsplit\n",
      "\n",
      "def get_istio_auth_session(url: str, username: str, password: str) -> dict:\n",
      "    \"\"\"\n",
      "    Determine if the specified URL is secured by Dex and try to obtain a session cookie.\n",
      "    WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\n",
      "             (we default default to using `staticPasswords` if both are enabled)\n",
      "\n",
      "    :param url: Kubeflow server URL, including protocol\n",
      "    :param username: Dex `staticPasswords` or `LDAP` username\n",
      "    :param password: Dex `staticPasswords` or `LDAP` password\n",
      "    :return: auth session information\n",
      "    \"\"\"\n",
      "    # define the default return object\n",
      "    auth_session = {\n",
      "        \"endpoint_url\": url,    # KF endpoint URL\n",
      "        \"redirect_url\": None,   # KF redirect URL, if applicable\n",
      "        \"dex_login_url\": None,  # Dex login URL (for POST of credentials)\n",
      "        \"is_secured\": None,     # True if KF endpoint is secured\n",
      "        \"session_cookie\": None  # Resulting session cookies in the form \"key1=value1; key2=value2\"\n",
      "    }\n",
      "\n",
      "    # use a persistent session (for cookies)\n",
      "    with requests.Session() as s:\n",
      "\n",
      "        ################\n",
      "        # Determine if Endpoint is Secured\n",
      "        ################\n",
      "        resp = s.get(url, allow_redirects=True)\n",
      "        if resp.status_code != 200:\n",
      "            raise RuntimeError(\n",
      "                f\"HTTP status code '{resp.status_code}' for GET against: {url}\"\n",
      "            )\n",
      "\n",
      "        auth_session[\"redirect_url\"] = resp.url\n",
      "\n",
      "        # if we were NOT redirected, then the endpoint is UNSECURED\n",
      "        if len(resp.history) == 0:\n",
      "            auth_session[\"is_secured\"] = False\n",
      "            return auth_session\n",
      "        else:\n",
      "            auth_session[\"is_secured\"] = True\n",
      "\n",
      "        ################\n",
      "        # Get Dex Login URL\n",
      "        ################\n",
      "        redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\n",
      "\n",
      "        # if we are at `/auth?=xxxx` path, we need to select an auth type\n",
      "        if re.search(r\"/auth$\", redirect_url_obj.path):\n",
      "\n",
      "            #######\n",
      "            # TIP: choose the default auth type by including ONE of the following\n",
      "            #######\n",
      "\n",
      "            # OPTION 1: set \"staticPasswords\" as default auth type\n",
      "            redirect_url_obj = redirect_url_obj._replace(\n",
      "                path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\n",
      "            )\n",
      "            # OPTION 2: set \"ldap\" as default auth type\n",
      "            # redirect_url_obj = redirect_url_obj._replace(\n",
      "            #     path=re.sub(r\"/auth$\", \"/auth/ldap\", redirect_url_obj.path)\n",
      "            # )\n",
      "\n",
      "        # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST)\n",
      "        if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\n",
      "            auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\n",
      "\n",
      "        # else, we need to be redirected to the actual login page\n",
      "        else:\n",
      "            # this GET should redirect us to the `/auth/xxxx/login` path\n",
      "            resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\n",
      "            if resp.status_code != 200:\n",
      "                raise RuntimeError(\n",
      "                    f\"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}\"\n",
      "                )\n",
      "\n",
      "            # set the login url\n",
      "            auth_session[\"dex_login_url\"] = resp.url\n",
      "\n",
      "        ################\n",
      "        # Attempt Dex Login\n",
      "        ################\n",
      "        resp = s.post(\n",
      "            auth_session[\"dex_login_url\"],\n",
      "            data={\"login\": username, \"password\": password},\n",
      "            allow_redirects=True\n",
      "        )\n",
      "        if len(resp.history) == 0:\n",
      "            raise RuntimeError(\n",
      "                f\"Login credentials were probably invalid - \"\n",
      "                f\"No redirect after POST to: {auth_session['dex_login_url']}\"\n",
      "            )\n",
      "\n",
      "        # store the session cookies in a \"key1=value1; key2=value2\" string\n",
      "        auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
      "\n",
      "    return auth_session\n",
      "### General Functions\n",
      "# Created and works\n",
      "def set_formatted_user(\n",
      "    user: str   \n",
      ") -> any:\n",
      "    return re.sub(r'[^a-z0-9]+', '-', user)\n",
      "## SWIFT Functions\n",
      "from decouple import Config,RepositoryEnv\n",
      "\n",
      "from keystoneauth1 import loading, session\n",
      "from keystoneauth1.identity import v3\n",
      "from keystoneclient.v3 import client as keystone_client\n",
      "\n",
      "import swiftclient as sc\n",
      "import pickle\n",
      "# Works\n",
      "def is_swift_client(\n",
      "    storage_client: any\n",
      ") -> any:\n",
      "    return isinstance(storage_client, sc.Connection)\n",
      "# Works\n",
      "def swift_setup_client(\n",
      "    pre_auth_url: str,\n",
      "    pre_auth_token: str,\n",
      "    user_domain_name: str,\n",
      "    project_domain_name: str,\n",
      "    project_name: str,\n",
      "    auth_version: str\n",
      ") -> any:\n",
      "    swift_client = sc.Connection(\n",
      "        preauthurl = pre_auth_url,\n",
      "        preauthtoken = pre_auth_token,\n",
      "        os_options = {\n",
      "            'user_domain_name': user_domain_name,\n",
      "            'project_domain_name': project_domain_name,\n",
      "            'project_name': project_name\n",
      "        },\n",
      "        auth_version = auth_version\n",
      "    )\n",
      "    return swift_client\n",
      "# Works\n",
      "def swift_create_bucket(\n",
      "    swift_client: any,\n",
      "    bucket_name: str\n",
      ") -> bool:\n",
      "    try:\n",
      "        swift_client.put_container(\n",
      "            container = bucket_name\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Works\n",
      "def swift_check_bucket(\n",
      "    swift_client: any,\n",
      "    bucket_name:str\n",
      ") -> any:\n",
      "    try:\n",
      "        bucket_info = swift_client.get_container(\n",
      "            container = bucket_name\n",
      "        )\n",
      "        bucket_metadata = bucket_info[0]\n",
      "        list_of_objects = bucket_info[1]\n",
      "        return {'metadata': bucket_metadata, 'objects': list_of_objects}\n",
      "    except Exception as e:\n",
      "        return {} \n",
      "# Refactored\n",
      "def swift_delete_bucket(\n",
      "    swift_client: any,\n",
      "    bucket_name: str\n",
      ") -> bool:\n",
      "    try:\n",
      "        swift_client.delete_container(\n",
      "            container = bucket_name\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Created\n",
      "def swift_list_buckets(\n",
      "    swift_client: any\n",
      ") -> any:\n",
      "    try:\n",
      "        account_buckets = swift_client.get_account()[1]\n",
      "        buckets = {}\n",
      "        for bucket in account_buckets:\n",
      "            bucket_name = bucket['name']\n",
      "            bucket_count = bucket['count']\n",
      "            bucket_size = bucket['bytes']\n",
      "            buckets[bucket_name] = {\n",
      "                'amount': bucket_count,\n",
      "                'size': bucket_size\n",
      "            }\n",
      "        return buckets\n",
      "    except Exception as e:\n",
      "        return {}\n",
      "# Works\n",
      "def swift_create_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> bool: \n",
      "    # This should be updated to handle 5 GB objects\n",
      "    # It also should handle metadata\n",
      "    try:\n",
      "        swift_client.put_object(\n",
      "            container = bucket_name,\n",
      "            obj = object_path,\n",
      "            contents = object_data,\n",
      "            headers = object_metadata\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Works\n",
      "def swift_check_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> any: \n",
      "    try:\n",
      "        object_metadata = swift_client.head_object(\n",
      "            container = bucket_name,\n",
      "            obj = object_path\n",
      "        )       \n",
      "        return object_metadata\n",
      "    except Exception as e:\n",
      "        return {} \n",
      "# Refactored\n",
      "def swift_get_object(\n",
      "    swift_client:any,\n",
      "    bucket_name: str,\n",
      "    object_path: str\n",
      ") -> any:\n",
      "    # This should handle metadata\n",
      "    try:\n",
      "        response = swift_client.get_object(\n",
      "            container = bucket_name,\n",
      "            obj = object_path \n",
      "        )\n",
      "        object_info = response[0]\n",
      "        object_data = response[1]\n",
      "        return {'data': object_data, 'info': object_info}\n",
      "    except Exception as e:\n",
      "        return {}     \n",
      "# Refactored   \n",
      "def swift_remove_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> bool: \n",
      "    try:\n",
      "        swift_client.delete_object(\n",
      "            container = bucket_name, \n",
      "            obj = object_path\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Works\n",
      "def swift_update_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> bool:  \n",
      "    remove = swift_remove_object(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name, \n",
      "        object_path = object_path\n",
      "    )\n",
      "    if not remove:\n",
      "        return False\n",
      "    create = swift_create_object(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name, \n",
      "        object_path = object_path, \n",
      "        object_data = object_data,\n",
      "        object_metadata = object_metadata\n",
      "    )\n",
      "    return create\n",
      "# Works\n",
      "def swift_create_or_update_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    bucket_info = swift_check_bucket(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name\n",
      "    )\n",
      "    \n",
      "    if len(bucket_info) == 0:\n",
      "        creation_status = swift_create_bucket(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "        if not creation_status:\n",
      "            return False\n",
      "    \n",
      "    object_info = swift_check_object(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name, \n",
      "        object_path = object_path\n",
      "    )\n",
      "    \n",
      "    if len(object_info) == 0:\n",
      "        return swift_create_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path, \n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "    else:\n",
      "        return swift_update_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path, \n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "## Storage Functions\n",
      "# 3-2-2\n",
      "\n",
      "# Refactored and Works\n",
      "def set_encoded_metadata(\n",
      "    used_client: str,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    encoded_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        key_initial = 'x-object-meta'\n",
      "        for key, value in object_metadata.items():\n",
      "            encoded_key = key_initial + '-' + key\n",
      "            if isinstance(value, list):\n",
      "                encoded_metadata[encoded_key] = 'list=' + ','.join(map(str, value))\n",
      "                continue\n",
      "            encoded_metadata[encoded_key] = str(value)\n",
      "    return encoded_metadata\n",
      "# Refactored and works\n",
      "def get_general_metadata(\n",
      "    used_client: str,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    general_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        key_initial = 'x-object-meta'\n",
      "        for key, value in object_metadata.items():\n",
      "            if not key_initial == key[:len(key_initial)]:\n",
      "                general_metadata[key] = value\n",
      "    return general_metadata\n",
      "# Refactored and works\n",
      "def get_decoded_metadata(\n",
      "    used_client: str,\n",
      "    object_metadata: any\n",
      ") -> any: \n",
      "    decoded_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        key_initial = 'x-object-meta'\n",
      "        for key, value in object_metadata.items():\n",
      "            if key_initial == key[:len(key_initial)]:\n",
      "                decoded_key = key[len(key_initial) + 1:]\n",
      "                if 'list=' in value:\n",
      "                    string_integers = value.split('=')[1]\n",
      "                    values = string_integers.split(',')\n",
      "                    if len(values) == 1 and values[0] == '':\n",
      "                        decoded_metadata[decoded_key] = []\n",
      "                    else:\n",
      "                        try:\n",
      "                            decoded_metadata[decoded_key] = list(map(int, values))\n",
      "                        except:\n",
      "                            decoded_metadata[decoded_key] = list(map(str, values))\n",
      "                    continue\n",
      "                if value.isnumeric():\n",
      "                    decoded_metadata[decoded_key] = int(value)\n",
      "                    continue\n",
      "                decoded_metadata[decoded_key] = value\n",
      "    return decoded_metadata\n",
      "# Refactored and works\n",
      "def set_bucket_names(\n",
      "    storage_parameters: any\n",
      ") -> any:\n",
      "    storage_names = []\n",
      "    bucket_prefix = storage_parameters['bucket-prefix']\n",
      "    ice_id = storage_parameters['ice-id']\n",
      "    user = storage_parameters['user']\n",
      "    storage_names.append(bucket_prefix + '-forwarder-' + ice_id)\n",
      "    storage_names.append(bucket_prefix + '-submitter-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "    storage_names.append(bucket_prefix + '-pipeline-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "    storage_names.append(bucket_prefix + '-experiment-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "    return storage_names\n",
      "# Refactored and works\n",
      "def setup_storage_client(\n",
      "    storage_parameters: any\n",
      ") -> any:\n",
      "    storage_client = None\n",
      "    if storage_parameters['used-client'] == 'swift':\n",
      "        storage_client = swift_setup_client(\n",
      "            pre_auth_url = storage_parameters['pre-auth-url'],\n",
      "            pre_auth_token = storage_parameters['pre-auth-token'],\n",
      "            user_domain_name = storage_parameters['user-domain-name'],\n",
      "            project_domain_name = storage_parameters['project-domain-name'],\n",
      "            project_name = storage_parameters['project-name'],\n",
      "            auth_version = storage_parameters['auth-version']\n",
      "        )\n",
      "    return storage_client\n",
      "# Refactored and works\n",
      "def check_object_metadata(\n",
      "    storage_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> any: \n",
      "    object_metadata = {\n",
      "        'general-meta': {},\n",
      "        'custom-meta': {}\n",
      "    }\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        all_metadata = swift_check_object(\n",
      "           swift_client = storage_client,\n",
      "           bucket_name = bucket_name,\n",
      "           object_path = object_path\n",
      "        ) \n",
      "\n",
      "        general_metadata = {}\n",
      "        custom_metadata = {}\n",
      "        if not len(all_metadata) == 0:\n",
      "            general_metadata = get_general_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = all_metadata\n",
      "            )\n",
      "            custom_metadata = get_decoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = all_metadata\n",
      "            )\n",
      "\n",
      "        object_metadata['general-meta'] = general_metadata\n",
      "        object_metadata['custom-meta'] = custom_metadata\n",
      "\n",
      "    return object_metadata\n",
      "# Refactored and works\n",
      "def get_object_content(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_path: str\n",
      ") -> any:\n",
      "    object_content = {}\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        fetched_object = swift_get_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "        )\n",
      "        object_content['data'] = pickle.loads(fetched_object['data'])\n",
      "        object_content['general-meta'] = get_general_metadata(\n",
      "            used_client = 'swift',\n",
      "            object_metadata = fetched_object['info']\n",
      "        )\n",
      "        object_content['custom-meta'] = get_decoded_metadata(\n",
      "            used_client = 'swift',\n",
      "            object_metadata = fetched_object['info']\n",
      "        )\n",
      "    return object_content\n",
      "# Refactored    \n",
      "def remove_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> bool: \n",
      "    removed = False\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        removed = swift_remove_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "        )\n",
      "    return removed\n",
      "# Refactored and works\n",
      "def create_or_update_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    success = False\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        formatted_data = pickle.dumps(object_data)\n",
      "        formatted_metadata = set_encoded_metadata(\n",
      "            used_client = 'swift',\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "\n",
      "        success = swift_create_or_update_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path,\n",
      "            object_data = formatted_data,\n",
      "            object_metadata = formatted_metadata\n",
      "        )\n",
      "    return success\n",
      "# Created and works\n",
      "def format_bucket_metadata(\n",
      "    used_client: str,\n",
      "    bucket_metadata: any\n",
      ") -> any:\n",
      "    formatted_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        relevant_values = {\n",
      "            'x-container-object-count': 'object-count',\n",
      "            'x-container-bytes-used-actual': 'used-bytes',\n",
      "            'last-modified': 'date',\n",
      "            'content-type': 'type'\n",
      "        }\n",
      "        formatted_metadata = {}\n",
      "        for key,value in bucket_metadata.items():\n",
      "            if key in relevant_values:\n",
      "                formatted_key = relevant_values[key]\n",
      "                formatted_metadata[formatted_key] = value\n",
      "    return formatted_metadata\n",
      "# Created and works\n",
      "def format_bucket_objects(\n",
      "    used_client: str,\n",
      "    bucket_objects: any\n",
      ") -> any:\n",
      "    formatted_objects = {}\n",
      "    if used_client == 'swift':\n",
      "        for bucket_object in bucket_objects:\n",
      "            formatted_object_metadata = {\n",
      "                'hash': 'id',\n",
      "                'bytes': 'used-bytes',\n",
      "                'last_modified': 'date'\n",
      "            }\n",
      "            object_key = None\n",
      "            object_metadata = {}\n",
      "            for key, value in bucket_object.items():\n",
      "                if key == 'name':\n",
      "                    object_key = value\n",
      "                if key in formatted_object_metadata:\n",
      "                    formatted_key = formatted_object_metadata[key]\n",
      "                    object_metadata[formatted_key] = value\n",
      "            formatted_objects[object_key] = object_metadata\n",
      "    return formatted_objects\n",
      "# Created and works\n",
      "def format_bucket_info(\n",
      "    used_client: str,\n",
      "    bucket_info: any\n",
      ") -> any:\n",
      "    bucket_metadata = {}\n",
      "    bucket_objects = {}\n",
      "    if used_client == 'swift':\n",
      "        bucket_metadata = format_bucket_metadata(\n",
      "            used_client = used_client,\n",
      "            bucket_metadata = bucket_info['metadata']\n",
      "        )\n",
      "        bucket_objects = format_bucket_objects(\n",
      "            used_client = used_client,\n",
      "            bucket_objects = bucket_info['objects']\n",
      "        )\n",
      "    return {'metadata': bucket_metadata, 'objects': bucket_objects} \n",
      "# Created and works\n",
      "def get_bucket_info(\n",
      "    storage_client: any,\n",
      "    bucket_name: str\n",
      ") -> any:\n",
      "    bucket_info = {}\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        unformatted_bucket_info = swift_check_bucket(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "        bucket_info = format_bucket_info(\n",
      "            used_client = 'swift',\n",
      "            bucket_info = unformatted_bucket_info\n",
      "        )\n",
      "    return bucket_info\n",
      "# Created and works\n",
      "def format_container_info(\n",
      "    used_client: str,\n",
      "    container_info: any\n",
      ") -> any:\n",
      "    formatted_container_info = {}\n",
      "    if used_client == 'swift':\n",
      "        for bucket in container_info:\n",
      "            bucket_name = bucket['name']\n",
      "            bucket_count = bucket['count']\n",
      "            bucket_size = bucket['bytes']\n",
      "            formatted_container_info[bucket_name] = {\n",
      "                'amount': bucket_count,\n",
      "                'size': bucket_size\n",
      "            }\n",
      "    return formatted_container_info\n",
      "# Created and works\n",
      "def get_container_info( \n",
      "    storage_client: any\n",
      ") -> any:\n",
      "    container_info = {}\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        unformatted_container_info = swift_list_buckets(\n",
      "            swift_client = storage_client \n",
      "        )\n",
      "        container_info = format_container_info(\n",
      "            used_client = 'swift',\n",
      "            container_info = unformatted_container_info\n",
      "        )\n",
      "    return container_info\n",
      "## Object Functions\n",
      "# 4-2-3\n",
      "\n",
      "# Created and works\n",
      "def set_object_path(\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any\n",
      "):\n",
      "    object_paths = {\n",
      "        'root': 'name',\n",
      "        'code': 'CODE/name',\n",
      "        'slurm': 'CODE/SLURM/name',\n",
      "        'ray': 'CODE/RAY/name',\n",
      "        'data': 'DATA/name',\n",
      "        'artifacts': 'ARTIFACTS/name',\n",
      "        'time': 'TIMES/name'\n",
      "    }\n",
      "\n",
      "    i = 0\n",
      "    path_split = object_paths[object_name].split('/')\n",
      "    for name in path_split:\n",
      "        if name in path_replacers:\n",
      "            replacer = path_replacers[name]\n",
      "            if 0 < len(replacer):\n",
      "                path_split[i] = replacer\n",
      "        i = i + 1\n",
      "    \n",
      "    if not len(path_names) == 0:\n",
      "        path_split.extend(path_names)\n",
      "\n",
      "    object_path = '/'.join(path_split)\n",
      "    print('Used object path:' + str(object_path))\n",
      "    return object_path\n",
      "# created and works\n",
      "def setup_storage(\n",
      "    storage_parameters: any\n",
      ") -> any:\n",
      "    storage_client = setup_storage_client(\n",
      "        storage_parameters = storage_parameters\n",
      "    ) \n",
      "    \n",
      "    storage_name = set_bucket_names(\n",
      "       storage_parameters = storage_parameters\n",
      "    )\n",
      "    \n",
      "    return storage_client, storage_name\n",
      "# Created and works\n",
      "def check_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any\n",
      ") -> bool:\n",
      "    object_path = set_object_path(\n",
      "        object_name = object_name,\n",
      "        path_replacers = path_replacers,\n",
      "        path_names = path_names\n",
      "    )\n",
      "    # Consider making these functions object storage agnostic\n",
      "    object_metadata = check_object_metadata(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name,\n",
      "        object_path = object_path\n",
      "    )\n",
      "    object_metadata['path'] = object_path\n",
      "    return object_metadata\n",
      "# Created and works\n",
      "def get_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any\n",
      ") -> any:\n",
      "    checked_object = check_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name,\n",
      "        object_name = object_name,\n",
      "        path_replacers = path_replacers,\n",
      "        path_names = path_names\n",
      "    )\n",
      "\n",
      "    object_data = None\n",
      "    if not len(checked_object['general-meta']) == 0:\n",
      "        # Consider making these functions object storage agnostic\n",
      "        object_data = get_object_content(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = checked_object['path']\n",
      "        )\n",
      "\n",
      "    return object_data\n",
      "# Created and Works\n",
      "def set_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any,\n",
      "    overwrite: bool,\n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      "):\n",
      "    checked_object = check_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name,\n",
      "        object_name = object_name,\n",
      "        path_replacers = path_replacers,\n",
      "        path_names = path_names\n",
      "    )\n",
      "    \n",
      "    perform = True\n",
      "    if not len(checked_object['general-meta']) == 0 and not overwrite:\n",
      "        perform = False\n",
      "    \n",
      "    if perform:\n",
      "        create_or_update_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = checked_object['path'],\n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "# Created and works\n",
      "def check_bucket(\n",
      "    storage_client: any,\n",
      "    bucket_name: str\n",
      ") -> any:\n",
      "    return get_bucket_info(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name\n",
      "    )\n",
      "# Created and works\n",
      "def check_buckets(\n",
      "    storage_client: any\n",
      ") -> any:\n",
      "    return get_container_info( \n",
      "        storage_client = storage_client\n",
      "    )\n",
      "## Metadata Function\n",
      "# Created and works\n",
      "def general_object_metadata():\n",
      "    general_object_metadata = {\n",
      "        'version': 1\n",
      "    }\n",
      "    return general_object_metadata\n",
      "## Time Functions\n",
      "def gather_time(\n",
      "    storage_client: any,\n",
      "    storage_name: any,\n",
      "    time_group: any,\n",
      "    time_name: any,\n",
      "    start_time: int,\n",
      "    end_time: int\n",
      "):\n",
      "    time_object = get_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = storage_name,\n",
      "        object_name = 'time',\n",
      "        path_replacers = {\n",
      "            'name': time_group\n",
      "        },\n",
      "        path_names = []\n",
      "    )\n",
      "\n",
      "    time_data = {}\n",
      "    time_metadata = {} \n",
      "    if time_object is None:\n",
      "        time_data = {}\n",
      "        time_metadata = general_object_metadata()\n",
      "    else:\n",
      "        time_data = time_object['data']\n",
      "        time_metadata = time_object['custom-meta']\n",
      "    \n",
      "    current_key_amount = len(time_data)\n",
      "    current_key_full = False\n",
      "    current_key = str(current_key_amount)\n",
      "    if 0 < current_key_amount:\n",
      "        time_object = time_data[current_key]\n",
      "        if 0 < time_object['total-seconds']:\n",
      "            current_key_full = True\n",
      "    \n",
      "    changed = False\n",
      "    if 0 < end_time and 0 < current_key_amount and not current_key_full:\n",
      "        stored_start_time = time_data[current_key]['start-time']\n",
      "        time_diff = (end_time-stored_start_time)\n",
      "        time_data[current_key]['end-time'] = end_time\n",
      "        time_data[current_key]['total-seconds'] = round(time_diff,5)\n",
      "        changed = True\n",
      "    else:\n",
      "        next_key_amount = len(time_data) + 1\n",
      "        new_key = str(next_key_amount)\n",
      "    \n",
      "        if 0 < start_time and 0 == end_time:\n",
      "            time_data[new_key] = {\n",
      "                'name': time_name,\n",
      "                'start-time': start_time,\n",
      "                'end-time': 0,\n",
      "                'total-seconds': 0\n",
      "            }\n",
      "            changed = True\n",
      "\n",
      "        if 0 < start_time and 0 < end_time:\n",
      "            time_diff = (end_time-start_time)\n",
      "            time_data[new_key] = {\n",
      "                'name': time_name,\n",
      "                'start-time': start_time,\n",
      "                'end-time': end_time,\n",
      "                'total-seconds': round(time_diff,5)\n",
      "            }\n",
      "            changed = True\n",
      "\n",
      "    if changed:\n",
      "        time_metadata['version'] = time_metadata['version'] + 1\n",
      "        set_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'time',\n",
      "            path_replacers = {\n",
      "                'name': time_group\n",
      "            },\n",
      "            path_names = [],\n",
      "            overwrite = True,\n",
      "            object_data = time_data,\n",
      "            object_metadata = time_metadata \n",
      "        )\n",
      "## Compose functions\n",
      "import subprocess\n",
      "\n",
      "# created\n",
      "def start_compose(\n",
      "    file_path: str\n",
      ") -> bool:\n",
      "    compose_up_command = 'docker compose -f (file) up -d'\n",
      "    modified_up_command = compose_up_command.replace('(file)', file_path)\n",
      "    \n",
      "    resulted_print = subprocess.run(\n",
      "        modified_up_command,\n",
      "        shell = True,\n",
      "        stdout = subprocess.PIPE,\n",
      "        stderr = subprocess.PIPE\n",
      "    )\n",
      "    \n",
      "    print_split = resulted_print.stderr.decode('utf-8').split('\\n')\n",
      "\n",
      "    deployed = False\n",
      "    for row in print_split:\n",
      "        empty_split = row.split(' ')\n",
      "        for word in empty_split:\n",
      "            if 0 == len(word):\n",
      "                continue\n",
      "            if word.lower() == 'started':\n",
      "                deployed = True\n",
      "    return deployed\n",
      "# created\n",
      "def stop_compose(\n",
      "    file_path: str\n",
      ") -> bool:\n",
      "    compose_down_command = 'docker compose -f (file) down'\n",
      "    modified_down_command = compose_down_command.replace('(file)', file_path)\n",
      "    \n",
      "    resulted_print = subprocess.run(\n",
      "        modified_down_command,\n",
      "        shell = True,\n",
      "        stdout = subprocess.PIPE,\n",
      "        stderr = subprocess.PIPE\n",
      "    )\n",
      "\n",
      "    print_split = resulted_print.stderr.decode('utf-8').split('\\n')\n",
      "\n",
      "    removed = False\n",
      "    for row in print_split:\n",
      "        empty_split = row.split(' ')\n",
      "        for word in empty_split:\n",
      "            if 0 == len(word):\n",
      "                continue\n",
      "            if word.lower() == 'removed':\n",
      "                removed = True\n",
      "    return removed\n",
      "## Access Functions\n",
      "def get_storage_parameters(\n",
      "    env_path: str,\n",
      "    auth_url: str,\n",
      "    pre_auth_url: str,\n",
      "    auth_version: str,\n",
      "    bucket_prefix: str,\n",
      "    ice_id: str,\n",
      "    user: str\n",
      "):\n",
      "    env_config = Config(RepositoryEnv(env_path))\n",
      "    swift_auth_url = auth_url\n",
      "    swift_user = env_config.get('CSC_USERNAME')\n",
      "    swift_key = env_config.get('CSC_PASSWORD')\n",
      "    swift_project_name = env_config.get('CSC_PROJECT_NAME')\n",
      "    swift_user_domain_name = env_config.get('CSC_USER_DOMAIN_NAME')\n",
      "    swift_project_domain_name = env_config.get('CSC_USER_DOMAIN_NAME')\n",
      "\n",
      "    loader = loading.get_plugin_loader('password')\n",
      "    auth = loader.load_from_options(\n",
      "        auth_url = swift_auth_url,\n",
      "        username = swift_user,\n",
      "        password = swift_key,\n",
      "        project_name = swift_project_name,\n",
      "        user_domain_name = swift_user_domain_name,\n",
      "        project_domain_name = swift_project_domain_name\n",
      "    )\n",
      "\n",
      "    keystone_session = session.Session(\n",
      "        auth = auth\n",
      "    )\n",
      "    swift_token = keystone_session.get_token()\n",
      "\n",
      "    swift_pre_auth_url = pre_auth_url\n",
      "    swift_auth_version = auth_version\n",
      "\n",
      "    storage_parameters = {\n",
      "        'bucket-prefix': bucket_prefix,\n",
      "        'ice-id': ice_id,\n",
      "        'user': user,\n",
      "        'used-client': 'swift',\n",
      "        'pre-auth-url': str(swift_pre_auth_url),\n",
      "        'pre-auth-token': str(swift_token),\n",
      "        'user-domain-name': str(swift_user_domain_name),\n",
      "        'project-domain-name': str(swift_project_domain_name),\n",
      "        'project-name': str(swift_project_name),\n",
      "        'auth-version': str(swift_auth_version)\n",
      "    }\n",
      "\n",
      "    return storage_parameters\n",
      "## Code Functions\n",
      "# Refactored\n",
      "def set_code(\n",
      "    storage_client: any,\n",
      "    storage_name: str,\n",
      "    file_path: str,\n",
      "    overwrite: bool\n",
      "):\n",
      "    file_data = None\n",
      "    print('User code storage:' + str(storage_name))\n",
      "    print('Used code path:' + str(file_path))\n",
      "    with open(file_path, 'r') as f:\n",
      "        file_data = f.read()\n",
      "\n",
      "    path_split = file_path.split('/')\n",
      "    directory_name = path_split[-2]\n",
      "    file_name = path_split[-1]\n",
      "    \n",
      "    set_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = storage_name,\n",
      "        object_name = directory_name,\n",
      "        path_replacers = {\n",
      "            'name': file_name\n",
      "        },\n",
      "        path_names = [],\n",
      "        overwrite = overwrite,\n",
      "        object_data = file_data,\n",
      "        object_metadata = general_object_metadata()\n",
      "    )\n",
      "# Refactored\n",
      "def get_code(\n",
      "    storage_client: any,\n",
      "    storage_name: str,\n",
      "    code_type: str,\n",
      "    code_file: str\n",
      ") -> any:\n",
      "    print('User code storage:' + str(storage_name))\n",
      "    fetched_object = get_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = storage_name,\n",
      "        object_name = code_type,\n",
      "        path_replacers = {\n",
      "            'name': code_file\n",
      "        },\n",
      "        path_names = []\n",
      "    )\n",
      "    code_object = {\n",
      "        'data': fetched_object['data'],\n",
      "        'metadata': fetched_object['custom-meta']\n",
      "    }\n",
      "    return code_object\n",
      "## Gaining Storage Access\n",
      "env_absolute_path = '/home/sfniila/.ssh/.env'\n",
      "storage_parameters = get_storage_parameters(\n",
      "    env_path = env_absolute_path,\n",
      "    auth_url = 'https://pouta.csc.fi:5001/v3',\n",
      "    pre_auth_url = 'https://a3s.fi:443/swift/v1/AUTH_6698ff90e6704a74930c33d6b66f1b5b',\n",
      "    auth_version = '3',\n",
      "    bucket_prefix = 'integration',\n",
      "    ice_id = 's0-c0-u1',\n",
      "    user = 'user@example.com'\n",
      ")\n",
      "\n",
      "storage_client, storage_names = setup_storage(\n",
      "    storage_parameters = storage_parameters\n",
      ")\n",
      "storage_names\n",
      "## KFP Access\n",
      "## SLURM Script\n",
      "%%writefile scripts/slurm/ray-cluster.sh\n",
      "#!/bin/bash\n",
      "#SBATCH --job-name=ray-cluster\n",
      "#SBATCH --account=project_()\n",
      "#SBATCH --partition=medium\n",
      "#SBATCH --time=00:20:00\n",
      "#SBATCH --nodes=2\n",
      "#SBATCH --ntasks-per-node=1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10GB\n",
      "\n",
      "module load pytorch\n",
      "\n",
      "echo \"Loaded modules:\"\n",
      "\n",
      "module list\n",
      "\n",
      "echo \"Activating venv\"\n",
      "\n",
      "source /users/()/exp-venv/bin/activate\n",
      "\n",
      "echo \"Venv active\"\n",
      "\n",
      "echo \"Installed packages\"\n",
      "\n",
      "pip list\n",
      "\n",
      "echo \"Packages listed\"\n",
      "\n",
      "echo \"Setting connection variables\"\n",
      "\n",
      "key_path=\"/users/()/cpouta-mahti.pem\"\n",
      "cloud_ip=\"()\"\n",
      "cloud_port=8280\n",
      "cloud_user=\"()\"\n",
      "cloud_fip=\"()\"\n",
      "\n",
      "echo \"Setting Ray variables\"\n",
      "\n",
      "hpc_head_port=8265\n",
      "hpc_dashboard_port=8280 \n",
      "nodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\")\n",
      "nodes_array=($nodes)\n",
      "head_node=${nodes_array[0]}\n",
      "head_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$head_node\" hostname --ip-address)\n",
      "\n",
      "echo \"Setting up Ray head\"\n",
      "\n",
      "ip_head=$head_node_ip:$hpc_head_port\n",
      "export ip_head\n",
      "echo \"IP Head: $ip_head\"\n",
      "\n",
      "echo \"Starting HEAD at $head_node\"\n",
      "srun --nodes=1 --ntasks=1 -w \"$head_node\" \\\n",
      "    singularity_wrapper exec ray start --head --node-ip-address=\"$head_node_ip\" --port=$hpc_head_port --dashboard-host=\"$head_node_ip\" --dashboard-port=$hpc_dashboard_port \\\n",
      "    --num-cpus \"${SLURM_CPUS_PER_TASK}\" --num-gpus 3 --block &\n",
      "\n",
      "echo \"Setting up SSH tunnel\"\n",
      "\n",
      "ssh -f -o StrictHostKeyChecking=no -i $key_path -N -R $cloud_ip:$cloud_port:$head_node_ip:$hpc_dashboard_port $cloud_user@$cloud_fip\n",
      "\n",
      "echo \"Reverse port forward running\"\n",
      "\n",
      "sleep 5\n",
      "\n",
      "echo \"Setting up Ray workers\"\n",
      "\n",
      "worker_num=$((SLURM_JOB_NUM_NODES - 1))\n",
      "\n",
      "for ((i = 1; i <= worker_num; i++)); do\n",
      "    node_i=${nodes_array[$i]}\n",
      "    echo \"Starting WORKER $i at $node_i\"\n",
      "    srun --nodes=1 --ntasks=1 -w \"$node_i\" \\\n",
      "         singularity_wrapper exec ray start --address \"$ip_head\" \\\n",
      "\t --num-cpus \"${SLURM_CPUS_PER_TASK}\" --num-gpus 3 --block &\n",
      "    sleep 1140\n",
      "done\n",
      "\n",
      "storage_names[-2]\n",
      "set_code(\n",
      "    storage_client = storage_client,\n",
      "    storage_name = storage_names[-2],\n",
      "    file_path = 'scripts/slurm/ray-cluster.sh',\n",
      "    overwrite = True\n",
      ")\n",
      "## Ray\n",
      "%%writefile scripts/ray/train-fmnist-cnn.py\n",
      "import sys\n",
      "import ray\n",
      "import json\n",
      "\n",
      "import re\n",
      "import swiftclient as sc\n",
      "import pickle\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchmetrics as TM\n",
      "\n",
      "import time as t\n",
      "\n",
      "# Boilerplate START\n",
      "def set_formatted_user(\n",
      "    user: str   \n",
      ") -> any:\n",
      "    return re.sub(r'[^a-z0-9]+', '-', user)\n",
      "def general_object_metadata():\n",
      "    general_object_metadata = {\n",
      "        'version': 1\n",
      "    }\n",
      "    return general_object_metadata\n",
      "# Works\n",
      "def is_swift_client(\n",
      "    storage_client: any\n",
      ") -> any:\n",
      "    return isinstance(storage_client, sc.Connection)\n",
      "# Works\n",
      "def swift_setup_client(\n",
      "    pre_auth_url: str,\n",
      "    pre_auth_token: str,\n",
      "    user_domain_name: str,\n",
      "    project_domain_name: str,\n",
      "    project_name: str,\n",
      "    auth_version: str\n",
      ") -> any:\n",
      "    swift_client = sc.Connection(\n",
      "        preauthurl = pre_auth_url,\n",
      "        preauthtoken = pre_auth_token,\n",
      "        os_options = {\n",
      "            'user_domain_name': user_domain_name,\n",
      "            'project_domain_name': project_domain_name,\n",
      "            'project_name': project_name\n",
      "        },\n",
      "        auth_version = auth_version\n",
      "    )\n",
      "    return swift_client\n",
      "# Works\n",
      "def swift_create_bucket(\n",
      "    swift_client: any,\n",
      "    bucket_name: str\n",
      ") -> bool:\n",
      "    try:\n",
      "        swift_client.put_container(\n",
      "            container = bucket_name\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Works\n",
      "def swift_check_bucket(\n",
      "    swift_client: any,\n",
      "    bucket_name:str\n",
      ") -> any:\n",
      "    try:\n",
      "        bucket_info = swift_client.get_container(\n",
      "            container = bucket_name\n",
      "        )\n",
      "        bucket_metadata = bucket_info[0]\n",
      "        list_of_objects = bucket_info[1]\n",
      "        return {'metadata': bucket_metadata, 'objects': list_of_objects}\n",
      "    except Exception as e:\n",
      "        return {} \n",
      "# Refactored\n",
      "def swift_delete_bucket(\n",
      "    swift_client: any,\n",
      "    bucket_name: str\n",
      ") -> bool:\n",
      "    try:\n",
      "        swift_client.delete_container(\n",
      "            container = bucket_name\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Created and works\n",
      "def swift_list_buckets(\n",
      "    swift_client: any\n",
      ") -> any:\n",
      "    try:\n",
      "        account_buckets = swift_client.get_account()[1]\n",
      "        return account_buckets\n",
      "    except Exception as e:\n",
      "        return {}\n",
      "# Works\n",
      "def swift_create_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> bool: \n",
      "    # This should be updated to handle 5 GB objects\n",
      "    # It also should handle metadata\n",
      "    try:\n",
      "        swift_client.put_object(\n",
      "            container = bucket_name,\n",
      "            obj = object_path,\n",
      "            contents = object_data,\n",
      "            headers = object_metadata\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Works\n",
      "def swift_check_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> any: \n",
      "    try:\n",
      "        object_metadata = swift_client.head_object(\n",
      "            container = bucket_name,\n",
      "            obj = object_path\n",
      "        )       \n",
      "        return object_metadata\n",
      "    except Exception as e:\n",
      "        return {} \n",
      "# Refactored and works\n",
      "def swift_get_object(\n",
      "    swift_client:any,\n",
      "    bucket_name: str,\n",
      "    object_path: str\n",
      ") -> any:\n",
      "    try:\n",
      "        response = swift_client.get_object(\n",
      "            container = bucket_name,\n",
      "            obj = object_path \n",
      "        )\n",
      "        object_info = response[0]\n",
      "        object_data = response[1]\n",
      "        return {'data': object_data, 'info': object_info}\n",
      "    except Exception as e:\n",
      "        return {}     \n",
      "# Refactored   \n",
      "def swift_remove_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> bool: \n",
      "    try:\n",
      "        swift_client.delete_object(\n",
      "            container = bucket_name, \n",
      "            obj = object_path\n",
      "        )\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        return False\n",
      "# Works\n",
      "def swift_update_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> bool:  \n",
      "    remove = swift_remove_object(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name, \n",
      "        object_path = object_path\n",
      "    )\n",
      "    if not remove:\n",
      "        return False\n",
      "    create = swift_create_object(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name, \n",
      "        object_path = object_path, \n",
      "        object_data = object_data,\n",
      "        object_metadata = object_metadata\n",
      "    )\n",
      "    return create\n",
      "# Works\n",
      "def swift_create_or_update_object(\n",
      "    swift_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    bucket_info = swift_check_bucket(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name\n",
      "    )\n",
      "    \n",
      "    if len(bucket_info) == 0:\n",
      "        creation_status = swift_create_bucket(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "        if not creation_status:\n",
      "            return False\n",
      "    \n",
      "    object_info = swift_check_object(\n",
      "        swift_client = swift_client, \n",
      "        bucket_name = bucket_name, \n",
      "        object_path = object_path\n",
      "    )\n",
      "    \n",
      "    if len(object_info) == 0:\n",
      "        return swift_create_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path, \n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "    else:\n",
      "        return swift_update_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path, \n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "\n",
      "    # Refactored and Works\n",
      "def set_encoded_metadata(\n",
      "    used_client: str,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    encoded_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        key_initial = 'x-object-meta'\n",
      "        for key, value in object_metadata.items():\n",
      "            encoded_key = key_initial + '-' + key\n",
      "            if isinstance(value, list):\n",
      "                encoded_metadata[encoded_key] = 'list=' + ','.join(map(str, value))\n",
      "                continue\n",
      "            encoded_metadata[encoded_key] = str(value)\n",
      "    return encoded_metadata\n",
      "# Refactored and works\n",
      "def get_general_metadata(\n",
      "    used_client: str,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    general_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        key_initial = 'x-object-meta'\n",
      "        for key, value in object_metadata.items():\n",
      "            if not key_initial == key[:len(key_initial)]:\n",
      "                general_metadata[key] = value\n",
      "    return general_metadata\n",
      "# Refactored and works\n",
      "def get_decoded_metadata(\n",
      "    used_client: str,\n",
      "    object_metadata: any\n",
      ") -> any: \n",
      "    decoded_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        key_initial = 'x-object-meta'\n",
      "        for key, value in object_metadata.items():\n",
      "            if key_initial == key[:len(key_initial)]:\n",
      "                decoded_key = key[len(key_initial) + 1:]\n",
      "                if 'list=' in value:\n",
      "                    string_integers = value.split('=')[1]\n",
      "                    values = string_integers.split(',')\n",
      "                    if len(values) == 1 and values[0] == '':\n",
      "                        decoded_metadata[decoded_key] = []\n",
      "                    else:\n",
      "                        try:\n",
      "                            decoded_metadata[decoded_key] = list(map(int, values))\n",
      "                        except:\n",
      "                            decoded_metadata[decoded_key] = list(map(str, values))\n",
      "                    continue\n",
      "                if value.isnumeric():\n",
      "                    decoded_metadata[decoded_key] = int(value)\n",
      "                    continue\n",
      "                decoded_metadata[decoded_key] = value\n",
      "    return decoded_metadata\n",
      "# Refactored and works\n",
      "def set_bucket_names(\n",
      "    storage_parameters: any\n",
      ") -> any:\n",
      "    storage_names = []\n",
      "    bucket_prefix = storage_parameters['bucket-prefix']\n",
      "    ice_id = storage_parameters['ice-id']\n",
      "    user = storage_parameters['user']\n",
      "    storage_names.append(bucket_prefix + '-forwarder-' + ice_id)\n",
      "    storage_names.append(bucket_prefix + '-submitter-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "    storage_names.append(bucket_prefix + '-pipeline-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "    storage_names.append(bucket_prefix + '-experiment-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "    return storage_names\n",
      "# created and works\n",
      "def setup_storage(\n",
      "    storage_parameters: any\n",
      ") -> any:\n",
      "    storage_client = setup_storage_client(\n",
      "        storage_parameters = storage_parameters\n",
      "    ) \n",
      "    \n",
      "    storage_name = set_bucket_names(\n",
      "    storage_parameters = storage_parameters\n",
      "    )\n",
      "    \n",
      "    return storage_client, storage_name\n",
      "# Refactored and works\n",
      "def setup_storage_client(\n",
      "    storage_parameters: any\n",
      ") -> any:\n",
      "    storage_client = None\n",
      "    if storage_parameters['used-client'] == 'swift':\n",
      "        storage_client = swift_setup_client(\n",
      "            pre_auth_url = storage_parameters['pre-auth-url'],\n",
      "            pre_auth_token = storage_parameters['pre-auth-token'],\n",
      "            user_domain_name = storage_parameters['user-domain-name'],\n",
      "            project_domain_name = storage_parameters['project-domain-name'],\n",
      "            project_name = storage_parameters['project-name'],\n",
      "            auth_version = storage_parameters['auth-version']\n",
      "        )\n",
      "    return storage_client\n",
      "# Refactored and works\n",
      "def check_object_metadata(\n",
      "    storage_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> any: \n",
      "    object_metadata = {\n",
      "        'general-meta': {},\n",
      "        'custom-meta': {}\n",
      "    }\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        all_metadata = swift_check_object(\n",
      "        swift_client = storage_client,\n",
      "        bucket_name = bucket_name,\n",
      "        object_path = object_path\n",
      "        ) \n",
      "\n",
      "        general_metadata = {}\n",
      "        custom_metadata = {}\n",
      "        if not len(all_metadata) == 0:\n",
      "            general_metadata = get_general_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = all_metadata\n",
      "            )\n",
      "            custom_metadata = get_decoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = all_metadata\n",
      "            )\n",
      "\n",
      "        object_metadata['general-meta'] = general_metadata\n",
      "        object_metadata['custom-meta'] = custom_metadata\n",
      "\n",
      "    return object_metadata\n",
      "# Refactored and works\n",
      "def get_object_content(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_path: str\n",
      ") -> any:\n",
      "    object_content = {}\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        fetched_object = swift_get_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "        )\n",
      "        object_content['data'] = pickle.loads(fetched_object['data'])\n",
      "        object_content['general-meta'] = get_general_metadata(\n",
      "            used_client = 'swift',\n",
      "            object_metadata = fetched_object['info']\n",
      "        )\n",
      "        object_content['custom-meta'] = get_decoded_metadata(\n",
      "            used_client = 'swift',\n",
      "            object_metadata = fetched_object['info']\n",
      "        )\n",
      "    return object_content\n",
      "# Refactored    \n",
      "def remove_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str\n",
      ") -> bool: \n",
      "    removed = False\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        removed = swift_remove_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "        )\n",
      "    return removed\n",
      "# Refactored and works\n",
      "def create_or_update_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str, \n",
      "    object_path: str, \n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      ") -> any:\n",
      "    success = False\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        formatted_data = pickle.dumps(object_data)\n",
      "        formatted_metadata = set_encoded_metadata(\n",
      "            used_client = 'swift',\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "\n",
      "        success = swift_create_or_update_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path,\n",
      "            object_data = formatted_data,\n",
      "            object_metadata = formatted_metadata\n",
      "        )\n",
      "    return success\n",
      "# Created and works\n",
      "def format_bucket_metadata(\n",
      "    used_client: str,\n",
      "    bucket_metadata: any\n",
      ") -> any:\n",
      "    formatted_metadata = {}\n",
      "    if used_client == 'swift':\n",
      "        relevant_values = {\n",
      "            'x-container-object-count': 'object-count',\n",
      "            'x-container-bytes-used-actual': 'used-bytes',\n",
      "            'last-modified': 'date',\n",
      "            'content-type': 'type'\n",
      "        }\n",
      "        formatted_metadata = {}\n",
      "        for key,value in bucket_metadata.items():\n",
      "            if key in relevant_values:\n",
      "                formatted_key = relevant_values[key]\n",
      "                formatted_metadata[formatted_key] = value\n",
      "    return formatted_metadata\n",
      "# Created and works\n",
      "def format_bucket_objects(\n",
      "    used_client: str,\n",
      "    bucket_objects: any\n",
      ") -> any:\n",
      "    formatted_objects = {}\n",
      "    if used_client == 'swift':\n",
      "        for bucket_object in bucket_objects:\n",
      "            formatted_object_metadata = {\n",
      "                'hash': 'id',\n",
      "                'bytes': 'used-bytes',\n",
      "                'last_modified': 'date'\n",
      "            }\n",
      "            object_key = None\n",
      "            object_metadata = {}\n",
      "            for key, value in bucket_object.items():\n",
      "                if key == 'name':\n",
      "                    object_key = value\n",
      "                if key in formatted_object_metadata:\n",
      "                    formatted_key = formatted_object_metadata[key]\n",
      "                    object_metadata[formatted_key] = value\n",
      "            formatted_objects[object_key] = object_metadata\n",
      "    return formatted_objects\n",
      "# Created and works\n",
      "def format_bucket_info(\n",
      "    used_client: str,\n",
      "    bucket_info: any\n",
      ") -> any:\n",
      "    bucket_metadata = {}\n",
      "    bucket_objects = {}\n",
      "    if used_client == 'swift':\n",
      "        bucket_metadata = format_bucket_metadata(\n",
      "            used_client = used_client,\n",
      "            bucket_metadata = bucket_info['metadata']\n",
      "        )\n",
      "        bucket_objects = format_bucket_objects(\n",
      "            used_client = used_client,\n",
      "            bucket_objects = bucket_info['objects']\n",
      "        )\n",
      "    return {'metadata': bucket_metadata, 'objects': bucket_objects} \n",
      "# Created and works\n",
      "def get_bucket_info(\n",
      "    storage_client: any,\n",
      "    bucket_name: str\n",
      ") -> any:\n",
      "    bucket_info = {}\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        unformatted_bucket_info = swift_check_bucket(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "        bucket_info = format_bucket_info(\n",
      "            used_client = 'swift',\n",
      "            bucket_info = unformatted_bucket_info\n",
      "        )\n",
      "    return bucket_info\n",
      "# Created and works\n",
      "def format_container_info(\n",
      "    used_client: str,\n",
      "    container_info: any\n",
      ") -> any:\n",
      "    formatted_container_info = {}\n",
      "    if used_client == 'swift':\n",
      "        for bucket in container_info:\n",
      "            bucket_name = bucket['name']\n",
      "            bucket_count = bucket['count']\n",
      "            bucket_size = bucket['bytes']\n",
      "            formatted_container_info[bucket_name] = {\n",
      "                'amount': bucket_count,\n",
      "                'size': bucket_size\n",
      "            }\n",
      "    return formatted_container_info\n",
      "# Created and works\n",
      "def get_container_info( \n",
      "    storage_client: any\n",
      ") -> any:\n",
      "    container_info = {}\n",
      "    if is_swift_client(storage_client = storage_client):\n",
      "        unformatted_container_info = swift_list_buckets(\n",
      "            swift_client = storage_client \n",
      "        )\n",
      "        container_info = format_container_info(\n",
      "            used_client = 'swift',\n",
      "            container_info = unformatted_container_info\n",
      "        )\n",
      "    return container_info\n",
      "\n",
      "    # Created and works\n",
      "def set_object_path(\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any\n",
      "):\n",
      "    object_paths = {\n",
      "        'root': 'name',\n",
      "        'code': 'CODE/name',\n",
      "        'slurm': 'CODE/SLURM/name',\n",
      "        'ray': 'CODE/RAY/name',\n",
      "        'data': 'DATA/name',\n",
      "        'artifacts': 'ARTIFACTS/name',\n",
      "        'time': 'TIMES/name'\n",
      "    }\n",
      "\n",
      "    i = 0\n",
      "    path_split = object_paths[object_name].split('/')\n",
      "    for name in path_split:\n",
      "        if name in path_replacers:\n",
      "            replacer = path_replacers[name]\n",
      "            if 0 < len(replacer):\n",
      "                path_split[i] = replacer\n",
      "        i = i + 1\n",
      "    \n",
      "    if not len(path_names) == 0:\n",
      "        path_split.extend(path_names)\n",
      "\n",
      "    object_path = '/'.join(path_split)\n",
      "    #print('Used object path:' + str(object_path))\n",
      "    return object_path\n",
      "# created and works\n",
      "def setup_storage(\n",
      "    storage_parameters: any\n",
      ") -> any:\n",
      "    storage_client = setup_storage_client(\n",
      "        storage_parameters = storage_parameters\n",
      "    ) \n",
      "    \n",
      "    storage_name = set_bucket_names(\n",
      "    storage_parameters = storage_parameters\n",
      "    )\n",
      "    \n",
      "    return storage_client, storage_name\n",
      "# Created and works\n",
      "def check_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any\n",
      ") -> bool:\n",
      "    object_path = set_object_path(\n",
      "        object_name = object_name,\n",
      "        path_replacers = path_replacers,\n",
      "        path_names = path_names\n",
      "    )\n",
      "    # Consider making these functions object storage agnostic\n",
      "    object_metadata = check_object_metadata(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name,\n",
      "        object_path = object_path\n",
      "    )\n",
      "    object_metadata['path'] = object_path\n",
      "    return object_metadata\n",
      "# Created and works\n",
      "def get_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any\n",
      ") -> any:\n",
      "    checked_object = check_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name,\n",
      "        object_name = object_name,\n",
      "        path_replacers = path_replacers,\n",
      "        path_names = path_names\n",
      "    )\n",
      "\n",
      "    object_data = None\n",
      "    if not len(checked_object['general-meta']) == 0:\n",
      "        # Consider making these functions object storage agnostic\n",
      "        object_data = get_object_content(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = checked_object['path']\n",
      "        )\n",
      "\n",
      "    return object_data\n",
      "# Created and Works\n",
      "def set_object(\n",
      "    storage_client: any,\n",
      "    bucket_name: str,\n",
      "    object_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any,\n",
      "    overwrite: bool,\n",
      "    object_data: any,\n",
      "    object_metadata: any\n",
      "):\n",
      "    checked_object = check_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name,\n",
      "        object_name = object_name,\n",
      "        path_replacers = path_replacers,\n",
      "        path_names = path_names\n",
      "    )\n",
      "    \n",
      "    perform = True\n",
      "    if not len(checked_object['general-meta']) == 0 and not overwrite:\n",
      "        perform = False\n",
      "    \n",
      "    if perform:\n",
      "        # Consider making these functions object storage agnostic\n",
      "        create_or_update_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = checked_object['path'],\n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "# Created and works\n",
      "def check_bucket(\n",
      "    storage_client: any,\n",
      "    bucket_name: str\n",
      ") -> any:\n",
      "    return get_bucket_info(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = bucket_name\n",
      "    )\n",
      "# Created and works\n",
      "def check_buckets(\n",
      "    storage_client: any\n",
      ") -> any:\n",
      "    return get_container_info( \n",
      "        storage_client = storage_client\n",
      "    )\n",
      "\n",
      "def gather_time(\n",
      "    storage_client: any,\n",
      "    storage_name: any,\n",
      "    time_group: any,\n",
      "    time_name: any,\n",
      "    start_time: int,\n",
      "    end_time: int\n",
      "):\n",
      "    time_object = get_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = storage_name,\n",
      "        object_name = 'time',\n",
      "        path_replacers = {\n",
      "            'name': time_group\n",
      "        },\n",
      "        path_names = []\n",
      "    )\n",
      "\n",
      "    time_data = {}\n",
      "    time_metadata = {} \n",
      "    if time_object is None:\n",
      "        time_data = {}\n",
      "        time_metadata = general_object_metadata()\n",
      "    else:\n",
      "        time_data = time_object['data']\n",
      "        time_metadata = time_object['custom-meta']\n",
      "    \n",
      "    current_key_amount = len(time_data)\n",
      "    current_key_full = False\n",
      "    current_key = str(current_key_amount)\n",
      "    if 0 < current_key_amount:\n",
      "        time_object = time_data[current_key]\n",
      "        if 0 < time_object['total-seconds']:\n",
      "            current_key_full = True\n",
      "    \n",
      "    changed = False\n",
      "    if 0 < end_time and 0 < current_key_amount and not current_key_full:\n",
      "        stored_start_time = time_data[current_key]['start-time']\n",
      "        time_diff = (end_time-stored_start_time)\n",
      "        time_data[current_key]['end-time'] = end_time\n",
      "        time_data[current_key]['total-seconds'] = round(time_diff,5)\n",
      "        changed = True\n",
      "    else:\n",
      "        next_key_amount = len(time_data) + 1\n",
      "        new_key = str(next_key_amount)\n",
      "    \n",
      "        if 0 < start_time and 0 == end_time:\n",
      "            time_data[new_key] = {\n",
      "                'name': time_name,\n",
      "                'start-time': start_time,\n",
      "                'end-time': 0,\n",
      "                'total-seconds': 0\n",
      "            }\n",
      "            changed = True\n",
      "\n",
      "        if 0 < start_time and 0 < end_time:\n",
      "            time_diff = (end_time-start_time)\n",
      "            time_data[new_key] = {\n",
      "                'name': time_name,\n",
      "                'start-time': start_time,\n",
      "                'end-time': end_time,\n",
      "                'total-seconds': round(time_diff,5)\n",
      "            }\n",
      "            changed = True\n",
      "\n",
      "    if changed:\n",
      "        time_metadata['version'] = time_metadata['version'] + 1\n",
      "        set_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'time',\n",
      "            path_replacers = {\n",
      "                'name': time_group\n",
      "            },\n",
      "            path_names = [],\n",
      "            overwrite = True,\n",
      "            object_data = time_data,\n",
      "            object_metadata = time_metadata \n",
      "        )\n",
      "# Boilerplate END\n",
      "\n",
      "# Pytorch START\n",
      "class CNNClassifier(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 4 * 4)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "\n",
      "def get_general_metrics():\n",
      "    general_metrics = TM.MetricCollection([\n",
      "        TM.classification.MulticlassAccuracy(\n",
      "            num_classes = 10,\n",
      "            average = 'macro'\n",
      "        ),\n",
      "        TM.classification.MulticlassPrecision(\n",
      "            num_classes = 10,\n",
      "            average = 'macro'\n",
      "        ),\n",
      "        TM.classification.MulticlassRecall(\n",
      "            num_classes = 10,\n",
      "            average = 'macro'\n",
      "        )\n",
      "    ])\n",
      "    return general_metrics\n",
      "    \n",
      "def get_class_metrics():\n",
      "    class_metrics = TM.MetricCollection([\n",
      "        TM.classification.MulticlassAccuracy(\n",
      "            num_classes = 10,\n",
      "            average = None\n",
      "        ),\n",
      "        TM.classification.MulticlassPrecision(\n",
      "            num_classes = 10,\n",
      "            average = None\n",
      "        ),\n",
      "        TM.classification.MulticlassRecall(\n",
      "            num_classes = 10,\n",
      "            average = None\n",
      "        )\n",
      "    ])\n",
      "    return class_metrics\n",
      "# Pytorch END\n",
      "\n",
      "@ray.remote\n",
      "def remote_model_training(\n",
      "    storage_client: any,\n",
      "    storage_name: any,\n",
      "    folder_name: str,\n",
      "    seed: int,\n",
      "    train_print_rate: int,\n",
      "    epochs: int,\n",
      "    learning_rate: float,\n",
      "    momentum: float,\n",
      "    train_loader: any, \n",
      "    test_loader: any\n",
      "):\n",
      "    try:\n",
      "        time_start = t.time()\n",
      "    \n",
      "        print('Defining model')\n",
      "        model = CNNClassifier()\n",
      "        criterion = torch.nn.CrossEntropyLoss()\n",
      "        optimizer = torch.optim.SGD(\n",
      "            model.parameters(), \n",
      "            lr = learning_rate, \n",
      "            momentum = momentum\n",
      "        )\n",
      "        torch.manual_seed(seed)\n",
      "    \n",
      "        print('Defining metrics')\n",
      "        general_metrics = get_general_metrics()\n",
      "        class_metrics = get_class_metrics()\n",
      "        \n",
      "        print('Starting model training')\n",
      "        current_epoch = 0\n",
      "        for epoch in range(epochs):\n",
      "            running_loss = 0.0\n",
      "            model.train()\n",
      "            for i, data in enumerate(train_loader):\n",
      "                inputs, labels = data\n",
      "                optimizer.zero_grad()\n",
      "                outputs = model(inputs)\n",
      "                \n",
      "                loss = criterion(outputs, labels)\n",
      "                \n",
      "                loss.backward()\n",
      "                optimizer.step()\n",
      "                running_loss += loss.item()\n",
      "    \n",
      "                preds = torch.max(outputs, 1)[1]\n",
      "        \n",
      "                general_metrics(preds, labels)\n",
      "                \n",
      "                if (i + 1) % train_print_rate == 0:\n",
      "                    avg_loss = running_loss / train_print_rate\n",
      "                    train_general_metrics = general_metrics.compute()\n",
      "                    acc = round(train_general_metrics['MulticlassAccuracy'].item(),3)\n",
      "                    pre = round(train_general_metrics['MulticlassPrecision'].item(),3)\n",
      "                    rec = round(train_general_metrics['MulticlassRecall'].item(),3)\n",
      "                    general_metrics.reset()\n",
      "                    print(f'Epoch: {epoch + 1}/{epochs}, Batch {i + 1}, Loss: {avg_loss}, Accuracy: {acc}, Precision: {pre}, Recall: {rec}')\n",
      "                    running_loss = 0.0 \n",
      "            current_epoch += 1\n",
      "        print('Training complete')\n",
      "        \n",
      "        general_metrics.reset()\n",
      "        \n",
      "        print('Starting model testing')\n",
      "        running_loss = 0.0\n",
      "        predictions = []\n",
      "        with torch.no_grad():\n",
      "            model.eval()\n",
      "            for i, data in enumerate(test_loader):\n",
      "                inputs, labels = data\n",
      "                outputs = model(inputs)\n",
      "                preds = torch.max(outputs, 1)[1]\n",
      "                loss = criterion(outputs, labels)\n",
      "                general_metrics(preds, labels)\n",
      "                class_metrics(preds, labels)\n",
      "                predictions.extend(preds.tolist())\n",
      "                running_loss += loss.item()\n",
      "        print('Testing complete')\n",
      "    \n",
      "        print('Storing created artifacts')\n",
      "        \n",
      "        test_general_metrics = general_metrics.compute()\n",
      "        test_class_metrics = class_metrics.compute()\n",
      "    \n",
      "        general_metrics.reset()\n",
      "        class_metrics.reset()\n",
      "        \n",
      "        print('Storing predictions')\n",
      "        \n",
      "        set_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'artifacts',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [\n",
      "                'predictions'\n",
      "            ],\n",
      "            overwrite = True,\n",
      "            object_data = predictions,\n",
      "            object_metadata = general_object_metadata()\n",
      "        )\n",
      "    \n",
      "        print('Formatting model parameters')\n",
      "        model_parameters = model.state_dict()\n",
      "        optimizer_parameters = optimizer.state_dict()\n",
      "    \n",
      "        parameters = {\n",
      "            'epoch': current_epoch,\n",
      "            'model': model_parameters,\n",
      "            'optimizer': optimizer_parameters\n",
      "        }\n",
      "\n",
      "        print('Storing parameters')\n",
      "        \n",
      "        set_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'artifacts',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [\n",
      "                'parameters'\n",
      "            ],\n",
      "            overwrite = True,\n",
      "            object_data = parameters,\n",
      "            object_metadata = general_object_metadata()\n",
      "        )\n",
      "    \n",
      "        print('Formatting model metrics')\n",
      "        accuracy = test_general_metrics['MulticlassAccuracy'].item()\n",
      "        precision = test_general_metrics['MulticlassPrecision'].item()\n",
      "        recall = test_general_metrics['MulticlassRecall'].item()\n",
      "    \n",
      "        class_accuracy = test_class_metrics['MulticlassAccuracy'].tolist()\n",
      "        class_precision = test_class_metrics['MulticlassPrecision'].tolist()\n",
      "        class_recall = test_class_metrics['MulticlassRecall'].tolist()\n",
      "    \n",
      "        metrics = {\n",
      "            'name': 'Convolutional-neural-network-classifier',\n",
      "            'accuracy': accuracy,\n",
      "            'precision': precision,\n",
      "            'recall': recall,\n",
      "            'class-accuracy': class_accuracy,\n",
      "            'class-precision': class_precision,\n",
      "            'class-recall': class_recall\n",
      "        }\n",
      "\n",
      "        print('Storing metrics')\n",
      "        \n",
      "        set_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'artifacts',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [\n",
      "                'metrics'\n",
      "            ],\n",
      "            overwrite = True,\n",
      "            object_data = metrics,\n",
      "            object_metadata = general_object_metadata()\n",
      "        )\n",
      "    \n",
      "        time_end = t.time()\n",
      "        \n",
      "        gather_time(\n",
      "            storage_client = storage_client,\n",
      "            storage_name = storage_name,\n",
      "            time_group = 'ray-jobs',\n",
      "            time_name = 'remote-model-training',\n",
      "            start_time = time_start,\n",
      "            end_time = time_end\n",
      "        )\n",
      "\n",
      "        return True\n",
      "    except Exception as e:\n",
      "        print(e)\n",
      "        return False\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    time_start = t.time()\n",
      "    print('Starting ray job')\n",
      "    print('Ray version is:' + str(ray.__version__))\n",
      "    print('Swiftclient version is:' + str(sc.__version__))\n",
      "    print('Torch version is:' + str(torch.__version__))\n",
      "    print('Torchmetrics version is:' + str(TM.__version__))\n",
      "    \n",
      "    input = json.loads(sys.argv[1])\n",
      "\n",
      "    storage_parameters = input['storage-parameters']\n",
      "\n",
      "    print('Setting storage client')\n",
      "    storage_client, storage_names = setup_storage(\n",
      "        storage_parameters = storage_parameters\n",
      "    )\n",
      "    print('Storage client setup')\n",
      "    \n",
      "    pipeline_storage = storage_names[-2]\n",
      "\n",
      "    print('Used bucket:' + str(pipeline_storage))\n",
      "\n",
      "    job_parameters = input['job-parameters']\n",
      "\n",
      "    folder_name = job_parameters['folder-name']\n",
      "    train_print_rate = job_parameters['train-print-rate']\n",
      "    seed = job_parameters['hp-seed']\n",
      "    epochs = job_parameters['hp-epochs']\n",
      "    learning_rate = job_parameters['hp-learning-rate']\n",
      "    momentum = job_parameters['hp-momentum']\n",
      "\n",
      "    print('Getting training data')\n",
      "\n",
      "    train_loader_object = get_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = pipeline_storage,\n",
      "        object_name = 'data',\n",
      "        path_replacers = {\n",
      "            'name': folder_name\n",
      "        },\n",
      "        path_names = [\n",
      "            'train'\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    train_loader = train_loader_object['data']\n",
      "    \n",
      "    print('Getting testing data')\n",
      "\n",
      "    test_loader_object = get_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = pipeline_storage,\n",
      "        object_name = 'data',\n",
      "        path_replacers = {\n",
      "            'name': folder_name\n",
      "        },\n",
      "        path_names = [\n",
      "            'test'\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    test_loader = test_loader_object['data']\n",
      "\n",
      "    print('Data loaded')\n",
      "\n",
      "    print('Starting training')\n",
      "    \n",
      "    training_status = ray.get(remote_model_training.remote(\n",
      "        storage_client = storage_client,\n",
      "        storage_name = pipeline_storage,\n",
      "        folder_name = folder_name,\n",
      "        seed = seed,\n",
      "        train_print_rate = train_print_rate,\n",
      "        epochs = epochs,\n",
      "        learning_rate = learning_rate,\n",
      "        momentum = momentum,\n",
      "        train_loader = train_loader,\n",
      "        test_loader = test_loader\n",
      "    ))\n",
      "\n",
      "    print('Training success:' + str(training_status))\n",
      "\n",
      "    time_end = t.time()\n",
      "\n",
      "    gather_time(\n",
      "        storage_client = storage_client,\n",
      "        storage_name = pipeline_storage,\n",
      "        time_group = 'ray-jobs',\n",
      "        time_name = 'train-fmnist-cnn',\n",
      "        start_time = time_start,\n",
      "        end_time = time_end\n",
      "    )\n",
      "\n",
      "    print('Ray job Complete')\n",
      "\n",
      "set_code(\n",
      "    storage_client = storage_client,\n",
      "    storage_name = storage_names[-2],\n",
      "    file_path = 'scripts/ray/train-fmnist-cnn.py',\n",
      "    overwrite = True\n",
      ")\n",
      "## Pipeline\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "\n",
      "import kfp\n",
      "import kfp.dsl as dsl\n",
      "from kfp.aws import use_aws_secret\n",
      "from kfp.v2.dsl import (\n",
      "    component,\n",
      "    Input,\n",
      "    Output,\n",
      "    Dataset,\n",
      "    Metrics,\n",
      "    Artifact,\n",
      "    Model\n",
      ")\n",
      "### Preprocessing\n",
      "\n",
      "Please create components folder before running this block.\n",
      "@component( \n",
      "    base_image = \"python:3.10\",\n",
      "    packages_to_install = [\n",
      "        \"python-swiftclient\",\n",
      "        \"torch==2.4.0+cpu\", \n",
      "        \"torchvision==0.19.0+cpu\"\n",
      "    ],\n",
      "    pip_index_urls=[\n",
      "        \"https://pypi.org/simple\",\n",
      "        \"https://download.pytorch.org/whl/cpu\",\n",
      "        \"https://download.pytorch.org/whl/cpu\"\n",
      "    ],\n",
      "    output_component_file = 'components/preprocess_component.yaml',\n",
      ")\n",
      "def preprocess( \n",
      "    storage_parameters: dict,\n",
      "    integration_parameters: dict,\n",
      ") -> bool:\n",
      "    import time as t \n",
      "    \n",
      "    import logging\n",
      "    import swiftclient as sc\n",
      "    import pickle\n",
      "    \n",
      "    import torch\n",
      "    from torchvision import datasets\n",
      "    import torchvision.transforms as T\n",
      "\n",
      "    import re\n",
      "\n",
      "    # BOILERPLATE START\n",
      "\n",
      "    def set_formatted_user(\n",
      "        user: str   \n",
      "    ) -> any:\n",
      "        return re.sub(r'[^a-z0-9]+', '-', user)\n",
      "    def general_object_metadata():\n",
      "        general_object_metadata = {\n",
      "            'version': 1\n",
      "        }\n",
      "        return general_object_metadata\n",
      "    # Works\n",
      "    def is_swift_client(\n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        return isinstance(storage_client, sc.Connection)\n",
      "    # Works\n",
      "    def swift_setup_client(\n",
      "        pre_auth_url: str,\n",
      "        pre_auth_token: str,\n",
      "        user_domain_name: str,\n",
      "        project_domain_name: str,\n",
      "        project_name: str,\n",
      "        auth_version: str\n",
      "    ) -> any:\n",
      "        swift_client = sc.Connection(\n",
      "            preauthurl = pre_auth_url,\n",
      "            preauthtoken = pre_auth_token,\n",
      "            os_options = {\n",
      "                'user_domain_name': user_domain_name,\n",
      "                'project_domain_name': project_domain_name,\n",
      "                'project_name': project_name\n",
      "            },\n",
      "            auth_version = auth_version\n",
      "        )\n",
      "        return swift_client\n",
      "    # Works\n",
      "    def swift_create_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> bool:\n",
      "        try:\n",
      "            swift_client.put_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_check_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name:str\n",
      "    ) -> any:\n",
      "        try:\n",
      "            bucket_info = swift_client.get_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            bucket_metadata = bucket_info[0]\n",
      "            list_of_objects = bucket_info[1]\n",
      "            return {'metadata': bucket_metadata, 'objects': list_of_objects}\n",
      "        except Exception as e:\n",
      "            return {} \n",
      "    # Refactored\n",
      "    def swift_delete_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> bool:\n",
      "        try:\n",
      "            swift_client.delete_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Created and works\n",
      "    def swift_list_buckets(\n",
      "        swift_client: any\n",
      "    ) -> any:\n",
      "        try:\n",
      "            account_buckets = swift_client.get_account()[1]\n",
      "            return account_buckets\n",
      "        except Exception as e:\n",
      "            return {}\n",
      "    # Works\n",
      "    def swift_create_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> bool: \n",
      "        # This should be updated to handle 5 GB objects\n",
      "        # It also should handle metadata\n",
      "        try:\n",
      "            swift_client.put_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path,\n",
      "                contents = object_data,\n",
      "                headers = object_metadata\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_check_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> any: \n",
      "        try:\n",
      "            object_metadata = swift_client.head_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path\n",
      "            )       \n",
      "            return object_metadata\n",
      "        except Exception as e:\n",
      "            return {} \n",
      "    # Refactored and works\n",
      "    def swift_get_object(\n",
      "        swift_client:any,\n",
      "        bucket_name: str,\n",
      "        object_path: str\n",
      "    ) -> any:\n",
      "        try:\n",
      "            response = swift_client.get_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path \n",
      "            )\n",
      "            object_info = response[0]\n",
      "            object_data = response[1]\n",
      "            return {'data': object_data, 'info': object_info}\n",
      "        except Exception as e:\n",
      "            return {}     \n",
      "    # Refactored   \n",
      "    def swift_remove_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> bool: \n",
      "        try:\n",
      "            swift_client.delete_object(\n",
      "                container = bucket_name, \n",
      "                obj = object_path\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_update_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> bool:  \n",
      "        remove = swift_remove_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path\n",
      "        )\n",
      "        if not remove:\n",
      "            return False\n",
      "        create = swift_create_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path, \n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "        return create\n",
      "    # Works\n",
      "    def swift_create_or_update_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        bucket_info = swift_check_bucket(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "        \n",
      "        if len(bucket_info) == 0:\n",
      "            creation_status = swift_create_bucket(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name\n",
      "            )\n",
      "            if not creation_status:\n",
      "                return False\n",
      "        \n",
      "        object_info = swift_check_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path\n",
      "        )\n",
      "        \n",
      "        if len(object_info) == 0:\n",
      "            return swift_create_object(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name, \n",
      "                object_path = object_path, \n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "        else:\n",
      "            return swift_update_object(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name, \n",
      "                object_path = object_path, \n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "    # Refactored and Works\n",
      "    def set_encoded_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        encoded_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                encoded_key = key_initial + '-' + key\n",
      "                if isinstance(value, list):\n",
      "                    encoded_metadata[encoded_key] = 'list=' + ','.join(map(str, value))\n",
      "                    continue\n",
      "                encoded_metadata[encoded_key] = str(value)\n",
      "        return encoded_metadata\n",
      "    # Refactored and works\n",
      "    def get_general_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        general_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                if not key_initial == key[:len(key_initial)]:\n",
      "                    general_metadata[key] = value\n",
      "        return general_metadata\n",
      "    # Refactored and works\n",
      "    def get_decoded_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any: \n",
      "        decoded_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                if key_initial == key[:len(key_initial)]:\n",
      "                    decoded_key = key[len(key_initial) + 1:]\n",
      "                    if 'list=' in value:\n",
      "                        string_integers = value.split('=')[1]\n",
      "                        values = string_integers.split(',')\n",
      "                        if len(values) == 1 and values[0] == '':\n",
      "                            decoded_metadata[decoded_key] = []\n",
      "                        else:\n",
      "                            try:\n",
      "                                decoded_metadata[decoded_key] = list(map(int, values))\n",
      "                            except:\n",
      "                                decoded_metadata[decoded_key] = list(map(str, values))\n",
      "                        continue\n",
      "                    if value.isnumeric():\n",
      "                        decoded_metadata[decoded_key] = int(value)\n",
      "                        continue\n",
      "                    decoded_metadata[decoded_key] = value\n",
      "        return decoded_metadata\n",
      "    # Refactored and works\n",
      "    def set_bucket_names(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_names = []\n",
      "        bucket_prefix = storage_parameters['bucket-prefix']\n",
      "        ice_id = storage_parameters['ice-id']\n",
      "        user = storage_parameters['user']\n",
      "        storage_names.append(bucket_prefix + '-forwarder-' + ice_id)\n",
      "        storage_names.append(bucket_prefix + '-submitter-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        storage_names.append(bucket_prefix + '-pipeline-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        storage_names.append(bucket_prefix + '-experiment-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        return storage_names\n",
      "    # created and works\n",
      "    def setup_storage(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = setup_storage_client(\n",
      "            storage_parameters = storage_parameters\n",
      "        ) \n",
      "        \n",
      "        storage_name = set_bucket_names(\n",
      "        storage_parameters = storage_parameters\n",
      "        )\n",
      "        \n",
      "        return storage_client, storage_name\n",
      "    # Refactored and works\n",
      "    def setup_storage_client(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = None\n",
      "        if storage_parameters['used-client'] == 'swift':\n",
      "            storage_client = swift_setup_client(\n",
      "                pre_auth_url = storage_parameters['pre-auth-url'],\n",
      "                pre_auth_token = storage_parameters['pre-auth-token'],\n",
      "                user_domain_name = storage_parameters['user-domain-name'],\n",
      "                project_domain_name = storage_parameters['project-domain-name'],\n",
      "                project_name = storage_parameters['project-name'],\n",
      "                auth_version = storage_parameters['auth-version']\n",
      "            )\n",
      "        return storage_client\n",
      "    # Refactored and works\n",
      "    def check_object_metadata(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> any: \n",
      "        object_metadata = {\n",
      "            'general-meta': {},\n",
      "            'custom-meta': {}\n",
      "        }\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            all_metadata = swift_check_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "            ) \n",
      "\n",
      "            general_metadata = {}\n",
      "            custom_metadata = {}\n",
      "            if not len(all_metadata) == 0:\n",
      "                general_metadata = get_general_metadata(\n",
      "                    used_client = 'swift',\n",
      "                    object_metadata = all_metadata\n",
      "                )\n",
      "                custom_metadata = get_decoded_metadata(\n",
      "                    used_client = 'swift',\n",
      "                    object_metadata = all_metadata\n",
      "                )\n",
      "\n",
      "            object_metadata['general-meta'] = general_metadata\n",
      "            object_metadata['custom-meta'] = custom_metadata\n",
      "\n",
      "        return object_metadata\n",
      "    # Refactored and works\n",
      "    def get_object_content(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_path: str\n",
      "    ) -> any:\n",
      "        object_content = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            fetched_object = swift_get_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path\n",
      "            )\n",
      "            object_content['data'] = pickle.loads(fetched_object['data'])\n",
      "            object_content['general-meta'] = get_general_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = fetched_object['info']\n",
      "            )\n",
      "            object_content['custom-meta'] = get_decoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = fetched_object['info']\n",
      "            )\n",
      "        return object_content\n",
      "    # Refactored    \n",
      "    def remove_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> bool: \n",
      "        removed = False\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            removed = swift_remove_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path\n",
      "            )\n",
      "        return removed\n",
      "    # Refactored and works\n",
      "    def create_or_update_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        success = False\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            formatted_data = pickle.dumps(object_data)\n",
      "            formatted_metadata = set_encoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "\n",
      "            success = swift_create_or_update_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path,\n",
      "                object_data = formatted_data,\n",
      "                object_metadata = formatted_metadata\n",
      "            )\n",
      "        return success\n",
      "    # Created and works\n",
      "    def format_bucket_metadata(\n",
      "        used_client: str,\n",
      "        bucket_metadata: any\n",
      "    ) -> any:\n",
      "        formatted_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            relevant_values = {\n",
      "                'x-container-object-count': 'object-count',\n",
      "                'x-container-bytes-used-actual': 'used-bytes',\n",
      "                'last-modified': 'date',\n",
      "                'content-type': 'type'\n",
      "            }\n",
      "            formatted_metadata = {}\n",
      "            for key,value in bucket_metadata.items():\n",
      "                if key in relevant_values:\n",
      "                    formatted_key = relevant_values[key]\n",
      "                    formatted_metadata[formatted_key] = value\n",
      "        return formatted_metadata\n",
      "    # Created and works\n",
      "    def format_bucket_objects(\n",
      "        used_client: str,\n",
      "        bucket_objects: any\n",
      "    ) -> any:\n",
      "        formatted_objects = {}\n",
      "        if used_client == 'swift':\n",
      "            for bucket_object in bucket_objects:\n",
      "                formatted_object_metadata = {\n",
      "                    'hash': 'id',\n",
      "                    'bytes': 'used-bytes',\n",
      "                    'last_modified': 'date'\n",
      "                }\n",
      "                object_key = None\n",
      "                object_metadata = {}\n",
      "                for key, value in bucket_object.items():\n",
      "                    if key == 'name':\n",
      "                        object_key = value\n",
      "                    if key in formatted_object_metadata:\n",
      "                        formatted_key = formatted_object_metadata[key]\n",
      "                        object_metadata[formatted_key] = value\n",
      "                formatted_objects[object_key] = object_metadata\n",
      "        return formatted_objects\n",
      "    # Created and works\n",
      "    def format_bucket_info(\n",
      "        used_client: str,\n",
      "        bucket_info: any\n",
      "    ) -> any:\n",
      "        bucket_metadata = {}\n",
      "        bucket_objects = {}\n",
      "        if used_client == 'swift':\n",
      "            bucket_metadata = format_bucket_metadata(\n",
      "                used_client = used_client,\n",
      "                bucket_metadata = bucket_info['metadata']\n",
      "            )\n",
      "            bucket_objects = format_bucket_objects(\n",
      "                used_client = used_client,\n",
      "                bucket_objects = bucket_info['objects']\n",
      "            )\n",
      "        return {'metadata': bucket_metadata, 'objects': bucket_objects} \n",
      "    # Created and works\n",
      "    def get_bucket_info(\n",
      "        storage_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> any:\n",
      "        bucket_info = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            unformatted_bucket_info = swift_check_bucket(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name\n",
      "            )\n",
      "            bucket_info = format_bucket_info(\n",
      "                used_client = 'swift',\n",
      "                bucket_info = unformatted_bucket_info\n",
      "            )\n",
      "        return bucket_info\n",
      "    # Created and works\n",
      "    def format_container_info(\n",
      "        used_client: str,\n",
      "        container_info: any\n",
      "    ) -> any:\n",
      "        formatted_container_info = {}\n",
      "        if used_client == 'swift':\n",
      "            for bucket in container_info:\n",
      "                bucket_name = bucket['name']\n",
      "                bucket_count = bucket['count']\n",
      "                bucket_size = bucket['bytes']\n",
      "                formatted_container_info[bucket_name] = {\n",
      "                    'amount': bucket_count,\n",
      "                    'size': bucket_size\n",
      "                }\n",
      "        return formatted_container_info\n",
      "    # Created and works\n",
      "    def get_container_info( \n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        container_info = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            unformatted_container_info = swift_list_buckets(\n",
      "                swift_client = storage_client \n",
      "            )\n",
      "            container_info = format_container_info(\n",
      "                used_client = 'swift',\n",
      "                container_info = unformatted_container_info\n",
      "            )\n",
      "        return container_info\n",
      "    # Created and works\n",
      "    def set_object_path(\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ):\n",
      "        object_paths = {\n",
      "            'root': 'name',\n",
      "            'code': 'CODE/name',\n",
      "            'slurm': 'CODE/SLURM/name',\n",
      "            'ray': 'CODE/RAY/name',\n",
      "            'data': 'DATA/name',\n",
      "            'artifacts': 'ARTIFACTS/name',\n",
      "            'time': 'TIMES/name'\n",
      "        }\n",
      "\n",
      "        i = 0\n",
      "        path_split = object_paths[object_name].split('/')\n",
      "        for name in path_split:\n",
      "            if name in path_replacers:\n",
      "                replacer = path_replacers[name]\n",
      "                if 0 < len(replacer):\n",
      "                    path_split[i] = replacer\n",
      "            i = i + 1\n",
      "        \n",
      "        if not len(path_names) == 0:\n",
      "            path_split.extend(path_names)\n",
      "\n",
      "        object_path = '/'.join(path_split)\n",
      "        #print('Used object path:' + str(object_path))\n",
      "        return object_path\n",
      "    # created and works\n",
      "    def setup_storage(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = setup_storage_client(\n",
      "            storage_parameters = storage_parameters\n",
      "        ) \n",
      "        \n",
      "        storage_name = set_bucket_names(\n",
      "        storage_parameters = storage_parameters\n",
      "        )\n",
      "        \n",
      "        return storage_client, storage_name\n",
      "    # Created and works\n",
      "    def check_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ) -> bool:\n",
      "        object_path = set_object_path(\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "        # Consider making these functions object storage agnostic\n",
      "        object_metadata = check_object_metadata(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "        )\n",
      "        object_metadata['path'] = object_path\n",
      "        return object_metadata\n",
      "    # Created and works\n",
      "    def get_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ) -> any:\n",
      "        checked_object = check_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "\n",
      "        object_data = None\n",
      "        if not len(checked_object['general-meta']) == 0:\n",
      "            # Consider making these functions object storage agnostic\n",
      "            object_data = get_object_content(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = checked_object['path']\n",
      "            )\n",
      "\n",
      "        return object_data\n",
      "    # Created and Works\n",
      "    def set_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any,\n",
      "        overwrite: bool,\n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ):\n",
      "        checked_object = check_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "        \n",
      "        perform = True\n",
      "        if not len(checked_object['general-meta']) == 0 and not overwrite:\n",
      "            perform = False\n",
      "        \n",
      "        if perform:\n",
      "            # Consider making these functions object storage agnostic\n",
      "            create_or_update_object(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = checked_object['path'],\n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "    # Created and works\n",
      "    def check_bucket(\n",
      "        storage_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> any:\n",
      "        return get_bucket_info(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "    # Created and works\n",
      "    def check_buckets(\n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        return get_container_info( \n",
      "            storage_client = storage_client\n",
      "        )\n",
      "\n",
      "    def gather_time(\n",
      "        storage_client: any,\n",
      "        storage_name: any,\n",
      "        time_group: any,\n",
      "        time_name: any,\n",
      "        start_time: int,\n",
      "        end_time: int\n",
      "    ):\n",
      "        time_object = get_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'time',\n",
      "            path_replacers = {\n",
      "                'name': time_group\n",
      "            },\n",
      "            path_names = []\n",
      "        )\n",
      "\n",
      "        time_data = {}\n",
      "        time_metadata = {} \n",
      "        if time_object is None:\n",
      "            time_data = {}\n",
      "            time_metadata = general_object_metadata()\n",
      "        else:\n",
      "            time_data = time_object['data']\n",
      "            time_metadata = time_object['custom-meta']\n",
      "        \n",
      "        current_key_amount = len(time_data)\n",
      "        current_key_full = False\n",
      "        current_key = str(current_key_amount)\n",
      "        if 0 < current_key_amount:\n",
      "            time_object = time_data[current_key]\n",
      "            if 0 < time_object['total-seconds']:\n",
      "                current_key_full = True\n",
      "        \n",
      "        changed = False\n",
      "        if 0 < end_time and 0 < current_key_amount and not current_key_full:\n",
      "            stored_start_time = time_data[current_key]['start-time']\n",
      "            time_diff = (end_time-stored_start_time)\n",
      "            time_data[current_key]['end-time'] = end_time\n",
      "            time_data[current_key]['total-seconds'] = round(time_diff,5)\n",
      "            changed = True\n",
      "        else:\n",
      "            next_key_amount = len(time_data) + 1\n",
      "            new_key = str(next_key_amount)\n",
      "        \n",
      "            if 0 < start_time and 0 == end_time:\n",
      "                time_data[new_key] = {\n",
      "                    'name': time_name,\n",
      "                    'start-time': start_time,\n",
      "                    'end-time': 0,\n",
      "                    'total-seconds': 0\n",
      "                }\n",
      "                changed = True\n",
      "\n",
      "            if 0 < start_time and 0 < end_time:\n",
      "                time_diff = (end_time-start_time)\n",
      "                time_data[new_key] = {\n",
      "                    'name': time_name,\n",
      "                    'start-time': start_time,\n",
      "                    'end-time': end_time,\n",
      "                    'total-seconds': round(time_diff,5)\n",
      "                }\n",
      "                changed = True\n",
      "\n",
      "        if changed:\n",
      "            time_metadata['version'] = time_metadata['version'] + 1\n",
      "            set_object(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = storage_name,\n",
      "                object_name = 'time',\n",
      "                path_replacers = {\n",
      "                    'name': time_group\n",
      "                },\n",
      "                path_names = [],\n",
      "                overwrite = True,\n",
      "                object_data = time_data,\n",
      "                object_metadata = time_metadata \n",
      "            )\n",
      "\n",
      "    ## Boilerplate END\n",
      "\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "\n",
      "    component_time_start = t.time()\n",
      "\n",
      "    storage_client, storage_names = setup_storage( \n",
      "        storage_parameters = storage_parameters\n",
      "    )\n",
      "\n",
      "    logger.info('Storage setup')\n",
      "\n",
      "    logger.info('Variable setup')\n",
      "    \n",
      "    pipeline_bucket = storage_names[-2]\n",
      "\n",
      "    logger.info('Utilized bucket: ' + str(pipeline_bucket))\n",
      "\n",
      "    folder_name = integration_parameters['folder-name']\n",
      "    train_batch_size = integration_parameters['ray-parameters']['job-parameters']['hp-train-batch-size']\n",
      "    test_batch_size = integration_parameters['ray-parameters']['job-parameters']['hp-test-batch-size']\n",
      "\n",
      "    logger.info('Checking train loader')\n",
      "\n",
      "    train_loader_metadata = check_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = pipeline_bucket,\n",
      "        object_name = 'data',\n",
      "        path_replacers = {\n",
      "            'name': folder_name\n",
      "        },\n",
      "        path_names = [\n",
      "            'train'\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    if len(train_loader_metadata['general-meta']) == 0:\n",
      "        logger.info('Preprocessing train')\n",
      "        \n",
      "        train_transform = T.Compose([\n",
      "            T.ToTensor(),\n",
      "            T.Normalize((0.5,), (0.5,))\n",
      "        ])\n",
      "\n",
      "        train_data = datasets.FashionMNIST(\n",
      "            root = './data', \n",
      "            train = True, \n",
      "            download = True, \n",
      "            transform = train_transform\n",
      "        )\n",
      "        \n",
      "        train_loader = torch.utils.data.DataLoader(\n",
      "            train_data, \n",
      "            batch_size = train_batch_size, \n",
      "            shuffle = True\n",
      "        )\n",
      "\n",
      "        set_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = pipeline_bucket,\n",
      "            object_name = 'data',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [\n",
      "                'train'\n",
      "            ],\n",
      "            overwrite = True,\n",
      "            object_data = train_loader,\n",
      "            object_metadata = general_object_metadata()\n",
      "        )\n",
      "        logger.info('Train loader stored')\n",
      "\n",
      "    logger.info('Checking test loader')\n",
      "\n",
      "    test_loader_metadata = check_object(\n",
      "        storage_client = storage_client,\n",
      "        bucket_name = pipeline_bucket,\n",
      "        object_name = 'data',\n",
      "        path_replacers = {\n",
      "            'name': folder_name\n",
      "        },\n",
      "        path_names = [\n",
      "            'test'\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    if len(test_loader_metadata['general-meta']) == 0:\n",
      "        logger.info('Preprocessing test')\n",
      "\n",
      "        test_transform = T.Compose([\n",
      "            T.ToTensor(),\n",
      "            T.Normalize((0.5,), (0.5,))\n",
      "        ])\n",
      "\n",
      "        test_data = datasets.FashionMNIST(\n",
      "            root = './data', \n",
      "            train = False, \n",
      "            download = True, \n",
      "            transform = test_transform\n",
      "        )\n",
      "\n",
      "        test_loader = torch.utils.data.DataLoader(\n",
      "            test_data, \n",
      "            batch_size = test_batch_size, \n",
      "            shuffle = False\n",
      "        )\n",
      "\n",
      "        set_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = pipeline_bucket,\n",
      "            object_name = 'data',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [ \n",
      "                'test'\n",
      "            ],\n",
      "            overwrite = True,\n",
      "            object_data = test_loader,\n",
      "            object_metadata = general_object_metadata()\n",
      "        )\n",
      "        logger.info('Test loader stored')\n",
      "        \n",
      "    component_time_end = t.time()\n",
      "    \n",
      "    gather_time(\n",
      "        storage_client = storage_client,\n",
      "        storage_name = pipeline_bucket,\n",
      "        time_group = 'components',\n",
      "        time_name = 'cloud-hpc-preprocess',\n",
      "        start_time = component_time_start,\n",
      "        end_time = component_time_end\n",
      "    )\n",
      "\n",
      "    logger.info('Preprocess complete')\n",
      "\n",
      "    return True\n",
      "### Training\n",
      "from typing import NamedTuple\n",
      "\n",
      "@component(\n",
      "    base_image = \"python:3.10\",\n",
      "    packages_to_install = [\n",
      "        \"python-swiftclient\",\n",
      "        \"ray[default]\",\n",
      "        \"mlflow~=2.12.2\", \n",
      "        \"boto3~=1.21.0\",\n",
      "        \"numpy\",\n",
      "        \"torch==2.4.0+cpu\" \n",
      "    ],\n",
      "    pip_index_urls=[\n",
      "        \"https://pypi.org/simple\",\n",
      "        \"https://pypi.org/simple\",\n",
      "        \"https://pypi.org/simple\",\n",
      "        \"https://pypi.org/simple\",\n",
      "        \"https://pypi.org/simple\",\n",
      "        \"https://download.pytorch.org/whl/cpu\"\n",
      "    ],\n",
      "    output_component_file = 'components/train_component.yaml',\n",
      ") \n",
      "def train(   \n",
      "    storage_parameters: dict,\n",
      "    integration_parameters: dict,\n",
      "    mlflow_parameters: dict\n",
      ") -> NamedTuple(\"Output\", [('storage_uri', str), ('run_id', str),]):\n",
      "    \n",
      "    import logging\n",
      "    from collections import namedtuple\n",
      "    \n",
      "    import swiftclient as sc\n",
      "    import pickle\n",
      "    # saved_model: Output[Model\n",
      "    import requests\n",
      "    import json\n",
      "    \n",
      "    from datetime import datetime\n",
      "    import time as t\n",
      "        \n",
      "    import os\n",
      "    import mlflow\n",
      "    import mlflow.pytorch\n",
      "    \n",
      "    from ray.job_submission import JobSubmissionClient\n",
      "    from ray.job_submission import JobStatus\n",
      "\n",
      "    import torch\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    import numpy as np\n",
      "    import re\n",
      "\n",
      "    # BOILERPLATE START\n",
      "\n",
      "    def set_formatted_user(\n",
      "        user: str   \n",
      "    ) -> any:\n",
      "        return re.sub(r'[^a-z0-9]+', '-', user)\n",
      "    def general_object_metadata():\n",
      "        general_object_metadata = {\n",
      "            'version': 1\n",
      "        }\n",
      "        return general_object_metadata\n",
      "    # Works\n",
      "    def is_swift_client(\n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        return isinstance(storage_client, sc.Connection)\n",
      "    # Works\n",
      "    def swift_setup_client(\n",
      "        pre_auth_url: str,\n",
      "        pre_auth_token: str,\n",
      "        user_domain_name: str,\n",
      "        project_domain_name: str,\n",
      "        project_name: str,\n",
      "        auth_version: str\n",
      "    ) -> any:\n",
      "        swift_client = sc.Connection(\n",
      "            preauthurl = pre_auth_url,\n",
      "            preauthtoken = pre_auth_token,\n",
      "            os_options = {\n",
      "                'user_domain_name': user_domain_name,\n",
      "                'project_domain_name': project_domain_name,\n",
      "                'project_name': project_name\n",
      "            },\n",
      "            auth_version = auth_version\n",
      "        )\n",
      "        return swift_client\n",
      "    # Works\n",
      "    def swift_create_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> bool:\n",
      "        try:\n",
      "            swift_client.put_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_check_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name:str\n",
      "    ) -> any:\n",
      "        try:\n",
      "            bucket_info = swift_client.get_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            bucket_metadata = bucket_info[0]\n",
      "            list_of_objects = bucket_info[1]\n",
      "            return {'metadata': bucket_metadata, 'objects': list_of_objects}\n",
      "        except Exception as e:\n",
      "            return {} \n",
      "    # Refactored\n",
      "    def swift_delete_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> bool:\n",
      "        try:\n",
      "            swift_client.delete_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Created and works\n",
      "    def swift_list_buckets(\n",
      "        swift_client: any\n",
      "    ) -> any:\n",
      "        try:\n",
      "            account_buckets = swift_client.get_account()[1]\n",
      "            return account_buckets\n",
      "        except Exception as e:\n",
      "            return {}\n",
      "    # Works\n",
      "    def swift_create_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> bool: \n",
      "        # This should be updated to handle 5 GB objects\n",
      "        # It also should handle metadata\n",
      "        try:\n",
      "            swift_client.put_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path,\n",
      "                contents = object_data,\n",
      "                headers = object_metadata\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_check_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> any: \n",
      "        try:\n",
      "            object_metadata = swift_client.head_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path\n",
      "            )       \n",
      "            return object_metadata\n",
      "        except Exception as e:\n",
      "            return {} \n",
      "    # Refactored and works\n",
      "    def swift_get_object(\n",
      "        swift_client:any,\n",
      "        bucket_name: str,\n",
      "        object_path: str\n",
      "    ) -> any:\n",
      "        try:\n",
      "            response = swift_client.get_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path \n",
      "            )\n",
      "            object_info = response[0]\n",
      "            object_data = response[1]\n",
      "            return {'data': object_data, 'info': object_info}\n",
      "        except Exception as e:\n",
      "            return {}     \n",
      "    # Refactored   \n",
      "    def swift_remove_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> bool: \n",
      "        try:\n",
      "            swift_client.delete_object(\n",
      "                container = bucket_name, \n",
      "                obj = object_path\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_update_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> bool:  \n",
      "        remove = swift_remove_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path\n",
      "        )\n",
      "        if not remove:\n",
      "            return False\n",
      "        create = swift_create_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path, \n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "        return create\n",
      "    # Works\n",
      "    def swift_create_or_update_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        bucket_info = swift_check_bucket(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "        \n",
      "        if len(bucket_info) == 0:\n",
      "            creation_status = swift_create_bucket(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name\n",
      "            )\n",
      "            if not creation_status:\n",
      "                return False\n",
      "        \n",
      "        object_info = swift_check_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path\n",
      "        )\n",
      "        \n",
      "        if len(object_info) == 0:\n",
      "            return swift_create_object(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name, \n",
      "                object_path = object_path, \n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "        else:\n",
      "            return swift_update_object(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name, \n",
      "                object_path = object_path, \n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "\n",
      "        # Refactored and Works\n",
      "    def set_encoded_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        encoded_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                encoded_key = key_initial + '-' + key\n",
      "                if isinstance(value, list):\n",
      "                    encoded_metadata[encoded_key] = 'list=' + ','.join(map(str, value))\n",
      "                    continue\n",
      "                encoded_metadata[encoded_key] = str(value)\n",
      "        return encoded_metadata\n",
      "    # Refactored and works\n",
      "    def get_general_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        general_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                if not key_initial == key[:len(key_initial)]:\n",
      "                    general_metadata[key] = value\n",
      "        return general_metadata\n",
      "    # Refactored and works\n",
      "    def get_decoded_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any: \n",
      "        decoded_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                if key_initial == key[:len(key_initial)]:\n",
      "                    decoded_key = key[len(key_initial) + 1:]\n",
      "                    if 'list=' in value:\n",
      "                        string_integers = value.split('=')[1]\n",
      "                        values = string_integers.split(',')\n",
      "                        if len(values) == 1 and values[0] == '':\n",
      "                            decoded_metadata[decoded_key] = []\n",
      "                        else:\n",
      "                            try:\n",
      "                                decoded_metadata[decoded_key] = list(map(int, values))\n",
      "                            except:\n",
      "                                decoded_metadata[decoded_key] = list(map(str, values))\n",
      "                        continue\n",
      "                    if value.isnumeric():\n",
      "                        decoded_metadata[decoded_key] = int(value)\n",
      "                        continue\n",
      "                    decoded_metadata[decoded_key] = value\n",
      "        return decoded_metadata\n",
      "    # Refactored and works\n",
      "    def set_bucket_names(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_names = []\n",
      "        bucket_prefix = storage_parameters['bucket-prefix']\n",
      "        ice_id = storage_parameters['ice-id']\n",
      "        user = storage_parameters['user']\n",
      "        storage_names.append(bucket_prefix + '-forwarder-' + ice_id)\n",
      "        storage_names.append(bucket_prefix + '-submitter-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        storage_names.append(bucket_prefix + '-pipeline-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        storage_names.append(bucket_prefix + '-experiment-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        return storage_names\n",
      "    # created and works\n",
      "    def setup_storage(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = setup_storage_client(\n",
      "            storage_parameters = storage_parameters\n",
      "        ) \n",
      "        \n",
      "        storage_name = set_bucket_names(\n",
      "        storage_parameters = storage_parameters\n",
      "        )\n",
      "        \n",
      "        return storage_client, storage_name\n",
      "    # Refactored and works\n",
      "    def setup_storage_client(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = None\n",
      "        if storage_parameters['used-client'] == 'swift':\n",
      "            storage_client = swift_setup_client(\n",
      "                pre_auth_url = storage_parameters['pre-auth-url'],\n",
      "                pre_auth_token = storage_parameters['pre-auth-token'],\n",
      "                user_domain_name = storage_parameters['user-domain-name'],\n",
      "                project_domain_name = storage_parameters['project-domain-name'],\n",
      "                project_name = storage_parameters['project-name'],\n",
      "                auth_version = storage_parameters['auth-version']\n",
      "            )\n",
      "        return storage_client\n",
      "    # Refactored and works\n",
      "    def check_object_metadata(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> any: \n",
      "        object_metadata = {\n",
      "            'general-meta': {},\n",
      "            'custom-meta': {}\n",
      "        }\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            all_metadata = swift_check_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "            ) \n",
      "\n",
      "            general_metadata = {}\n",
      "            custom_metadata = {}\n",
      "            if not len(all_metadata) == 0:\n",
      "                general_metadata = get_general_metadata(\n",
      "                    used_client = 'swift',\n",
      "                    object_metadata = all_metadata\n",
      "                )\n",
      "                custom_metadata = get_decoded_metadata(\n",
      "                    used_client = 'swift',\n",
      "                    object_metadata = all_metadata\n",
      "                )\n",
      "\n",
      "            object_metadata['general-meta'] = general_metadata\n",
      "            object_metadata['custom-meta'] = custom_metadata\n",
      "\n",
      "        return object_metadata\n",
      "    # Refactored and works\n",
      "    def get_object_content(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_path: str\n",
      "    ) -> any:\n",
      "        object_content = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            fetched_object = swift_get_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path\n",
      "            )\n",
      "            object_content['data'] = pickle.loads(fetched_object['data'])\n",
      "            object_content['general-meta'] = get_general_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = fetched_object['info']\n",
      "            )\n",
      "            object_content['custom-meta'] = get_decoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = fetched_object['info']\n",
      "            )\n",
      "        return object_content\n",
      "    # Refactored    \n",
      "    def remove_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> bool: \n",
      "        removed = False\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            removed = swift_remove_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path\n",
      "            )\n",
      "        return removed\n",
      "    # Refactored and works\n",
      "    def create_or_update_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        success = False\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            formatted_data = pickle.dumps(object_data)\n",
      "            formatted_metadata = set_encoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "\n",
      "            success = swift_create_or_update_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path,\n",
      "                object_data = formatted_data,\n",
      "                object_metadata = formatted_metadata\n",
      "            )\n",
      "        return success\n",
      "    # Created and works\n",
      "    def format_bucket_metadata(\n",
      "        used_client: str,\n",
      "        bucket_metadata: any\n",
      "    ) -> any:\n",
      "        formatted_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            relevant_values = {\n",
      "                'x-container-object-count': 'object-count',\n",
      "                'x-container-bytes-used-actual': 'used-bytes',\n",
      "                'last-modified': 'date',\n",
      "                'content-type': 'type'\n",
      "            }\n",
      "            formatted_metadata = {}\n",
      "            for key,value in bucket_metadata.items():\n",
      "                if key in relevant_values:\n",
      "                    formatted_key = relevant_values[key]\n",
      "                    formatted_metadata[formatted_key] = value\n",
      "        return formatted_metadata\n",
      "    # Created and works\n",
      "    def format_bucket_objects(\n",
      "        used_client: str,\n",
      "        bucket_objects: any\n",
      "    ) -> any:\n",
      "        formatted_objects = {}\n",
      "        if used_client == 'swift':\n",
      "            for bucket_object in bucket_objects:\n",
      "                formatted_object_metadata = {\n",
      "                    'hash': 'id',\n",
      "                    'bytes': 'used-bytes',\n",
      "                    'last_modified': 'date'\n",
      "                }\n",
      "                object_key = None\n",
      "                object_metadata = {}\n",
      "                for key, value in bucket_object.items():\n",
      "                    if key == 'name':\n",
      "                        object_key = value\n",
      "                    if key in formatted_object_metadata:\n",
      "                        formatted_key = formatted_object_metadata[key]\n",
      "                        object_metadata[formatted_key] = value\n",
      "                formatted_objects[object_key] = object_metadata\n",
      "        return formatted_objects\n",
      "    # Created and works\n",
      "    def format_bucket_info(\n",
      "        used_client: str,\n",
      "        bucket_info: any\n",
      "    ) -> any:\n",
      "        bucket_metadata = {}\n",
      "        bucket_objects = {}\n",
      "        if used_client == 'swift':\n",
      "            bucket_metadata = format_bucket_metadata(\n",
      "                used_client = used_client,\n",
      "                bucket_metadata = bucket_info['metadata']\n",
      "            )\n",
      "            bucket_objects = format_bucket_objects(\n",
      "                used_client = used_client,\n",
      "                bucket_objects = bucket_info['objects']\n",
      "            )\n",
      "        return {'metadata': bucket_metadata, 'objects': bucket_objects} \n",
      "    # Created and works\n",
      "    def get_bucket_info(\n",
      "        storage_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> any:\n",
      "        bucket_info = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            unformatted_bucket_info = swift_check_bucket(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name\n",
      "            )\n",
      "            bucket_info = format_bucket_info(\n",
      "                used_client = 'swift',\n",
      "                bucket_info = unformatted_bucket_info\n",
      "            )\n",
      "        return bucket_info\n",
      "    # Created and works\n",
      "    def format_container_info(\n",
      "        used_client: str,\n",
      "        container_info: any\n",
      "    ) -> any:\n",
      "        formatted_container_info = {}\n",
      "        if used_client == 'swift':\n",
      "            for bucket in container_info:\n",
      "                bucket_name = bucket['name']\n",
      "                bucket_count = bucket['count']\n",
      "                bucket_size = bucket['bytes']\n",
      "                formatted_container_info[bucket_name] = {\n",
      "                    'amount': bucket_count,\n",
      "                    'size': bucket_size\n",
      "                }\n",
      "        return formatted_container_info\n",
      "    # Created and works\n",
      "    def get_container_info( \n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        container_info = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            unformatted_container_info = swift_list_buckets(\n",
      "                swift_client = storage_client \n",
      "            )\n",
      "            container_info = format_container_info(\n",
      "                used_client = 'swift',\n",
      "                container_info = unformatted_container_info\n",
      "            )\n",
      "        return container_info\n",
      "    \n",
      "        # Created and works\n",
      "    def set_object_path(\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ):\n",
      "        object_paths = {\n",
      "            'root': 'name',\n",
      "            'code': 'CODE/name',\n",
      "            'slurm': 'CODE/SLURM/name',\n",
      "            'ray': 'CODE/RAY/name',\n",
      "            'data': 'DATA/name',\n",
      "            'artifacts': 'ARTIFACTS/name',\n",
      "            'time': 'TIMES/name'\n",
      "        }\n",
      "\n",
      "        i = 0\n",
      "        path_split = object_paths[object_name].split('/')\n",
      "        for name in path_split:\n",
      "            if name in path_replacers:\n",
      "                replacer = path_replacers[name]\n",
      "                if 0 < len(replacer):\n",
      "                    path_split[i] = replacer\n",
      "            i = i + 1\n",
      "        \n",
      "        if not len(path_names) == 0:\n",
      "            path_split.extend(path_names)\n",
      "\n",
      "        object_path = '/'.join(path_split)\n",
      "        #print('Used object path:' + str(object_path))\n",
      "        return object_path\n",
      "    # created and works\n",
      "    def setup_storage(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = setup_storage_client(\n",
      "            storage_parameters = storage_parameters\n",
      "        ) \n",
      "        \n",
      "        storage_name = set_bucket_names(\n",
      "        storage_parameters = storage_parameters\n",
      "        )\n",
      "        \n",
      "        return storage_client, storage_name\n",
      "    # Created and works\n",
      "    def check_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ) -> bool:\n",
      "        object_path = set_object_path(\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "        # Consider making these functions object storage agnostic\n",
      "        object_metadata = check_object_metadata(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "        )\n",
      "        object_metadata['path'] = object_path\n",
      "        return object_metadata\n",
      "    # Created and works\n",
      "    def get_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ) -> any:\n",
      "        checked_object = check_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "\n",
      "        object_data = None\n",
      "        if not len(checked_object['general-meta']) == 0:\n",
      "            # Consider making these functions object storage agnostic\n",
      "            object_data = get_object_content(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = checked_object['path']\n",
      "            )\n",
      "\n",
      "        return object_data\n",
      "    # Created and Works\n",
      "    def set_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any,\n",
      "        overwrite: bool,\n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ):\n",
      "        checked_object = check_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "        \n",
      "        perform = True\n",
      "        if not len(checked_object['general-meta']) == 0 and not overwrite:\n",
      "            perform = False\n",
      "        \n",
      "        if perform:\n",
      "            # Consider making these functions object storage agnostic\n",
      "            create_or_update_object(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = checked_object['path'],\n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "    # Created and works\n",
      "    def check_bucket(\n",
      "        storage_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> any:\n",
      "        return get_bucket_info(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "    # Created and works\n",
      "    def check_buckets(\n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        return get_container_info( \n",
      "            storage_client = storage_client\n",
      "        )\n",
      "\n",
      "    def gather_time(\n",
      "        storage_client: any,\n",
      "        storage_name: any,\n",
      "        time_group: any,\n",
      "        time_name: any,\n",
      "        start_time: int,\n",
      "        end_time: int\n",
      "    ):\n",
      "        time_object = get_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'time',\n",
      "            path_replacers = {\n",
      "                'name': time_group\n",
      "            },\n",
      "            path_names = []\n",
      "        )\n",
      "\n",
      "        time_data = {}\n",
      "        time_metadata = {} \n",
      "        if time_object is None:\n",
      "            time_data = {}\n",
      "            time_metadata = general_object_metadata()\n",
      "        else:\n",
      "            time_data = time_object['data']\n",
      "            time_metadata = time_object['custom-meta']\n",
      "        \n",
      "        current_key_amount = len(time_data)\n",
      "        current_key_full = False\n",
      "        current_key = str(current_key_amount)\n",
      "        if 0 < current_key_amount:\n",
      "            time_object = time_data[current_key]\n",
      "            if 0 < time_object['total-seconds']:\n",
      "                current_key_full = True\n",
      "        \n",
      "        changed = False\n",
      "        if 0 < end_time and 0 < current_key_amount and not current_key_full:\n",
      "            stored_start_time = time_data[current_key]['start-time']\n",
      "            time_diff = (end_time-stored_start_time)\n",
      "            time_data[current_key]['end-time'] = end_time\n",
      "            time_data[current_key]['total-seconds'] = round(time_diff,5)\n",
      "            changed = True\n",
      "        else:\n",
      "            next_key_amount = len(time_data) + 1\n",
      "            new_key = str(next_key_amount)\n",
      "        \n",
      "            if 0 < start_time and 0 == end_time:\n",
      "                time_data[new_key] = {\n",
      "                    'name': time_name,\n",
      "                    'start-time': start_time,\n",
      "                    'end-time': 0,\n",
      "                    'total-seconds': 0\n",
      "                }\n",
      "                changed = True\n",
      "\n",
      "            if 0 < start_time and 0 < end_time:\n",
      "                time_diff = (end_time-start_time)\n",
      "                time_data[new_key] = {\n",
      "                    'name': time_name,\n",
      "                    'start-time': start_time,\n",
      "                    'end-time': end_time,\n",
      "                    'total-seconds': round(time_diff,5)\n",
      "                }\n",
      "                changed = True\n",
      "\n",
      "        if changed:\n",
      "            time_metadata['version'] = time_metadata['version'] + 1\n",
      "            set_object(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = storage_name,\n",
      "                object_name = 'time',\n",
      "                path_replacers = {\n",
      "                    'name': time_group\n",
      "                },\n",
      "                path_names = [],\n",
      "                overwrite = True,\n",
      "                object_data = time_data,\n",
      "                object_metadata = time_metadata \n",
      "            )\n",
      "\n",
      "    class CNNClassifier(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 6, 5)\n",
      "            self.pool = nn.MaxPool2d(2, 2)\n",
      "            self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "            self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
      "            self.fc2 = nn.Linear(120, 84)\n",
      "            self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = self.pool(F.relu(self.conv1(x)))\n",
      "            x = self.pool(F.relu(self.conv2(x)))\n",
      "            x = x.view(-1, 16 * 4 * 4)\n",
      "            x = F.relu(self.fc1(x))\n",
      "            x = F.relu(self.fc2(x))\n",
      "            x = self.fc3(x)\n",
      "            return x\n",
      "\n",
      "    def parse_torchmetrics(\n",
      "        metrics: any,\n",
      "        labels: any\n",
      "    ):\n",
      "        collected_metrics = {}\n",
      "        for key,value in metrics.items():\n",
      "            if key == 'name':\n",
      "                continue\n",
      "            if 'class-' in key:\n",
      "                image_labels = {\n",
      "                    0: 'top',\n",
      "                    1: 'trouser',\n",
      "                    2: 'pullover',\n",
      "                    3: 'dress',\n",
      "                    4: 'coat',\n",
      "                    5: 'sandal',\n",
      "                    6: 'shirt',\n",
      "                    7: 'sneaker',\n",
      "                    8: 'bag',\n",
      "                    9: 'ankle-boot',\n",
      "                }\n",
      "                #logger.info('')\n",
      "                i = 0\n",
      "                for class_value in value:\n",
      "                    formatted_key = key.replace('class', labels[i])\n",
      "                    rounded_value = round(class_value,5)\n",
      "                    #logger.info(str(formatted_key) + '=' + str(rounded_value))\n",
      "                    collected_metrics[formatted_key] = rounded_value\n",
      "                    i += 1\n",
      "                continue\n",
      "            rounded_value = round(value,5)\n",
      "            #logger.info(str(key) + '=' + str(rounded_value))\n",
      "            collected_metrics[key] = rounded_value\n",
      "        return collected_metrics\n",
      "    \n",
      "    def setup_mlflow(\n",
      "        logger: any,\n",
      "        mlflow_parameters: any\n",
      "    ):\n",
      "        mlflow_s3_endpoint_url = mlflow_parameters['s3-endpoint-url']\n",
      "        mlflow_tracking_uri = mlflow_parameters['tracking-uri']\n",
      "        mlflow_experiment_name = mlflow_parameters['experiment-name']\n",
      "        \n",
      "        os.environ['MLFLOW_S3_ENDPOINT_URL'] = mlflow_s3_endpoint_url\n",
      "\n",
      "        logger.info(f\"Using MLflow tracking URI: {mlflow_tracking_uri}\")\n",
      "        mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
      "        logger.info(f\"Using MLflow experiment: {mlflow_experiment_name}\")\n",
      "        mlflow.set_experiment(mlflow_experiment_name)\n",
      "\n",
      "    def set_route(\n",
      "        route_type: str,\n",
      "        route_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ): \n",
      "        # Check job-run and job-cancel\n",
      "        routes = {\n",
      "            'root': 'TYPE:/name',\n",
      "            'logs': 'GET:/general/logs/name',\n",
      "            'structure': 'GET:/general/structure',\n",
      "            'setup': 'POST:/setup/config',\n",
      "            'start': 'POST:/setup/start',\n",
      "            'stop': 'POST:/setup/stop',\n",
      "            'job-submit': 'POST:/requests/submit/job',\n",
      "            'job-run': 'PUT:/requests/run/job/name',\n",
      "            'job-cancel': 'PUT:/requests/cancel/job/name',\n",
      "            'forwarding-submit': 'POST:/requests/submit/forwarding',\n",
      "            'forwarding-cancel': 'PUT:/requests/cancel/type/user/key',\n",
      "            'task-request': 'PUT:/tasks/request/signature',\n",
      "            'task-result': 'GET:/tasks/result/id',\n",
      "            'job-artifact': 'GET:/artifacts/job/type/name',\n",
      "            'forwarding-artifact': 'GET:/artifacts/forwarding/type/user/key'\n",
      "        }\n",
      "\n",
      "        route = None\n",
      "        if route_name in routes:\n",
      "            i = 0\n",
      "            route = routes[route_name].split('/')\n",
      "            for name in route:\n",
      "                if name in path_replacers:\n",
      "                    replacer = path_replacers[name]\n",
      "                    if 0 < len(replacer):\n",
      "                        route[i] = replacer\n",
      "                i = i + 1\n",
      "\n",
      "            if not len(path_names) == 0:\n",
      "                route.extend(path_names)\n",
      "\n",
      "            if not len(route_type) == 0:\n",
      "                route[0] = route_type + ':'\n",
      "            \n",
      "            route = '/'.join(route)\n",
      "        #print('Used route: ' + str(route))\n",
      "        return route\n",
      "    # Created and works\n",
      "    def get_response(\n",
      "        route_type: str,\n",
      "        route_url: str,\n",
      "        route_input: any\n",
      "    ) -> any:\n",
      "        route_response = None\n",
      "        if route_type == 'POST':\n",
      "            route_response = requests.post(\n",
      "                url = route_url,\n",
      "                json = route_input\n",
      "            )\n",
      "        if route_type == 'PUT':\n",
      "            route_response = requests.put(\n",
      "                url = route_url\n",
      "            )\n",
      "        if route_type == 'GET':\n",
      "            route_response = requests.get(\n",
      "                url = route_url\n",
      "            )\n",
      "        return route_response\n",
      "    # Created and works\n",
      "    def set_full_url(\n",
      "        address: str,\n",
      "        port: str,\n",
      "        used_route: str\n",
      "    ) -> str:\n",
      "        url_prefix = 'http://' + address + ':' + port\n",
      "        route_split = used_route.split(':')\n",
      "        url_type = route_split[0]\n",
      "        used_path = route_split[1]\n",
      "        full_url = url_prefix + used_path\n",
      "        return url_type, full_url\n",
      "    # Created and works\n",
      "    def request_route(\n",
      "        address: str,\n",
      "        port: str,\n",
      "        route_type: str,\n",
      "        route_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any,\n",
      "        route_input: any,\n",
      "        timeout: any\n",
      "    ) -> any:\n",
      "        used_route = set_route(\n",
      "            route_type = route_type,\n",
      "            route_name = route_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "\n",
      "        url_type, full_url = set_full_url(\n",
      "            address = address,\n",
      "            port = port,\n",
      "            used_route = used_route\n",
      "        )\n",
      "        \n",
      "        route_response = get_response(\n",
      "            route_type = url_type,\n",
      "            route_url = full_url,\n",
      "            route_input = route_input\n",
      "        )\n",
      "\n",
      "        route_status_code = None\n",
      "        route_returned_text = {}\n",
      "        if not route_response is None:\n",
      "            route_status_code = route_response.status_code\n",
      "            if route_status_code == 200:\n",
      "                route_text = json.loads(route_response.text)\n",
      "\n",
      "                if 'id' in route_text: \n",
      "                    task_result_route = set_route(\n",
      "                        route_type = '',\n",
      "                        route_name = 'task-result',\n",
      "                        path_replacers = {\n",
      "                            'id': route_text['id']\n",
      "                        },\n",
      "                        path_names = []\n",
      "                    )\n",
      "\n",
      "                    task_url_type, task_full_url = set_full_url(\n",
      "                        address = address,\n",
      "                        port = port,\n",
      "                        used_route = task_result_route\n",
      "                    )\n",
      "\n",
      "                    start = t.time()\n",
      "                    while t.time() - start <= timeout:\n",
      "                        task_route_response = get_response(\n",
      "                            route_type = task_url_type,\n",
      "                            route_url = task_full_url,\n",
      "                            route_input = {}\n",
      "                        )\n",
      "                        \n",
      "                        task_status_code = route_response.status_code\n",
      "                            \n",
      "                        if task_status_code == 200:\n",
      "                            task_text = json.loads(task_route_response.text)\n",
      "                            if task_text['status'] == 'FAILED':\n",
      "                                break\n",
      "                            \n",
      "                            if task_text['status'] == 'SUCCESS':\n",
      "                                route_returned_text = task_text['result']\n",
      "                                break\n",
      "                        else:\n",
      "                            break\n",
      "                else:\n",
      "                    route_returned_text = route_text\n",
      "        return route_status_code, route_returned_text\n",
      "    \n",
      "    def start_forwarder_scheduler(\n",
      "        address: str,\n",
      "        port: str,\n",
      "        scheduler_request: any\n",
      "    ) -> bool:\n",
      "        scheduler_route_code, scheduler_route_text = request_route(\n",
      "            address = address,\n",
      "            port = port,\n",
      "            route_type = '',\n",
      "            route_name = 'start',\n",
      "            path_replacers = {},\n",
      "            path_names = [],\n",
      "            route_input = scheduler_request,\n",
      "            timeout = 120\n",
      "        )\n",
      "        configured = False\n",
      "        if scheduler_route_code == 200 and scheduler_route_text:\n",
      "            configured = True\n",
      "        return configured\n",
      "    # Created\n",
      "    def start_forwarder(\n",
      "        address: str,\n",
      "        port: str,\n",
      "        configuration: any,\n",
      "        scheduler_request: any\n",
      "    ) -> bool:\n",
      "        forwarder_route_code, forwarder_route_text  = request_route(\n",
      "            address = address,\n",
      "            port = port,\n",
      "            route_type = '',\n",
      "            route_name = 'setup',\n",
      "            path_replacers = {},\n",
      "            path_names = [],\n",
      "            route_input = configuration,\n",
      "            timeout = 120\n",
      "        )\n",
      "        configured = start_forwarder_scheduler(\n",
      "            address = address,\n",
      "            port = port,\n",
      "            scheduler_request = scheduler_request\n",
      "        )\n",
      "        return configured\n",
      "    \n",
      "    def test_url(\n",
      "        target_url: str,\n",
      "        timeout: int\n",
      "    ) -> bool:\n",
      "        try:\n",
      "            response = requests.head(\n",
      "                url = target_url, \n",
      "                timeout = timeout\n",
      "            )\n",
      "            if response.status_code == 200:\n",
      "                return True\n",
      "            return False\n",
      "        except requests.ConnectionError:\n",
      "            return False\n",
      "        \n",
      "    def setup_ray(\n",
      "        logger: any,\n",
      "        services: any,\n",
      "        timeout: int\n",
      "    ):\n",
      "        logger.info('Setting up ray client')\n",
      "        start = t.time()\n",
      "        ray_client = None\n",
      "        if 0 < len(services):\n",
      "            ray_dashboard_url = 'http://' + services['ray-dashboard']\n",
      "            ray_exists = None\n",
      "            while t.time() - start <= timeout:\n",
      "                logger.info('Testing ray client url: ' + str(ray_dashboard_url))\n",
      "                ray_exists = test_url(\n",
      "                    target_url = ray_dashboard_url,\n",
      "                    timeout = 5\n",
      "                )\n",
      "                logger.info('Ray client exists: ' + str(ray_exists))\n",
      "                if ray_exists:\n",
      "                    break\n",
      "                t.sleep(5)\n",
      "            if ray_exists:\n",
      "                ray_client = JobSubmissionClient(\n",
      "                    address = ray_dashboard_url\n",
      "                )\n",
      "                logger.info(\"Ray setup\")\n",
      "        return ray_client\n",
      "\n",
      "    def get_ray_job(\n",
      "        logger: any,\n",
      "        storage_client: any,\n",
      "        storage_name: str,\n",
      "        ray_job_file: str\n",
      "    ) -> any:\n",
      "        #logger.info('Ray job path:' + str(ray_job_path))\n",
      "        ray_job_object = get_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'ray',\n",
      "            path_replacers = {\n",
      "                'name': ray_job_file\n",
      "            },\n",
      "            path_names = []\n",
      "        )\n",
      "\n",
      "        ray_job_object_data = ray_job_object['data']\n",
      "\n",
      "        current_directory = os.getcwd()\n",
      "        ray_job_directory = os.path.join(current_directory, 'jobs')\n",
      "\n",
      "        if not os.path.exists(ray_job_directory):\n",
      "            logger.info('Make directory')\n",
      "            os.makedirs(ray_job_directory)\n",
      "        \n",
      "        used_ray_job_path = os.path.join(ray_job_directory, ray_job_file)\n",
      "        \n",
      "        logger.info('Job writing path:' + str(used_ray_job_path))\n",
      "        with open(used_ray_job_path, 'w') as file:\n",
      "            file.write(ray_job_object_data)\n",
      "\n",
      "        return ray_job_directory\n",
      "\n",
      "    def submit_ray_job(\n",
      "        logger: any,\n",
      "        ray_client: any,\n",
      "        ray_parameters: any,\n",
      "        ray_job_file: any,\n",
      "        working_directory: str,\n",
      "        ray_job_envs: any\n",
      "    ) -> any:\n",
      "        logger.info('Submitting ray job ' + str(ray_job_file) + ' using directory ' + str(working_directory))\n",
      "        command = \"python \" + str(ray_job_file)\n",
      "        if 0 < len(ray_parameters):\n",
      "            command = command + \" '\" + json.dumps(ray_parameters) + \"'\"\n",
      "        job_id = ray_client.submit_job(\n",
      "            entrypoint = command,\n",
      "            runtime_env = {\n",
      "                'working_dir': str(working_directory),\n",
      "                'env_vars': ray_job_envs\n",
      "            }\n",
      "        )\n",
      "        return job_id\n",
      "\n",
      "    def wait_ray_job(\n",
      "        logger: any,\n",
      "        ray_client: any,\n",
      "        ray_job_id: int, \n",
      "        waited_status: any,\n",
      "        timeout: int\n",
      "    ) -> any:\n",
      "        logger.info('Waiting ray job ' + str(ray_job_id))\n",
      "        start = t.time()\n",
      "        job_status = None\n",
      "        while t.time() - start <= timeout:\n",
      "            status = ray_client.get_job_status(ray_job_id)\n",
      "            logger.info(f\"status: {status}\")\n",
      "            if status in waited_status:\n",
      "                job_status = status\n",
      "                break\n",
      "            t.sleep(5)\n",
      "        job_logs = ray_client.get_job_logs(ray_job_id)\n",
      "        return job_status, job_logs\n",
      "\n",
      "    def ray_job_handler(\n",
      "        logger: any,\n",
      "        storage_client: any,\n",
      "        storage_name: str,\n",
      "        ray_client: any,\n",
      "        ray_parameters: any,\n",
      "        ray_job_file: str,\n",
      "        ray_job_envs: any,\n",
      "        timeout: int\n",
      "    ) -> bool:\n",
      "        logger.info('Setting up ray job')\n",
      "\n",
      "        ray_job_directory = get_ray_job(\n",
      "            logger = logger,\n",
      "            storage_client = storage_client,\n",
      "            storage_name = storage_name,\n",
      "            ray_job_file = ray_job_file\n",
      "        )\n",
      "\n",
      "        logger.info('Submitting a ray job')\n",
      "        ray_job_id = submit_ray_job(\n",
      "            logger = logger,\n",
      "            ray_client = ray_client,\n",
      "            ray_parameters = ray_parameters,\n",
      "            ray_job_file = ray_job_file,\n",
      "            working_directory = ray_job_directory,\n",
      "            ray_job_envs = ray_job_envs\n",
      "        )\n",
      "\n",
      "        logger.info('Ray batch job id: ' + str(ray_job_id))\n",
      "        \n",
      "        ray_job_status, ray_job_logs = wait_ray_job(\n",
      "            logger = logger,\n",
      "            ray_client = ray_client,\n",
      "            ray_job_id = ray_job_id,\n",
      "            waited_status = {\n",
      "                JobStatus.SUCCEEDED, \n",
      "                JobStatus.STOPPED, \n",
      "                JobStatus.FAILED\n",
      "            }, \n",
      "            timeout = timeout\n",
      "        )\n",
      "        logger.info('Ray batch job ended:')\n",
      "        success = True\n",
      "        if not ray_job_status == JobStatus.SUCCEEDED:\n",
      "            logger.info('RAY batch job failed')\n",
      "            success = False\n",
      "        else:\n",
      "            logger.info('RAY batch job succeeded')\n",
      "        logger.info(ray_job_logs)\n",
      "        return success\n",
      "\n",
      "    def parse_job_sacct(\n",
      "        logger: any,\n",
      "        sacct: any\n",
      "    ) -> any:\n",
      "        collected_parameters = {}\n",
      "        collected_metrics = {}\n",
      "        logger.info('')\n",
      "        logger.info('Sacct:')\n",
      "        for row in sacct.keys():\n",
      "            logger.info('Row ' + str(row))\n",
      "            row_metadata = sacct[row]['metadata']\n",
      "            row_metrics = sacct[row]['metrics']\n",
      "        \n",
      "            relevant_metadata = [\n",
      "                'job-name',\n",
      "                'state'\n",
      "            ]\n",
      "            \n",
      "            for key, value in row_metadata.items():\n",
      "                if key in relevant_metadata:\n",
      "                    logger.info(str(key) + '=' + str(value))\n",
      "                    collected_parameters[str(row) + '-' + key] = value\n",
      "        \n",
      "            relevant_metrics = [\n",
      "                'ave-cpu-seconds',\n",
      "                'cpu-time-seconds',\n",
      "                'elapsed-seconds',\n",
      "                'total-cpu-seconds',\n",
      "            ]\n",
      "            \n",
      "            for key, value in row_metrics.items():\n",
      "                if key in relevant_metrics:\n",
      "                    logger.info(str(key) + '=' + str(value))\n",
      "                    collected_metrics[str(row) + '-' + key] = value\n",
      "        \n",
      "            start_time = row_metrics['start-time']\n",
      "            submit_time = row_metrics['submit-time']\n",
      "            end_time = row_metrics['end-time']\n",
      "        \n",
      "            submit_date = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d-%H:%M:%S')\n",
      "            total_start_seconds = (submit_time-start_time)\n",
      "            total_end_seconds = (end_time-submit_time)\n",
      "        \n",
      "            logger.info('submit-date=' + str(submit_date))\n",
      "            collected_parameters[str(row) + '-submit-date'] = submit_date\n",
      "            \n",
      "            logger.info('total-submit-start-seconds=' + str(total_start_seconds))\n",
      "            collected_metrics[str(row) + '-total-submit-start-seconds'] = total_start_seconds\n",
      "            \n",
      "            logger.info('total-start-end-seconds=' + str(total_end_seconds))\n",
      "            collected_metrics[str(row) + '-total-start-end-seconds'] = total_end_seconds\n",
      "            \n",
      "            logger.info('')\n",
      "        return collected_parameters, collected_metrics\n",
      "\n",
      "    def parse_job_seff(\n",
      "        logger: any,\n",
      "        seff: any\n",
      "    ) -> any:\n",
      "        collected_parameters = {}\n",
      "        collected_metrics = {}\n",
      "        relevant_metadata = [\n",
      "            'billed-project',\n",
      "            'cluster',\n",
      "            'status'\n",
      "        ]\n",
      "\n",
      "        logger.info('')\n",
      "        logger.info('Seff:')\n",
      "        seff_metadata = seff['metadata']\n",
      "        for key,value in seff_metadata.items():\n",
      "            if key in relevant_metadata:\n",
      "                logger.info(str(key) + '=' + str(value))\n",
      "                collected_parameters[key] = value\n",
      "        \n",
      "        relevant_metrics = [\n",
      "            'billing-units',\n",
      "            'cpu-efficiency-percentage',\n",
      "            'cpu-efficiency-seconds',\n",
      "            'cpu-utilized-seconds',\n",
      "            'job-wall-clock-time-seconds',\n",
      "            'memory-efficiency-percentage'\n",
      "        ]\n",
      "\n",
      "\n",
      "        seff_metrics = seff['metrics']\n",
      "        for key,value in seff_metrics.items():\n",
      "            if key in relevant_metrics:\n",
      "                logger.info(str(key) + '=' + str(value))\n",
      "                collected_metrics[key] = value\n",
      "\n",
      "        return collected_parameters, collected_metrics\n",
      "    \n",
      "    # BOILERPLATE END\n",
      "    \n",
      "    logging.basicConfig(level = logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "\n",
      "    component_time_start = t.time()\n",
      "\n",
      "    storage_client, storage_names = setup_storage(\n",
      "        storage_parameters = storage_parameters\n",
      "    )\n",
      "\n",
      "    logger.info('Storage setup')\n",
      "\n",
      "    output = namedtuple('Output', ['storage_uri', 'run_id'])\n",
      "\n",
      "    pipeline_bucket = storage_names[-2]\n",
      "\n",
      "    logger.info('Used bucket:' + str(pipeline_bucket))\n",
      "    \n",
      "    logger.info(\"Variable setup\")\n",
      "\n",
      "    configuration = integration_parameters['configuration']\n",
      "    scheduler_request = integration_parameters['scheduler-request']\n",
      "    forwarder_address = integration_parameters['forwarder-address']\n",
      "    forwarder_port = integration_parameters['forwarder-port']\n",
      "    forwarding_request = integration_parameters['forwarding-request']\n",
      "    user = integration_parameters['user']\n",
      "    job_request = integration_parameters['job-request']\n",
      "    ray_parameters = integration_parameters['ray-parameters']\n",
      "    ray_job_file = integration_parameters['ray-job-file']\n",
      "    ray_job_envs = integration_parameters['ray-job-envs']\n",
      "    ray_job_timeout = integration_parameters['ray-job-timeout']\n",
      "    folder_name = integration_parameters['folder-name']\n",
      "    \n",
      "    logger.info(forwarder_address)\n",
      "\n",
      "    logger.info(\"Starting forwarder\")\n",
      "\n",
      "    forwarder_started = start_forwarder(\n",
      "        address = forwarder_address, \n",
      "        port = forwarder_port,\n",
      "        configuration = configuration,\n",
      "        scheduler_request = scheduler_request\n",
      "    )\n",
      "\n",
      "    if not forwarder_started:\n",
      "        return output('none', 'none')\n",
      "\n",
      "    logger.info(\"Forwarder started\")\n",
      "\n",
      "    logger.info(\"Submitting forwarding request\")\n",
      "    logger.info(forwarder_address)\n",
      "\n",
      "    status_code, forwarding = request_route(\n",
      "        address = forwarder_address,\n",
      "        port = forwarder_port,\n",
      "        route_type = '',\n",
      "        route_name = 'forwarding-submit',\n",
      "        path_replacers = {},\n",
      "        path_names = [],\n",
      "        route_input = forwarding_request,\n",
      "        timeout = 300\n",
      "    )\n",
      "    \n",
      "    if not status_code == 200:\n",
      "        return output('none', 'none')\n",
      "\n",
      "    logger.info(\"Request success\")\n",
      "\n",
      "    # failed here\n",
      "\n",
      "    forwarder_keys = forwarding['keys']\n",
      "    forwarding_split = forwarder_keys.split(',')\n",
      "    import_key = forwarding_split[0]\n",
      "\n",
      "    logger.info(\"Import key: \" + str(import_key))\n",
      "\n",
      "    if import_key == '0':\n",
      "        return output('none', 'none')\n",
      "    \n",
      "    logger.info(\"Waiting forwarding services\")\n",
      "\n",
      "    forwarding_timeout = 5000\n",
      "    start = t.time()\n",
      "    current_services = None\n",
      "    while t.time() - start <= forwarding_timeout:\n",
      "        status_code, forwarding_data = request_route(\n",
      "            address = forwarder_address,\n",
      "            port = forwarder_port,\n",
      "            route_type = '',\n",
      "            route_name = 'forwarding-artifact',\n",
      "            path_replacers = {\n",
      "                'type': 'status-imports',\n",
      "                'user': user,\n",
      "                'key': import_key\n",
      "            },\n",
      "            path_names = [],\n",
      "            route_input = {},\n",
      "            timeout = 500\n",
      "        )\n",
      "\n",
      "        if not status_code == 200:\n",
      "            return output('none', 'none')\n",
      "\n",
      "        if not 'forwarding-status' in forwarding_data:\n",
      "            return output('none', 'none')\n",
      "        # Fails for some reason\n",
      "        forwarding_status = forwarding_data['forwarding-status']    \n",
      "\n",
      "        if 'cancel' in forwarding_status or 'deleted' in forwarding_status:\n",
      "            if forwarding_status['cancel'] or forwarding_status['deleted']:\n",
      "                break\n",
      "\n",
      "        if 'created' in forwarding_status:\n",
      "            if forwarding_status['created']:\n",
      "                current_services = forwarding_status['services']\n",
      "                break\n",
      "        t.sleep(5)\n",
      "    \n",
      "    if current_services is None:\n",
      "        return output('none', 'none')\n",
      "        \n",
      "    logger.info(\"Services up\")\n",
      "\n",
      "    logger.info(\"Submitting job request\")\n",
      "\n",
      "    status_code, job_key = request_route(\n",
      "        address = forwarder_address,\n",
      "        port = forwarder_port,\n",
      "        route_type = '',\n",
      "        route_name = 'job-submit',\n",
      "        path_replacers = {},\n",
      "        path_names = [],\n",
      "        route_input = job_request,\n",
      "        timeout = 300\n",
      "    )\n",
      "\n",
      "    current_job_key = job_key['key']\n",
      "\n",
      "    logger.info(\"Current job key: \" + str(current_job_key))\n",
      "\n",
      "    if not status_code == 200:\n",
      "        return output('none', 'none')\n",
      "\n",
      "    if current_job_key == '0':\n",
      "        return output('none', 'none')\n",
      "\n",
      "    logger.info(\"Starting job\")\n",
      "    \n",
      "    status_code, job_start = request_route(\n",
      "        address = forwarder_address,\n",
      "        port = forwarder_port,\n",
      "        route_type = '',\n",
      "        route_name = 'job-run',\n",
      "        path_replacers = {\n",
      "            'name': user\n",
      "        },\n",
      "        path_names = [\n",
      "            current_job_key\n",
      "        ],\n",
      "        route_input = {},\n",
      "        timeout = 300\n",
      "    )\n",
      "\n",
      "    job_start_status = job_start['status']\n",
      "\n",
      "    logger.info(\"Job started: \" + str(job_start_status))\n",
      "\n",
      "    if not job_start_status == 'success':\n",
      "        return output('none', 'none')\n",
      "\n",
      "    logger.info(\"Waiting job to run\")\n",
      "\n",
      "    current_jobid = None\n",
      "    job_timeout = 5000\n",
      "    start = t.time()\n",
      "    while t.time() - start <= job_timeout:\n",
      "        status_code, job_data = request_route(\n",
      "            address = forwarder_address,\n",
      "            port = forwarder_port,\n",
      "            route_type = '',\n",
      "            route_name = 'job-artifact',\n",
      "            path_replacers = {\n",
      "                'type': 'status',\n",
      "                'name': user\n",
      "            },\n",
      "            path_names = [\n",
      "                current_job_key\n",
      "            ],\n",
      "            route_input = {},\n",
      "            timeout = 500\n",
      "        )\n",
      "\n",
      "        if not status_code == 200:\n",
      "            return output('none', 'none')\n",
      "\n",
      "        if not 'job-status' in job_data:\n",
      "            return output('none', 'none')\n",
      "\n",
      "        job_status = job_data['job-status']\n",
      "\n",
      "        if 'cancel' in job_status or 'stopped' in job_status:\n",
      "            if job_status['cancel'] or job_status['stopped']:\n",
      "                break\n",
      "\n",
      "        if 'pending' in job_status or 'running' in job_status:\n",
      "            if job_status['pending'] or job_status['running']:\n",
      "                current_jobid = job_status['id']\n",
      "                break\n",
      "\n",
      "        t.sleep(5)\n",
      "\n",
      "    if current_jobid is None:\n",
      "        return output('none', 'none')\n",
      "    \n",
      "    logger.info('SLURM job id: ' + str(current_jobid))\n",
      "\n",
      "    logger.info('Setting up Ray')\n",
      "    logger.info(current_services)\n",
      "    ray_client = setup_ray(\n",
      "        logger = logger,\n",
      "        services = current_services,\n",
      "        timeout = ray_job_timeout\n",
      "    )\n",
      "    \n",
      "    if ray_client is None:\n",
      "        return output('none', 'none')\n",
      "\n",
      "    logger.info('Ray client setup')\n",
      "\n",
      "    logger.info('Setting up MLFlow')\n",
      "   \n",
      "    setup_mlflow(\n",
      "        logger = logger,\n",
      "        mlflow_parameters = mlflow_parameters\n",
      "    )\n",
      "\n",
      "    logger.info('MLflow setup')\n",
      "\n",
      "    with mlflow.start_run() as run:\n",
      "        run_id = run.info.run_id\n",
      "        logger.info(f\"Run ID: {run_id}\")\n",
      "\n",
      "        logger.info('Running ray job: ' + str(ray_job_file))\n",
      "\n",
      "        ray_job_success = ray_job_handler(\n",
      "            logger = logger,\n",
      "            storage_client = storage_client,\n",
      "            storage_name = pipeline_bucket,\n",
      "            ray_client = ray_client,\n",
      "            ray_parameters = ray_parameters,\n",
      "            ray_job_file = ray_job_file,\n",
      "            ray_job_envs = ray_job_envs,\n",
      "            timeout = ray_job_timeout\n",
      "        )\n",
      "\n",
      "        logger.info('Ray job ran: ' + str(ray_job_success))\n",
      "\n",
      "        status_code, cancel_data = request_route(\n",
      "            address = forwarder_address,\n",
      "            port = forwarder_port,\n",
      "            route_type = '',\n",
      "            route_name = 'job-cancel',\n",
      "            path_replacers = {\n",
      "                'name': user,\n",
      "            },\n",
      "            path_names = [\n",
      "                current_job_key\n",
      "            ],\n",
      "            route_input = {},\n",
      "            timeout = 300\n",
      "        )\n",
      "        \n",
      "        logger.info('SLURM job cancel: ' + str(cancel_data))\n",
      "        if not ray_job_success:\n",
      "            return output('none', 'none')\n",
      "\n",
      "        logger.info('Collecting Artifacts')\n",
      " \n",
      "        collected_parameters = {}\n",
      "        collected_metrics = {}\n",
      "\n",
      "        logger.info('Hyperarameters')\n",
      "\n",
      "        for key,value in ray_parameters.items():\n",
      "            if 'hp-' in key:\n",
      "                formatted_key = key.replace('hp-', '')\n",
      "                logger.info(str(key) + '=' + str(value))\n",
      "                collected_parameters[formatted_key] = value\n",
      "\n",
      "        logger.info('Getting model parameters')\n",
      "\n",
      "        parameters_object = get_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = pipeline_bucket,\n",
      "            object_name = 'artifacts',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [\n",
      "                'parameters'\n",
      "            ]\n",
      "        )\n",
      "        model_available = False\n",
      "        if not parameters_object is None:\n",
      "            parameters_object_data = parameters_object['data']\n",
      "\n",
      "            logger.info('Logging model')\n",
      "\n",
      "            trained_model = CNNClassifier()\n",
      "            trained_model.load_state_dict(parameters_object_data['model'])\n",
      "            trained_model.eval()\n",
      "            \n",
      "            model_name = mlflow_parameters['model-name']\n",
      "            registered_name = mlflow_parameters['registered-name']\n",
      "            mlflow.pytorch.log_model(\n",
      "                trained_model, \n",
      "                model_name,\n",
      "                registered_model_name = registered_name\n",
      "            )\n",
      "            model_available = True\n",
      "\n",
      "        logger.info('Getting model predictions')\n",
      "\n",
      "        predictions_object = get_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = pipeline_bucket,\n",
      "            object_name = 'artifacts',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [\n",
      "                'predictions'\n",
      "            ]\n",
      "        ) \n",
      "\n",
      "        if not predictions_object is None:\n",
      "            predictions_object_data = predictions_object['data']\n",
      "\n",
      "            logger.info(\"Logging predictions\")\n",
      "            np.save(\"predictions.npy\", predictions_object_data)\n",
      "            mlflow.log_artifact(\n",
      "                local_path = \"predictions.npy\",\n",
      "                artifact_path = \"predicted_qualities/\"\n",
      "            )\n",
      "        \n",
      "        logger.info(\"Logging metrics\")\n",
      "        metrics_object = get_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = pipeline_bucket,\n",
      "            object_name = 'artifacts',\n",
      "            path_replacers = {\n",
      "                'name': folder_name\n",
      "            },\n",
      "            path_names = [\n",
      "                'metrics'\n",
      "            ]\n",
      "        ) \n",
      "\n",
      "        if not metrics_object is None:\n",
      "            performance = metrics_object['data']\n",
      "\n",
      "            parsed_performance = parse_torchmetrics(\n",
      "                metrics = performance,\n",
      "                labels = {\n",
      "                    0: 'top',\n",
      "                    1: 'trouser',\n",
      "                    2: 'pullover',\n",
      "                    3: 'dress',\n",
      "                    4: 'coat',\n",
      "                    5: 'sandal',\n",
      "                    6: 'shirt',\n",
      "                    7: 'sneaker',\n",
      "                    8: 'bag',\n",
      "                    9: 'ankle-boot',\n",
      "                }\n",
      "            )\n",
      "\n",
      "            for key,value in parsed_performance.items():\n",
      "                collected_metrics[key] = value\n",
      "                    \n",
      "        logger.info(\"Waiting sacct and seff\")\n",
      "        store_timeout = 5000\n",
      "        start = t.time()\n",
      "        stored = False\n",
      "        while t.time() - start <= store_timeout:\n",
      "            status_code, job_data = request_route(\n",
      "                address = forwarder_address,\n",
      "                port = forwarder_port,\n",
      "                route_type = '',\n",
      "                route_name = 'job-artifact',\n",
      "                path_replacers = {\n",
      "                    'type': 'status',\n",
      "                    'name': user\n",
      "                },\n",
      "                path_names = [\n",
      "                    current_job_key\n",
      "                ],\n",
      "                route_input = {},\n",
      "                timeout = 500\n",
      "            )\n",
      "\n",
      "            if not status_code == 200:\n",
      "                break\n",
      "\n",
      "            if not 'job-status' in job_data:\n",
      "                break\n",
      "\n",
      "            job_status = job_data['job-status']\n",
      "\n",
      "            if 'stored' in job_status:\n",
      "                if job_status['stored']:\n",
      "                    stored = True\n",
      "                    break\n",
      "\n",
      "            t.sleep(5)\n",
      "\n",
      "        if stored:\n",
      "            logger.info(\"Fetching sacct\")\n",
      "            status_code, sacct_data = request_route(\n",
      "                address = forwarder_address,\n",
      "                port = forwarder_port,\n",
      "                route_type = '',\n",
      "                route_name = 'job-artifact',\n",
      "                path_replacers = {\n",
      "                    'type': 'sacct',\n",
      "                    'name': user\n",
      "                },\n",
      "                path_names = [\n",
      "                    current_job_key\n",
      "                ],\n",
      "                route_input = {},\n",
      "                timeout = 300\n",
      "            )\n",
      "\n",
      "            if status_code == 200:\n",
      "                if 'job-sacct' in sacct_data:\n",
      "                    logger.info(\"Logging sacct\")\n",
      "                    job_sacct = sacct_data['job-sacct']\n",
      "                    params, metrics = parse_job_sacct(\n",
      "                        logger = logger,\n",
      "                        sacct = job_sacct\n",
      "                    )\n",
      "\n",
      "                    for key,value in params.items():\n",
      "                        collected_parameters[key] = value\n",
      "\n",
      "                    for key,value in metrics.items():\n",
      "                        collected_metrics[key] = value\n",
      "\n",
      "            logger.info(\"Fetching seff\")\n",
      "            status_code, seff_data = request_route(\n",
      "                address = forwarder_address,\n",
      "                port = forwarder_port,\n",
      "                route_type = '',\n",
      "                route_name = 'job-artifact',\n",
      "                path_replacers = {\n",
      "                    'type': 'seff',\n",
      "                    'name': user\n",
      "                },\n",
      "                path_names = [\n",
      "                    current_job_key\n",
      "                ],\n",
      "                route_input = {},\n",
      "                timeout = 300\n",
      "            )\n",
      "            \n",
      "            if status_code == 200:\n",
      "                if 'job-seff' in seff_data:\n",
      "                    logger.info(\"Logging seff\")\n",
      "                    job_seff = seff_data['job-seff']\n",
      "                    params, metrics = parse_job_seff(\n",
      "                        logger = logger,\n",
      "                        seff = job_seff \n",
      "                    )\n",
      "\n",
      "                    for key,value in params.items():\n",
      "                        collected_parameters[key] = value\n",
      "\n",
      "                    for key,value in metrics.items():\n",
      "                        collected_metrics[key] = value\n",
      "\n",
      "        logger.info(\"Logging parameters and metrics\")\n",
      "\n",
      "        for key,value in collected_parameters.items():\n",
      "            mlflow.log_param(key, value)\n",
      "        logger.info(\"Parameters logged\")\n",
      "\n",
      "        for key,value in collected_metrics.items():\n",
      "            mlflow.log_metric(key, value)\n",
      "        logger.info(\"Metrics logged\")\n",
      "        \n",
      "        logger.info(\"Canceling imports\")\n",
      "\n",
      "        status_code, cancel_data = request_route(\n",
      "            address = forwarder_address,\n",
      "            port = forwarder_port,\n",
      "            route_type = '',\n",
      "            route_name = 'forwarding-cancel',\n",
      "            path_replacers = {\n",
      "                'type': 'imports',\n",
      "                'user': user,\n",
      "                'key': import_key\n",
      "            },\n",
      "            path_names = [],\n",
      "            route_input = {},\n",
      "            timeout = 300\n",
      "        )\n",
      "\n",
      "        logger.info(\"Cancellation success\")\n",
      "        \n",
      "        component_time_end = t.time()\n",
      "        \n",
      "        logger.info(\"Storing time\")\n",
      "        gather_time( \n",
      "            storage_client = storage_client,\n",
      "            storage_name = pipeline_bucket,\n",
      "            time_group = 'components',\n",
      "            time_name = 'cloud-hpc-train',\n",
      "            start_time = component_time_start,\n",
      "            end_time = component_time_end\n",
      "        )\n",
      "\n",
      "        if not model_available:\n",
      "            return output('none', 'none')\n",
      "\n",
      "        # return str(mlflow.get_artifact_uri())\n",
      "        return output(mlflow.get_artifact_uri(), run_id)\n",
      "## Evaluate\n",
      "@component(\n",
      "    base_image = \"python:3.10\",\n",
      "    packages_to_install = [\n",
      "        \"python-swiftclient\",\n",
      "        \"mlflow~=2.12.2\", \n",
      "    ],\n",
      "    output_component_file = 'components/evaluate_component.yaml',\n",
      ")\n",
      "def evaluate(   \n",
      "    storage_parameters: dict,\n",
      "    integration_parameters: dict,\n",
      "    mlflow_parameters: dict,\n",
      "    metric_parameters: dict,\n",
      "    run_id: str\n",
      ") -> bool:\n",
      "    from mlflow.tracking import MlflowClient\n",
      "    import logging\n",
      "\n",
      "    import time as t\n",
      "    \n",
      "    import swiftclient as sc\n",
      "    import pickle\n",
      "\n",
      "    import re\n",
      "    import json\n",
      "    import requests\n",
      "\n",
      "    # BOILERPLATE START\n",
      "\n",
      "    def set_formatted_user(\n",
      "        user: str   \n",
      "    ) -> any:\n",
      "        return re.sub(r'[^a-z0-9]+', '-', user)\n",
      "    def general_object_metadata():\n",
      "        general_object_metadata = {\n",
      "            'version': 1\n",
      "        }\n",
      "        return general_object_metadata\n",
      "    # Works\n",
      "    def is_swift_client(\n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        return isinstance(storage_client, sc.Connection)\n",
      "    # Works\n",
      "    def swift_setup_client(\n",
      "        pre_auth_url: str,\n",
      "        pre_auth_token: str,\n",
      "        user_domain_name: str,\n",
      "        project_domain_name: str,\n",
      "        project_name: str,\n",
      "        auth_version: str\n",
      "    ) -> any:\n",
      "        swift_client = sc.Connection(\n",
      "            preauthurl = pre_auth_url,\n",
      "            preauthtoken = pre_auth_token,\n",
      "            os_options = {\n",
      "                'user_domain_name': user_domain_name,\n",
      "                'project_domain_name': project_domain_name,\n",
      "                'project_name': project_name\n",
      "            },\n",
      "            auth_version = auth_version\n",
      "        )\n",
      "        return swift_client\n",
      "    # Works\n",
      "    def swift_create_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> bool:\n",
      "        try:\n",
      "            swift_client.put_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_check_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name:str\n",
      "    ) -> any:\n",
      "        try:\n",
      "            bucket_info = swift_client.get_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            bucket_metadata = bucket_info[0]\n",
      "            list_of_objects = bucket_info[1]\n",
      "            return {'metadata': bucket_metadata, 'objects': list_of_objects}\n",
      "        except Exception as e:\n",
      "            return {} \n",
      "    # Refactored\n",
      "    def swift_delete_bucket(\n",
      "        swift_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> bool:\n",
      "        try:\n",
      "            swift_client.delete_container(\n",
      "                container = bucket_name\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Created and works\n",
      "    def swift_list_buckets(\n",
      "        swift_client: any\n",
      "    ) -> any:\n",
      "        try:\n",
      "            account_buckets = swift_client.get_account()[1]\n",
      "            return account_buckets\n",
      "        except Exception as e:\n",
      "            return {}\n",
      "    # Works\n",
      "    def swift_create_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> bool: \n",
      "        # This should be updated to handle 5 GB objects\n",
      "        # It also should handle metadata\n",
      "        try:\n",
      "            swift_client.put_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path,\n",
      "                contents = object_data,\n",
      "                headers = object_metadata\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_check_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> any: \n",
      "        try:\n",
      "            object_metadata = swift_client.head_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path\n",
      "            )       \n",
      "            return object_metadata\n",
      "        except Exception as e:\n",
      "            return {} \n",
      "    # Refactored and works\n",
      "    def swift_get_object(\n",
      "        swift_client:any,\n",
      "        bucket_name: str,\n",
      "        object_path: str\n",
      "    ) -> any:\n",
      "        try:\n",
      "            response = swift_client.get_object(\n",
      "                container = bucket_name,\n",
      "                obj = object_path \n",
      "            )\n",
      "            object_info = response[0]\n",
      "            object_data = response[1]\n",
      "            return {'data': object_data, 'info': object_info}\n",
      "        except Exception as e:\n",
      "            return {}     \n",
      "    # Refactored   \n",
      "    def swift_remove_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> bool: \n",
      "        try:\n",
      "            swift_client.delete_object(\n",
      "                container = bucket_name, \n",
      "                obj = object_path\n",
      "            )\n",
      "            return True\n",
      "        except Exception as e:\n",
      "            return False\n",
      "    # Works\n",
      "    def swift_update_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> bool:  \n",
      "        remove = swift_remove_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path\n",
      "        )\n",
      "        if not remove:\n",
      "            return False\n",
      "        create = swift_create_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path, \n",
      "            object_data = object_data,\n",
      "            object_metadata = object_metadata\n",
      "        )\n",
      "        return create\n",
      "    # Works\n",
      "    def swift_create_or_update_object(\n",
      "        swift_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        bucket_info = swift_check_bucket(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "        \n",
      "        if len(bucket_info) == 0:\n",
      "            creation_status = swift_create_bucket(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name\n",
      "            )\n",
      "            if not creation_status:\n",
      "                return False\n",
      "        \n",
      "        object_info = swift_check_object(\n",
      "            swift_client = swift_client, \n",
      "            bucket_name = bucket_name, \n",
      "            object_path = object_path\n",
      "        )\n",
      "        \n",
      "        if len(object_info) == 0:\n",
      "            return swift_create_object(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name, \n",
      "                object_path = object_path, \n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "        else:\n",
      "            return swift_update_object(\n",
      "                swift_client = swift_client, \n",
      "                bucket_name = bucket_name, \n",
      "                object_path = object_path, \n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "\n",
      "        # Refactored and Works\n",
      "    def set_encoded_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        encoded_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                encoded_key = key_initial + '-' + key\n",
      "                if isinstance(value, list):\n",
      "                    encoded_metadata[encoded_key] = 'list=' + ','.join(map(str, value))\n",
      "                    continue\n",
      "                encoded_metadata[encoded_key] = str(value)\n",
      "        return encoded_metadata\n",
      "    # Refactored and works\n",
      "    def get_general_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        general_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                if not key_initial == key[:len(key_initial)]:\n",
      "                    general_metadata[key] = value\n",
      "        return general_metadata\n",
      "    # Refactored and works\n",
      "    def get_decoded_metadata(\n",
      "        used_client: str,\n",
      "        object_metadata: any\n",
      "    ) -> any: \n",
      "        decoded_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            key_initial = 'x-object-meta'\n",
      "            for key, value in object_metadata.items():\n",
      "                if key_initial == key[:len(key_initial)]:\n",
      "                    decoded_key = key[len(key_initial) + 1:]\n",
      "                    if 'list=' in value:\n",
      "                        string_integers = value.split('=')[1]\n",
      "                        values = string_integers.split(',')\n",
      "                        if len(values) == 1 and values[0] == '':\n",
      "                            decoded_metadata[decoded_key] = []\n",
      "                        else:\n",
      "                            try:\n",
      "                                decoded_metadata[decoded_key] = list(map(int, values))\n",
      "                            except:\n",
      "                                decoded_metadata[decoded_key] = list(map(str, values))\n",
      "                        continue\n",
      "                    if value.isnumeric():\n",
      "                        decoded_metadata[decoded_key] = int(value)\n",
      "                        continue\n",
      "                    decoded_metadata[decoded_key] = value\n",
      "        return decoded_metadata\n",
      "    # Refactored and works\n",
      "    def set_bucket_names(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_names = []\n",
      "        bucket_prefix = storage_parameters['bucket-prefix']\n",
      "        ice_id = storage_parameters['ice-id']\n",
      "        user = storage_parameters['user']\n",
      "        storage_names.append(bucket_prefix + '-forwarder-' + ice_id)\n",
      "        storage_names.append(bucket_prefix + '-submitter-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        storage_names.append(bucket_prefix + '-pipeline-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        storage_names.append(bucket_prefix + '-experiment-' + ice_id + '-' + set_formatted_user(user = user))\n",
      "        return storage_names\n",
      "    # created and works\n",
      "    def setup_storage(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = setup_storage_client(\n",
      "            storage_parameters = storage_parameters\n",
      "        ) \n",
      "        \n",
      "        storage_name = set_bucket_names(\n",
      "        storage_parameters = storage_parameters\n",
      "        )\n",
      "        \n",
      "        return storage_client, storage_name\n",
      "    # Refactored and works\n",
      "    def setup_storage_client(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = None\n",
      "        if storage_parameters['used-client'] == 'swift':\n",
      "            storage_client = swift_setup_client(\n",
      "                pre_auth_url = storage_parameters['pre-auth-url'],\n",
      "                pre_auth_token = storage_parameters['pre-auth-token'],\n",
      "                user_domain_name = storage_parameters['user-domain-name'],\n",
      "                project_domain_name = storage_parameters['project-domain-name'],\n",
      "                project_name = storage_parameters['project-name'],\n",
      "                auth_version = storage_parameters['auth-version']\n",
      "            )\n",
      "        return storage_client\n",
      "    # Refactored and works\n",
      "    def check_object_metadata(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> any: \n",
      "        object_metadata = {\n",
      "            'general-meta': {},\n",
      "            'custom-meta': {}\n",
      "        }\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            all_metadata = swift_check_object(\n",
      "            swift_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "            ) \n",
      "\n",
      "            general_metadata = {}\n",
      "            custom_metadata = {}\n",
      "            if not len(all_metadata) == 0:\n",
      "                general_metadata = get_general_metadata(\n",
      "                    used_client = 'swift',\n",
      "                    object_metadata = all_metadata\n",
      "                )\n",
      "                custom_metadata = get_decoded_metadata(\n",
      "                    used_client = 'swift',\n",
      "                    object_metadata = all_metadata\n",
      "                )\n",
      "\n",
      "            object_metadata['general-meta'] = general_metadata\n",
      "            object_metadata['custom-meta'] = custom_metadata\n",
      "\n",
      "        return object_metadata\n",
      "    # Refactored and works\n",
      "    def get_object_content(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_path: str\n",
      "    ) -> any:\n",
      "        object_content = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            fetched_object = swift_get_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path\n",
      "            )\n",
      "            object_content['data'] = pickle.loads(fetched_object['data'])\n",
      "            object_content['general-meta'] = get_general_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = fetched_object['info']\n",
      "            )\n",
      "            object_content['custom-meta'] = get_decoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = fetched_object['info']\n",
      "            )\n",
      "        return object_content\n",
      "    # Refactored    \n",
      "    def remove_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str\n",
      "    ) -> bool: \n",
      "        removed = False\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            removed = swift_remove_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path\n",
      "            )\n",
      "        return removed\n",
      "    # Refactored and works\n",
      "    def create_or_update_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str, \n",
      "        object_path: str, \n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ) -> any:\n",
      "        success = False\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            formatted_data = pickle.dumps(object_data)\n",
      "            formatted_metadata = set_encoded_metadata(\n",
      "                used_client = 'swift',\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "\n",
      "            success = swift_create_or_update_object(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = object_path,\n",
      "                object_data = formatted_data,\n",
      "                object_metadata = formatted_metadata\n",
      "            )\n",
      "        return success\n",
      "    # Created and works\n",
      "    def format_bucket_metadata(\n",
      "        used_client: str,\n",
      "        bucket_metadata: any\n",
      "    ) -> any:\n",
      "        formatted_metadata = {}\n",
      "        if used_client == 'swift':\n",
      "            relevant_values = {\n",
      "                'x-container-object-count': 'object-count',\n",
      "                'x-container-bytes-used-actual': 'used-bytes',\n",
      "                'last-modified': 'date',\n",
      "                'content-type': 'type'\n",
      "            }\n",
      "            formatted_metadata = {}\n",
      "            for key,value in bucket_metadata.items():\n",
      "                if key in relevant_values:\n",
      "                    formatted_key = relevant_values[key]\n",
      "                    formatted_metadata[formatted_key] = value\n",
      "        return formatted_metadata\n",
      "    # Created and works\n",
      "    def format_bucket_objects(\n",
      "        used_client: str,\n",
      "        bucket_objects: any\n",
      "    ) -> any:\n",
      "        formatted_objects = {}\n",
      "        if used_client == 'swift':\n",
      "            for bucket_object in bucket_objects:\n",
      "                formatted_object_metadata = {\n",
      "                    'hash': 'id',\n",
      "                    'bytes': 'used-bytes',\n",
      "                    'last_modified': 'date'\n",
      "                }\n",
      "                object_key = None\n",
      "                object_metadata = {}\n",
      "                for key, value in bucket_object.items():\n",
      "                    if key == 'name':\n",
      "                        object_key = value\n",
      "                    if key in formatted_object_metadata:\n",
      "                        formatted_key = formatted_object_metadata[key]\n",
      "                        object_metadata[formatted_key] = value\n",
      "                formatted_objects[object_key] = object_metadata\n",
      "        return formatted_objects\n",
      "    # Created and works\n",
      "    def format_bucket_info(\n",
      "        used_client: str,\n",
      "        bucket_info: any\n",
      "    ) -> any:\n",
      "        bucket_metadata = {}\n",
      "        bucket_objects = {}\n",
      "        if used_client == 'swift':\n",
      "            bucket_metadata = format_bucket_metadata(\n",
      "                used_client = used_client,\n",
      "                bucket_metadata = bucket_info['metadata']\n",
      "            )\n",
      "            bucket_objects = format_bucket_objects(\n",
      "                used_client = used_client,\n",
      "                bucket_objects = bucket_info['objects']\n",
      "            )\n",
      "        return {'metadata': bucket_metadata, 'objects': bucket_objects} \n",
      "    # Created and works\n",
      "    def get_bucket_info(\n",
      "        storage_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> any:\n",
      "        bucket_info = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            unformatted_bucket_info = swift_check_bucket(\n",
      "                swift_client = storage_client,\n",
      "                bucket_name = bucket_name\n",
      "            )\n",
      "            bucket_info = format_bucket_info(\n",
      "                used_client = 'swift',\n",
      "                bucket_info = unformatted_bucket_info\n",
      "            )\n",
      "        return bucket_info\n",
      "    # Created and works\n",
      "    def format_container_info(\n",
      "        used_client: str,\n",
      "        container_info: any\n",
      "    ) -> any:\n",
      "        formatted_container_info = {}\n",
      "        if used_client == 'swift':\n",
      "            for bucket in container_info:\n",
      "                bucket_name = bucket['name']\n",
      "                bucket_count = bucket['count']\n",
      "                bucket_size = bucket['bytes']\n",
      "                formatted_container_info[bucket_name] = {\n",
      "                    'amount': bucket_count,\n",
      "                    'size': bucket_size\n",
      "                }\n",
      "        return formatted_container_info\n",
      "    # Created and works\n",
      "    def get_container_info( \n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        container_info = {}\n",
      "        if is_swift_client(storage_client = storage_client):\n",
      "            unformatted_container_info = swift_list_buckets(\n",
      "                swift_client = storage_client \n",
      "            )\n",
      "            container_info = format_container_info(\n",
      "                used_client = 'swift',\n",
      "                container_info = unformatted_container_info\n",
      "            )\n",
      "        return container_info\n",
      "    \n",
      "        # Created and works\n",
      "    def set_object_path(\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ):\n",
      "        object_paths = {\n",
      "            'root': 'name',\n",
      "            'code': 'CODE/name',\n",
      "            'slurm': 'CODE/SLURM/name',\n",
      "            'ray': 'CODE/RAY/name',\n",
      "            'data': 'DATA/name',\n",
      "            'artifacts': 'ARTIFACTS/name',\n",
      "            'time': 'TIMES/name'\n",
      "        }\n",
      "\n",
      "        i = 0\n",
      "        path_split = object_paths[object_name].split('/')\n",
      "        for name in path_split:\n",
      "            if name in path_replacers:\n",
      "                replacer = path_replacers[name]\n",
      "                if 0 < len(replacer):\n",
      "                    path_split[i] = replacer\n",
      "            i = i + 1\n",
      "        \n",
      "        if not len(path_names) == 0:\n",
      "            path_split.extend(path_names)\n",
      "\n",
      "        object_path = '/'.join(path_split)\n",
      "        #print('Used object path:' + str(object_path))\n",
      "        return object_path\n",
      "    # created and works\n",
      "    def setup_storage(\n",
      "        storage_parameters: any\n",
      "    ) -> any:\n",
      "        storage_client = setup_storage_client(\n",
      "            storage_parameters = storage_parameters\n",
      "        ) \n",
      "        \n",
      "        storage_name = set_bucket_names(\n",
      "        storage_parameters = storage_parameters\n",
      "        )\n",
      "        \n",
      "        return storage_client, storage_name\n",
      "    # Created and works\n",
      "    def check_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ) -> bool:\n",
      "        object_path = set_object_path(\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "        # Consider making these functions object storage agnostic\n",
      "        object_metadata = check_object_metadata(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_path = object_path\n",
      "        )\n",
      "        object_metadata['path'] = object_path\n",
      "        return object_metadata\n",
      "    # Created and works\n",
      "    def get_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ) -> any:\n",
      "        checked_object = check_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "\n",
      "        object_data = None\n",
      "        if not len(checked_object['general-meta']) == 0:\n",
      "            # Consider making these functions object storage agnostic\n",
      "            object_data = get_object_content(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = checked_object['path']\n",
      "            )\n",
      "\n",
      "        return object_data\n",
      "    # Created and Works\n",
      "    def set_object(\n",
      "        storage_client: any,\n",
      "        bucket_name: str,\n",
      "        object_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any,\n",
      "        overwrite: bool,\n",
      "        object_data: any,\n",
      "        object_metadata: any\n",
      "    ):\n",
      "        checked_object = check_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name,\n",
      "            object_name = object_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "        \n",
      "        perform = True\n",
      "        if not len(checked_object['general-meta']) == 0 and not overwrite:\n",
      "            perform = False\n",
      "        \n",
      "        if perform:\n",
      "            # Consider making these functions object storage agnostic\n",
      "            create_or_update_object(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = bucket_name,\n",
      "                object_path = checked_object['path'],\n",
      "                object_data = object_data,\n",
      "                object_metadata = object_metadata\n",
      "            )\n",
      "    # Created and works\n",
      "    def check_bucket(\n",
      "        storage_client: any,\n",
      "        bucket_name: str\n",
      "    ) -> any:\n",
      "        return get_bucket_info(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = bucket_name\n",
      "        )\n",
      "    # Created and works\n",
      "    def check_buckets(\n",
      "        storage_client: any\n",
      "    ) -> any:\n",
      "        return get_container_info( \n",
      "            storage_client = storage_client\n",
      "        )\n",
      "\n",
      "    def gather_time(\n",
      "        storage_client: any,\n",
      "        storage_name: any,\n",
      "        time_group: any,\n",
      "        time_name: any,\n",
      "        start_time: int,\n",
      "        end_time: int\n",
      "    ):\n",
      "        time_object = get_object(\n",
      "            storage_client = storage_client,\n",
      "            bucket_name = storage_name,\n",
      "            object_name = 'time',\n",
      "            path_replacers = {\n",
      "                'name': time_group\n",
      "            },\n",
      "            path_names = []\n",
      "        )\n",
      "\n",
      "        time_data = {}\n",
      "        time_metadata = {} \n",
      "        if time_object is None:\n",
      "            time_data = {}\n",
      "            time_metadata = general_object_metadata()\n",
      "        else:\n",
      "            time_data = time_object['data']\n",
      "            time_metadata = time_object['custom-meta']\n",
      "        \n",
      "        current_key_amount = len(time_data)\n",
      "        current_key_full = False\n",
      "        current_key = str(current_key_amount)\n",
      "        if 0 < current_key_amount:\n",
      "            time_object = time_data[current_key]\n",
      "            if 0 < time_object['total-seconds']:\n",
      "                current_key_full = True\n",
      "        \n",
      "        changed = False\n",
      "        if 0 < end_time and 0 < current_key_amount and not current_key_full:\n",
      "            stored_start_time = time_data[current_key]['start-time']\n",
      "            time_diff = (end_time-stored_start_time)\n",
      "            time_data[current_key]['end-time'] = end_time\n",
      "            time_data[current_key]['total-seconds'] = round(time_diff,5)\n",
      "            changed = True\n",
      "        else:\n",
      "            next_key_amount = len(time_data) + 1\n",
      "            new_key = str(next_key_amount)\n",
      "        \n",
      "            if 0 < start_time and 0 == end_time:\n",
      "                time_data[new_key] = {\n",
      "                    'name': time_name,\n",
      "                    'start-time': start_time,\n",
      "                    'end-time': 0,\n",
      "                    'total-seconds': 0\n",
      "                }\n",
      "                changed = True\n",
      "\n",
      "            if 0 < start_time and 0 < end_time:\n",
      "                time_diff = (end_time-start_time)\n",
      "                time_data[new_key] = {\n",
      "                    'name': time_name,\n",
      "                    'start-time': start_time,\n",
      "                    'end-time': end_time,\n",
      "                    'total-seconds': round(time_diff,5)\n",
      "                }\n",
      "                changed = True\n",
      "\n",
      "        if changed:\n",
      "            time_metadata['version'] = time_metadata['version'] + 1\n",
      "            set_object(\n",
      "                storage_client = storage_client,\n",
      "                bucket_name = storage_name,\n",
      "                object_name = 'time',\n",
      "                path_replacers = {\n",
      "                    'name': time_group\n",
      "                },\n",
      "                path_names = [],\n",
      "                overwrite = True,\n",
      "                object_data = time_data,\n",
      "                object_metadata = time_metadata \n",
      "            )\n",
      "    \n",
      "    def set_route(\n",
      "        route_type: str,\n",
      "        route_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any\n",
      "    ): \n",
      "        # Check job-run and job-cancel\n",
      "        routes = {\n",
      "            'root': 'TYPE:/name',\n",
      "            'logs': 'GET:/general/logs/name',\n",
      "            'structure': 'GET:/general/structure',\n",
      "            'setup': 'POST:/setup/config',\n",
      "            'start': 'POST:/setup/start',\n",
      "            'stop': 'POST:/setup/stop',\n",
      "            'job-submit': 'POST:/requests/submit/job',\n",
      "            'job-run': 'PUT:/requests/run/job/name',\n",
      "            'job-cancel': 'PUT:/requests/cancel/job/name',\n",
      "            'forwarding-submit': 'POST:/requests/submit/forwarding',\n",
      "            'forwarding-cancel': 'PUT:/requests/cancel/type/user/key',\n",
      "            'task-request': 'PUT:/tasks/request/signature',\n",
      "            'task-result': 'GET:/tasks/result/id',\n",
      "            'job-artifact': 'GET:/artifacts/job/type/name',\n",
      "            'forwarding-artifact': 'GET:/artifacts/forwarding/type/user/key'\n",
      "        }\n",
      "\n",
      "        route = None\n",
      "        if route_name in routes:\n",
      "            i = 0\n",
      "            route = routes[route_name].split('/')\n",
      "            for name in route:\n",
      "                if name in path_replacers:\n",
      "                    replacer = path_replacers[name]\n",
      "                    if 0 < len(replacer):\n",
      "                        route[i] = replacer\n",
      "                i = i + 1\n",
      "\n",
      "            if not len(path_names) == 0:\n",
      "                route.extend(path_names)\n",
      "\n",
      "            if not len(route_type) == 0:\n",
      "                route[0] = route_type + ':'\n",
      "            \n",
      "            route = '/'.join(route)\n",
      "        #print('Used route: ' + str(route))\n",
      "        return route\n",
      "    # Created and works\n",
      "    def get_response(\n",
      "        route_type: str,\n",
      "        route_url: str,\n",
      "        route_input: any\n",
      "    ) -> any:\n",
      "        route_response = None\n",
      "        if route_type == 'POST':\n",
      "            route_response = requests.post(\n",
      "                url = route_url,\n",
      "                json = route_input\n",
      "            )\n",
      "        if route_type == 'PUT':\n",
      "            route_response = requests.put(\n",
      "                url = route_url\n",
      "            )\n",
      "        if route_type == 'GET':\n",
      "            route_response = requests.get(\n",
      "                url = route_url\n",
      "            )\n",
      "        return route_response\n",
      "    # Created and works\n",
      "    def set_full_url(\n",
      "        address: str,\n",
      "        port: str,\n",
      "        used_route: str\n",
      "    ) -> str:\n",
      "        url_prefix = 'http://' + address + ':' + port\n",
      "        route_split = used_route.split(':')\n",
      "        url_type = route_split[0]\n",
      "        used_path = route_split[1]\n",
      "        full_url = url_prefix + used_path\n",
      "        return url_type, full_url\n",
      "    # Created and works\n",
      "    def request_route(\n",
      "        address: str,\n",
      "        port: str,\n",
      "        route_type: str,\n",
      "        route_name: str,\n",
      "        path_replacers: any,\n",
      "        path_names: any,\n",
      "        route_input: any,\n",
      "        timeout: any\n",
      "    ) -> any:\n",
      "        used_route = set_route(\n",
      "            route_type = route_type,\n",
      "            route_name = route_name,\n",
      "            path_replacers = path_replacers,\n",
      "            path_names = path_names\n",
      "        )\n",
      "\n",
      "        url_type, full_url = set_full_url(\n",
      "            address = address,\n",
      "            port = port,\n",
      "            used_route = used_route\n",
      "        )\n",
      "\n",
      "        route_response = get_response(\n",
      "            route_type = url_type,\n",
      "            route_url = full_url,\n",
      "            route_input = route_input\n",
      "        )\n",
      "\n",
      "        route_status_code = None\n",
      "        route_returned_text = {}\n",
      "        if not route_response is None:\n",
      "            route_status_code = route_response.status_code\n",
      "            if route_status_code == 200:\n",
      "                route_text = json.loads(route_response.text)\n",
      "\n",
      "                if 'id' in route_text: \n",
      "                    task_result_route = set_route(\n",
      "                        route_type = '',\n",
      "                        route_name = 'task-result',\n",
      "                        path_replacers = {\n",
      "                            'id': route_text['id']\n",
      "                        },\n",
      "                        path_names = []\n",
      "                    )\n",
      "\n",
      "                    task_url_type, task_full_url = set_full_url(\n",
      "                        address = address,\n",
      "                        port = port,\n",
      "                        used_route = task_result_route\n",
      "                    )\n",
      "\n",
      "                    start = t.time()\n",
      "                    while t.time() - start <= timeout:\n",
      "                        task_route_response = get_response(\n",
      "                            route_type = task_url_type,\n",
      "                            route_url = task_full_url,\n",
      "                            route_input = {}\n",
      "                        )\n",
      "                        \n",
      "                        task_status_code = route_response.status_code\n",
      "                            \n",
      "                        if task_status_code == 200:\n",
      "                            task_text = json.loads(task_route_response.text)\n",
      "                            if task_text['status'] == 'FAILED':\n",
      "                                break\n",
      "                            \n",
      "                            if task_text['status'] == 'SUCCESS':\n",
      "                                route_returned_text = task_text['result']\n",
      "                                break\n",
      "                        else:\n",
      "                            break\n",
      "                else:\n",
      "                    route_returned_text = route_text\n",
      "        return route_status_code, route_returned_text\n",
      "\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "\n",
      "    component_time_start = t.time()\n",
      "\n",
      "    storage_client, storage_names = setup_storage( \n",
      "        storage_parameters = storage_parameters\n",
      "    )\n",
      "\n",
      "    logger.info('Storage setup')\n",
      "\n",
      "    pipeline_bucket = storage_names[-2]\n",
      "\n",
      "    logger.info('Used bucket: ' + str(pipeline_bucket))\n",
      "\n",
      "    logger.info('Setting up MLflow')\n",
      "\n",
      "    mlflow_tracking_uri = mlflow_parameters['tracking-uri']\n",
      "    \n",
      "    client = MlflowClient(\n",
      "        tracking_uri = mlflow_tracking_uri\n",
      "    )\n",
      "    info = client.get_run(run_id)\n",
      "    training_metrics = info.data.metrics\n",
      "\n",
      "    logger.info(f\"Training metrics: {training_metrics}\")\n",
      "\n",
      "    # compare the evaluation metrics with the defined thresholds\n",
      "    success = True\n",
      "    for key, value in metric_parameters.items():\n",
      "        logger.info(f\"Checked metric {key} with threshold {value}\")\n",
      "        if key not in training_metrics:\n",
      "            continue\n",
      "        training_metric = training_metrics[key]\n",
      "        if training_metric < value:\n",
      "            logger.error(f\"Metric {key} failed with {training_metric}. Evaluation not passed!\")\n",
      "            success = False\n",
      "\n",
      "    logger.info('Stopping forwarder scheduler')\n",
      "\n",
      "    forwarder_address = integration_parameters['forwarder-address']\n",
      "    forwarder_port = integration_parameters['forwarder-port']\n",
      "    \n",
      "    forwarder_scheduler_stopped = request_route(\n",
      "        address = forwarder_address,\n",
      "        port = forwarder_port,\n",
      "        route_type = '',\n",
      "        route_name = 'stop',\n",
      "        path_replacers = {},\n",
      "        path_names = [],\n",
      "        route_input = {},\n",
      "        timeout = 120\n",
      "    ) \n",
      "\n",
      "    logger.info('Forwarder scheduler stopped: ' + str(forwarder_scheduler_stopped))\n",
      "    \n",
      "    component_time_end = t.time()\n",
      "\n",
      "    gather_time(\n",
      "        storage_client = storage_client,\n",
      "        storage_name = pipeline_bucket,\n",
      "        time_group = 'components',\n",
      "        time_name = 'cloud-hpc-evaluate',\n",
      "        start_time = component_time_start,\n",
      "        end_time = component_time_end\n",
      "    )\n",
      "\n",
      "    return success\n",
      "### Pipeline\n",
      "@dsl.pipeline(\n",
      "    name = 'cloud-hpc-pipeline',\n",
      "    description = 'Cloud-HPC integrated pipeline for Fashion MNIST CNN',\n",
      ")\n",
      "def pipeline(\n",
      "    storage_parameters: dict,\n",
      "    integration_parameters: dict,\n",
      "    mlflow_parameters: dict,\n",
      "    metric_parameters: dict\n",
      "):\n",
      "    preprocess_task = preprocess(\n",
      "        storage_parameters = storage_parameters,\n",
      "        integration_parameters = integration_parameters\n",
      "    )\n",
      "\n",
      "    preprocess_sucess = preprocess_task.output\n",
      "    \n",
      "    with dsl.Condition(preprocess_sucess == 'true'):\n",
      "        train_task = train(\n",
      "            storage_parameters = storage_parameters,\n",
      "            integration_parameters = integration_parameters,\n",
      "            mlflow_parameters = mlflow_parameters\n",
      "        )\n",
      "        train_task.after(preprocess_task)\n",
      "        train_task.apply(use_aws_secret(secret_name=\"aws-secret\"))\n",
      "\n",
      "        with dsl.Condition(train_task.outputs[\"run_id\"] != \"none\"):\n",
      "            evaluate_task = evaluate(\n",
      "                storage_parameters = storage_parameters,\n",
      "                integration_parameters = integration_parameters,\n",
      "                mlflow_parameters = mlflow_parameters,\n",
      "                metric_parameters = metric_parameters,\n",
      "                run_id = train_task.outputs[\"run_id\"]\n",
      "            )\n",
      "            evaluate_task.after(train_task)\n",
      "## Submission\n",
      "### Authentication\n",
      "\n",
      "If you have not used Allas, please check the following CSC docs.\n",
      "\n",
      "- [Python with swift](https://docs.csc.fi/data/Allas/using_allas/python_swift/)\n",
      "- [Keystoneauth loader](https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.loading.html)\n",
      "- [Keystoneauth session](https://docs.openstack.org/keystoneauth/latest/api/keystoneauth1.session.html)\n",
      "- [Swiftclient](https://docs.openstack.org/python-swiftclient/stein/swiftclient.html)\n",
      "- [CPouta APIs](https://docs.csc.fi/cloud/pouta/api-access/)\n",
      "\n",
      "The short version is that you can get auth_url, auth_version, user, key, project name and user domain from the first document and the pre auth url from CPouta/Allas dashboard API access in object_store row. Use these docs to create a .env containing used credentials:\n",
      "#### Arguments\n",
      "storage_parameters = get_storage_parameters(\n",
      "    env_path = env_absolute_path,\n",
      "    auth_url = 'https://pouta.csc.fi:5001/v3',\n",
      "    pre_auth_url = 'https://a3s.fi:443/swift/v1/AUTH_6698ff90e6704a74930c33d6b66f1b5b',\n",
      "    auth_version = '3',\n",
      "    bucket_prefix = 'integration',\n",
      "    ice_id = 's0-c0-u1',\n",
      "    user = 'user@example.com'\n",
      ")\n",
      "\n",
      "storage_client, storage_names = setup_storage(\n",
      "    storage_parameters = storage_parameters\n",
      ")\n",
      "forwarder_configuration = {\n",
      "    'ice-id': 's0-c0-u1',\n",
      "    'enviroments': {\n",
      "        'secrets-path': '',\n",
      "        'cloud': {\n",
      "            'platforms': [\n",
      "                'cpouta'\n",
      "            ]\n",
      "        },\n",
      "        'storage': {\n",
      "            'platforms': [\n",
      "                'allas'\n",
      "            ],\n",
      "            'object-store': {\n",
      "                'used-client': storage_parameters['used-client'],\n",
      "                'pre-auth-url': storage_parameters['pre-auth-url'],\n",
      "                'pre-auth-token': storage_parameters['pre-auth-token'],\n",
      "                'user-domain-name': storage_parameters['user-domain-name'],\n",
      "                'project-domain-name': storage_parameters['project-domain-name'],\n",
      "                'project-name': storage_parameters['project-name'],\n",
      "                'auth-version': storage_parameters['auth-version'],\n",
      "                'bucket-prefix': storage_parameters['bucket-prefix']\n",
      "            }\n",
      "        },\n",
      "        'hpc': {\n",
      "            'platforms': [\n",
      "                'mahti'\n",
      "            ]\n",
      "        },\n",
      "        'integration': {\n",
      "            'platforms': [\n",
      "                'cpouta-mahti'\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "forwarder_scheduler_request = {\n",
      "    'task-times': [\n",
      "        '55',\n",
      "        '170',\n",
      "        '265'\n",
      "    ]\n",
      "}\n",
      "forwarding_request = {\n",
      "    'user': 'user@example.com',\n",
      "    'imports': [\n",
      "        {\n",
      "            'name': 'ray-dashboard',\n",
      "            'address': '192.168.1.13',\n",
      "            'port': '8280'\n",
      "        }\n",
      "    ],\n",
      "    'exports': []\n",
      "}\n",
      "job_request = {\n",
      "    'user': 'user@example.com',\n",
      "    'target': 'hpc/mahti',\n",
      "    'name': '/users/()/ray-cluster.sh',\n",
      "    'enviroment': {\n",
      "        'submission-modules': [\n",
      "            'gcc',\n",
      "            'openmpi',\n",
      "            'openblas',\n",
      "            'csc-tools',\n",
      "            'StdEnv'\n",
      "        ],\n",
      "        'venv': {\n",
      "            'name': 'exp-venv',\n",
      "            'directory': 'users',\n",
      "            'configuration-modules': [\n",
      "                'pytorch'\n",
      "            ],\n",
      "            'packages': [\n",
      "                'ray',\n",
      "                'python-swiftclient',\n",
      "                'torch',\n",
      "                'torchmetrics'\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    'files': {\n",
      "        'provide': [\n",
      "            {\n",
      "                'source': 'local/submitter/secrets|integration|cpouta-mahti|key',\n",
      "                'target': 'hpc/mahti/users/()/cpouta-mahti.pem',\n",
      "                'overwrite': False\n",
      "            },\n",
      "            {\n",
      "                'source': 'storage/allas/pipeline/CODE/SLURM/ray-cluster.sh',\n",
      "                'target': 'hpc/mahti/users/()/ray-cluster.sh',\n",
      "                'overwrite': True\n",
      "            }\n",
      "        ],\n",
      "        'store': [\n",
      "            {\n",
      "                'source': 'hpc/mahti/users/()/slurm-(id).out',\n",
      "                'target': 'storage/allas/pipeline/LOGS/(key)',\n",
      "                'remove': False\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    'workflow': {\n",
      "        'requires': [],\n",
      "        'enables': []\n",
      "    }\n",
      "}\n",
      "integration_parameters = {\n",
      "    'forwarder-address': 'fastapi-service.forwarder.svc.cluster.local',\n",
      "    'forwarder-port': '6500',\n",
      "    'configuration': forwarder_configuration,\n",
      "    'scheduler-request': forwarder_scheduler_request,\n",
      "    'forwarding-request': forwarding_request,\n",
      "    'user': 'user@example.com',\n",
      "    'job-request': job_request,\n",
      "    'ray-job-file': 'train-fmnist-cnn.py',\n",
      "    'ray-job-envs': {},\n",
      "    'folder-name': 'FMNIST',\n",
      "    'ray-parameters': {\n",
      "        'storage-parameters': storage_parameters,\n",
      "        'job-parameters': {\n",
      "            'folder-name': 'FMNIST',\n",
      "            'train-print-rate': 2000,\n",
      "            'hp-train-batch-size': 4,\n",
      "            'hp-test-batch-size': 4,\n",
      "            'hp-seed': 42,\n",
      "            'hp-epochs': 5,\n",
      "            'hp-learning-rate': 0.001,\n",
      "            'hp-momentum': 0.9\n",
      "        }\n",
      "    },\n",
      "    'ray-job-timeout': 5000\n",
      "}\n",
      "mlflow_parameters = {\n",
      "    'tracking-uri': 'http://mlflow.mlflow.svc.cluster.local:5000',\n",
      "    's3-endpoint-url': 'http://mlflow-minio-service.mlflow.svc.cluster.local:9000',\n",
      "    'experiment-name': 'cloud-hpc-fmnist-pipeline',\n",
      "    'model-name': 'cloud-hpc-fmnist-cnn',\n",
      "    'registered-name': 'CLOUD-HPC-FMNIST-CNN'\n",
      "}\n",
      "metric_parameters = {\n",
      "    'accuracy': 0.80,\n",
      "    'precision': 0.80,\n",
      "    'recall': 0.80\n",
      "}\n",
      "pipeline_arguments = {\n",
      "    'storage_parameters': storage_parameters,\n",
      "    'integration_parameters': integration_parameters,\n",
      "    'mlflow_parameters': mlflow_parameters,\n",
      "    'metric_parameters': metric_parameters\n",
      "}\n",
      "def set_route(\n",
      "    route_type: str,\n",
      "    route_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any\n",
      "): \n",
      "    routes = {\n",
      "        'root': 'TYPE:/name',\n",
      "        'logs': 'GET:/general/logs/name',\n",
      "        'structure': 'GET:/general/structure',\n",
      "        'setup': 'POST:/setup/config',\n",
      "        'start': 'POST:/setup/start',\n",
      "        'stop': 'POST:/setup/stop',\n",
      "        'job-submit': 'POST:/requests/submit/job',\n",
      "        'job-run': 'PUT:/requests/run/job/key',\n",
      "        'job-cancel': 'PUT:/requests/cancel/job/key',\n",
      "        'forwarding-submit': 'POST:/requests/submit/forwarding',\n",
      "        'forwarding-cancel': 'PUT:/requests/cancel/type/user/key',\n",
      "        'task-request': 'PUT:/tasks/request/signature',\n",
      "        'task-result': 'GET:/tasks/result/id',\n",
      "        'job-artifact': 'GET:/artifacts/job/type/name',\n",
      "        'forwarding-artifact': 'GET:/artifacts/forwarding/type/user/key'\n",
      "    }\n",
      "\n",
      "    route = None\n",
      "    if route_name in routes:\n",
      "        i = 0\n",
      "        route = routes[route_name].split('/')\n",
      "        for name in route:\n",
      "            if name in path_replacers:\n",
      "                replacer = path_replacers[name]\n",
      "                if 0 < len(replacer):\n",
      "                    route[i] = replacer\n",
      "            i = i + 1\n",
      "\n",
      "        if not len(path_names) == 0:\n",
      "            route.extend(path_names)\n",
      "\n",
      "        if not len(route_type) == 0:\n",
      "            route[0] = route_type + ':'\n",
      "        \n",
      "        route = '/'.join(route)\n",
      "    print('Used route: ' + str(route))\n",
      "    return route\n",
      "# Created and works\n",
      "def get_response(\n",
      "    route_type: str,\n",
      "    route_url: str,\n",
      "    route_input: any\n",
      ") -> any:\n",
      "    route_response = None\n",
      "    if route_type == 'POST':\n",
      "        route_response = requests.post(\n",
      "            url = route_url,\n",
      "            json = route_input\n",
      "        )\n",
      "    if route_type == 'PUT':\n",
      "        route_response = requests.put(\n",
      "            url = route_url\n",
      "        )\n",
      "    if route_type == 'GET':\n",
      "        route_response = requests.get(\n",
      "            url = route_url\n",
      "        )\n",
      "    return route_response\n",
      "# Created and works\n",
      "def set_full_url(\n",
      "    address: str,\n",
      "    port: str,\n",
      "    used_route: str\n",
      ") -> str:\n",
      "    url_prefix = 'http://' + address + ':' + port\n",
      "    route_split = used_route.split(':')\n",
      "    url_type = route_split[0]\n",
      "    used_path = route_split[1]\n",
      "    full_url = url_prefix + used_path\n",
      "    return url_type, full_url\n",
      "# Created and works\n",
      "def request_route(\n",
      "    address: str,\n",
      "    port: str,\n",
      "    route_type: str,\n",
      "    route_name: str,\n",
      "    path_replacers: any,\n",
      "    path_names: any,\n",
      "    route_input: any,\n",
      "    timeout: any\n",
      ") -> any:\n",
      "    used_route = set_route(\n",
      "        route_type = route_type,\n",
      "        route_name = route_name,\n",
      "        path_replacers = path_replacers,\n",
      "        path_names = path_names\n",
      "    )\n",
      "\n",
      "    url_type, full_url = set_full_url(\n",
      "        address = address,\n",
      "        port = port,\n",
      "        used_route = used_route\n",
      "    )\n",
      "\n",
      "    route_response = get_response(\n",
      "        route_type = url_type,\n",
      "        route_url = full_url,\n",
      "        route_input = route_input\n",
      "    )\n",
      "\n",
      "    route_status_code = None\n",
      "    route_returned_text = {}\n",
      "    if not route_response is None:\n",
      "        route_status_code = route_response.status_code\n",
      "        if route_status_code == 200:\n",
      "            route_text = json.loads(route_response.text)\n",
      "\n",
      "            if 'id' in route_text: \n",
      "                task_result_route = set_route(\n",
      "                    route_type = '',\n",
      "                    route_name = 'task-result',\n",
      "                    path_replacers = {\n",
      "                        'id': route_text['id']\n",
      "                    },\n",
      "                    path_names = []\n",
      "                )\n",
      "\n",
      "                task_url_type, task_full_url = set_full_url(\n",
      "                    address = address,\n",
      "                    port = port,\n",
      "                    used_route = task_result_route\n",
      "                )\n",
      "\n",
      "                start = time.time()\n",
      "                while time.time() - start <= timeout:\n",
      "                    task_route_response = get_response(\n",
      "                        route_type = task_url_type,\n",
      "                        route_url = task_full_url,\n",
      "                        route_input = {}\n",
      "                    )\n",
      "                    \n",
      "                    task_status_code = route_response.status_code\n",
      "                        \n",
      "                    if task_status_code == 200:\n",
      "                        #print(task_route_response.text)\n",
      "                        task_text = json.loads(task_route_response.text)\n",
      "                        if task_text['status'] == 'FAILED':\n",
      "                            break\n",
      "                        \n",
      "                        if task_text['status'] == 'SUCCESS':\n",
      "                            #print(task_text)\n",
      "                            route_returned_text = task_text['result']\n",
      "                            break\n",
      "                    else:\n",
      "                        break\n",
      "            else:\n",
      "                route_returned_text = route_text\n",
      "    return route_status_code, route_returned_text\n",
      "storage_names\n",
      "## Start Submitter\n",
      "deployment_folder = '/home/()/Project/cloud-hpc-oss-mlops-platform/applications/article/submitter/deployment/production'\n",
      "stack_file = deployment_folder + '/' + 'stack.yaml'\n",
      "scheduler_file = deployment_folder + '/' + 'scheduler.yaml'\n",
      "submitter_file_paths = [\n",
      "    stack_file,\n",
      "    scheduler_file\n",
      "]\n",
      "submitter_address = '127.0.0.1'\n",
      "submitter_port = '6600'\n",
      "secrets_path = '/run/secrets/secret-metadata'\n",
      "submitter_configuration = {\n",
      "    'ice-id': 's0-c0-u1',\n",
      "    'user': 'user@example.com',\n",
      "    'enviroments': {\n",
      "        'secrets-path': secrets_path,\n",
      "        'cloud': {\n",
      "            'platforms': [\n",
      "                'cpouta'\n",
      "            ]\n",
      "        },\n",
      "        'storage': {\n",
      "            'platforms': [\n",
      "                'allas'\n",
      "            ],\n",
      "            'object-store': {\n",
      "                'used-client': storage_parameters['used-client'],\n",
      "                'pre-auth-url': storage_parameters['pre-auth-url'],\n",
      "                'pre-auth-token': storage_parameters['pre-auth-token'],\n",
      "                'user-domain-name': storage_parameters['user-domain-name'],\n",
      "                'project-domain-name': storage_parameters['project-domain-name'],\n",
      "                'project-name': storage_parameters['project-name'],\n",
      "                'auth-version': storage_parameters['auth-version'],\n",
      "                'bucket-prefix': storage_parameters['bucket-prefix'],\n",
      "            }\n",
      "        },\n",
      "        'hpc': {\n",
      "            'platforms': [\n",
      "                'mahti'\n",
      "            ]\n",
      "        },\n",
      "        'integration': {\n",
      "            'platforms': [\n",
      "                'cpouta-mahti'\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "start_compose(\n",
      "    file_path = submitter_file_paths[0]\n",
      ")\n",
      "You can check the submitter frontend and monitor with localhost:6600 and localhost:6601\n",
      "import time\n",
      "import json\n",
      "route_code, route_text = request_route(\n",
      "    address = '127.0.0.1',\n",
      "    port = '6600',\n",
      "    route_type = '',\n",
      "    route_name = 'setup',\n",
      "    path_replacers = {},\n",
      "    path_names = [],\n",
      "    route_input = submitter_configuration,\n",
      "    timeout = 240\n",
      ")\n",
      "print(route_code)\n",
      "print(route_text)\n",
      "start_compose(\n",
      "    file_path = submitter_file_paths[1]\n",
      ")\n",
      "## Local Forwards\n",
      "\n",
      "In order to use and monitor the pipeline progress, open following local forwards:\n",
      "\n",
      "```\n",
      "kubeflow\n",
      "ssh -L 8080:localhost:8080 cpouta\n",
      "kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n",
      "localhost:8080\n",
      "\n",
      "forwarder frontend\n",
      "\n",
      "ssh -L 6500:localhost:6500 cpouta\n",
      "kubectl port-forward svc/fastapi-service 6500:6500 -n forwarder\n",
      "localhost:6500\n",
      "\n",
      "forwarder monitor\n",
      "\n",
      "ssh -L 6501:localhost:6501 cpouta\n",
      "kubectl port-forward svc/flower-service 6501:6501 -n forwarder\n",
      "localhost:6501\n",
      "\n",
      "ray dashboard\n",
      "ssh -L 127.0.0.1:8280:192.168.1.13:8280 cpouta\n",
      "```\n",
      "#### Submit Pipeline\n",
      "import time as t\n",
      "# Refactored\n",
      "def wait_kubeflow_run(\n",
      "    kfp_client: any,\n",
      "    timeout: int,\n",
      "    run_id: str\n",
      "):\n",
      "    start = t.time()\n",
      "    run_status = None\n",
      "    print('Checking kubeflow run: ' + str(run_id))\n",
      "    while t.time() - start <= timeout:\n",
      "        run_details = kfp_client.get_run(\n",
      "            run_id = run_id\n",
      "        )\n",
      "        run_status = run_details.run.status\n",
      "        print('Run status: ' + str(run_status))\n",
      "        if run_status == 'Failed':\n",
      "            run_status = False\n",
      "            break\n",
      "        if run_status == 'Succeeded':\n",
      "            run_status = True\n",
      "            break\n",
      "        if run_status == 'Error':\n",
      "            run_status = True\n",
      "            break\n",
      "        t.sleep(10)\n",
      "    return run_status\n",
      "# Created\n",
      "def manage_kubeflow_run(\n",
      "    submitter_file_paths: any,\n",
      "    submitter_address: str,\n",
      "    submitter_port: str,\n",
      "    submitter_configuration: str,\n",
      "    storage_client: any,\n",
      "    storage_name: str,\n",
      "    kfp_client: any,\n",
      "    run_name: str,\n",
      "    experiment_name: str,\n",
      "    pipeline_arguments: any,\n",
      "    timeout: int\n",
      "):\n",
      "    time_start = t.time()\n",
      "    \n",
      "    print('Submitting pipeline')\n",
      "    run_details = kfp_client.create_run_from_pipeline_func(\n",
      "        pipeline_func = pipeline,\n",
      "        run_name = run_name,\n",
      "        experiment_name = experiment_name,\n",
      "        arguments = pipeline_arguments,\n",
      "        mode = kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
      "        enable_caching = True,\n",
      "        namespace = \"kubeflow-user-example-com\"\n",
      "    )\n",
      "\n",
      "    run_status = wait_kubeflow_run(\n",
      "        kfp_client = kfp_client,\n",
      "        timeout = timeout,\n",
      "        run_id = run_details.run_id\n",
      "    )\n",
      "\n",
      "    print('Run status: ' + str(run_status))\n",
      "    \n",
      "    time_end = t.time()\n",
      "\n",
      "    gather_time(\n",
      "        storage_client = storage_client,\n",
      "        storage_name = storage_name,\n",
      "        time_group = 'components',\n",
      "        time_name = 'cloud-hpc-pipeline',\n",
      "        start_time = time_start,\n",
      "        end_time = time_end\n",
      "    )\n",
      "\n",
      "    print('Kubeflow pipeline complete')\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")\n",
      "\n",
      "import kfp\n",
      "import kfp.dsl as dsl\n",
      "from kfp.aws import use_aws_secret\n",
      "from kfp.v2.dsl import (\n",
      "    component,\n",
      "    Input,\n",
      "    Output,\n",
      "    Dataset,\n",
      "    Metrics,\n",
      "    Artifact,\n",
      "    Model\n",
      ")\n",
      "\n",
      "KUBEFLOW_ENDPOINT = 'http://localhost:8080'\n",
      "KUBEFLOW_USERNAME = 'user@example.com'\n",
      "KUBEFLOW_PASSWORD = '12341234'\n",
      "\n",
      "auth_session = get_istio_auth_session(\n",
      "    url=KUBEFLOW_ENDPOINT,\n",
      "    username=KUBEFLOW_USERNAME,\n",
      "    password=KUBEFLOW_PASSWORD\n",
      ")\n",
      "\n",
      "kfp_client = kfp.Client(host=f\"{KUBEFLOW_ENDPOINT}/pipeline\", cookies=auth_session[\"session_cookie\"])\n",
      "manage_kubeflow_run(\n",
      "    submitter_file_paths = submitter_file_paths,\n",
      "    submitter_address = submitter_address,\n",
      "    submitter_port = submitter_port,\n",
      "    submitter_configuration = submitter_configuration,\n",
      "    storage_client = storage_client,\n",
      "    storage_name = storage_names[-2],\n",
      "    kfp_client = kfp_client,\n",
      "    run_name = \"cloud-hpc-fmnist-exp-run\",\n",
      "    experiment_name = \"cloud-hpc-fmnist-experiment\",\n",
      "    pipeline_arguments = pipeline_arguments,\n",
      "    timeout = 5000\n",
      ")\n",
      "## Stop Submitter\n",
      "stop_compose(\n",
      "    file_path = submitter_file_paths[1]\n",
      ")\n",
      "stop_compose(\n",
      "    file_path = submitter_file_paths[0]\n",
      ")\n",
      "## Removing completed or error kubeflow pods\n",
      "\n",
      "If your kubeflow pipelines fail due to not having enough memory, then run these\n",
      "to remove kubeflow pods:\n",
      "\n",
      "```\n",
      "kubectl delete pod -n kubeflow-user-example-com --field-selector=status.phase=Failed\n",
      "kubectl delete pod -n kubeflow-user-example-com --field-selector=status.phase=Succeeded\n",
      "```\n",
      "## Infrastructure\n",
      "Packages:\n",
      "- Ray version is:2.9.3\n",
      "- Swiftclient version is:4.4.0\n",
      "- PyTorch version is:2.2.1+cu121\n",
      "- Torchmetrics version is:1.3.1\n",
      "\n",
      "Hardware:\n",
      "\n",
      "- OSS:\n",
      "    - Resource request:\n",
      "        ```\n",
      "        Allocated resources:\n",
      "          (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "          Resource           Requests           Limits\n",
      "          --------           --------           ------\n",
      "          cpu                6570m (82%)        61800m (772%)\n",
      "          memory             11732346112 (35%)  36710Mi (117%)\n",
      "          ephemeral-storage  0 (0%)             0 (0%)\n",
      "          hugepages-1Gi      0 (0%)             0 (0%)\n",
      "          hugepages-2Mi      0 (0%)             0 (0%)\n",
      "        ```\n",
      "- CPouta:\n",
      "    - Image = Ubuntu 22.04\n",
      "    - Flavor = standard.xxlarge\n",
      "        - CPU Cores = 8\n",
      "        - RAM = 31 GiB\n",
      "        - DISK = 80 GB\n",
      "        - Memory/core = 3.8 GiB\n",
      "        - Redundancy = P,R,N\n",
      "        - BU/h = 8\n",
      "    - Floating IP\n",
      "        - BU/h = 0.2\n",
      "    - Security groups:\n",
      "        - Laptop SSH\n",
      "        - Mahti-node-1 SSH\n",
      "        - Mahti-node-2 SSH \n",
      "- Allas (format is pickle):\n",
      "    - Integration-forwarder-s0-c0-u1 = 270.15 KB\n",
      "    - integration-pipeline-s0-c0-u1-user-example-com = 53.47 MB\n",
      "    - integration-submitter-s0-c0-u1-user-example-com = 362.32 KB\n",
      "- Mahti:\n",
      "    - Type = SLURM Cluster Node\n",
      "    - Partition = Medium\n",
      "    - Nodes = 2\n",
      "    - Node CPU = AMD Zen 2 Rome 7H12 x 64 core\n",
      "    - Node RAM = 10GB\n",
      "    - OS = Linux\n",
      "## Model Analysis\n",
      "## MLflow\n",
      "\n",
      "To use the MLflow analysis tools, run the following\n",
      "```\n",
      "ssh -L 5000:localhost:5000 cpouta\n",
      "kubectl -n mlflow port-forward svc/mlflow 5000:5000\n",
      "```\n",
      "\n",
      "and go to localhost:5000\n",
      "\n",
      "## Grafana\n",
      "\n",
      "To use Grafana dashboards, run the following\n",
      "\n",
      "```\n",
      "ssh -L 5050:localhost:5050 cpouta\n",
      "kubectl port-forward svc/grafana 5050:3000 --namespace monitoring\n",
      "```\n",
      "\n",
      "login with admin-admin and import the provided dashboard\n",
      "model_metrics = get_object(\n",
      "    storage_client = storage_client,\n",
      "    bucket_name = storage_names[-2],\n",
      "    object_name = 'root',\n",
      "    path_replacers = {\n",
      "        'name': 'ARTIFACTS'\n",
      "    },\n",
      "    path_names = [\n",
      "        'FMNIST',\n",
      "        'metrics'\n",
      "    ]\n",
      ")\n",
      "model_metrics_data = model_metrics['data']\n",
      "with open('artifacts/cloud-hpc-metrics-1.json', 'w') as f:\n",
      "    json.dump(model_metrics_data, f, indent = 4)\n",
      "model_metrics['data']\n",
      "model_parameters = get_object(\n",
      "    storage_client = storage_client,\n",
      "    bucket_name = storage_names[-2],\n",
      "    object_name = 'root',\n",
      "    path_replacers = {\n",
      "        'name': 'ARTIFACTS'\n",
      "    },\n",
      "    path_names = [\n",
      "        'FMNIST',\n",
      "        'parameters'\n",
      "    ]\n",
      ")\n",
      "import torch\n",
      "model_parameters_data = model_parameters['data']\n",
      "\n",
      "model = model_parameters_data['model']\n",
      "optimizer = model_parameters_data['optimizer']\n",
      "\n",
      "torch.save(model, 'artifacts/cloud-hpc-model-3.pth')\n",
      "torch.save(optimizer, 'artifacts/cloud-hpc-optimizer-3.pth')\n",
      "model_predictions = get_object(\n",
      "    storage_client = storage_client,\n",
      "    bucket_name = storage_names[-2],\n",
      "    object_name = 'root',\n",
      "    path_replacers = {\n",
      "        'name': 'ARTIFACTS'\n",
      "    },\n",
      "    path_names = [\n",
      "        'FMNIST',\n",
      "        'predictions'\n",
      "    ]\n",
      ")\n",
      "import numpy as np\n",
      "model_predictions_data = model_predictions['data']\n",
      "np.save('artifacts/cloud-hpc-predictions-1.npy', model_predictions_data)\n",
      "import torch\n",
      "from torchvision import datasets\n",
      "import torchvision.transforms as T\n",
      "test_transform = T.Compose([\n",
      "    T.ToTensor(),\n",
      "    T.Normalize((0.5,), (0.5,))\n",
      "])\n",
      "\n",
      "test_data = datasets.FashionMNIST(\n",
      "    root = './data', \n",
      "    train = False, \n",
      "    download = True, \n",
      "    transform = test_transform\n",
      ")\n",
      "\n",
      "test_loader = torch.utils.data.DataLoader(\n",
      "    test_data, \n",
      "    batch_size = 4, \n",
      "    shuffle = False\n",
      ")\n",
      "first_batch = next(iter(test_loader))\n",
      "inputs, labels = first_batch\n",
      "sample_inputs = inputs.numpy().tolist()\n",
      "sample_labels = labels.numpy().tolist()\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class CNNClassifier(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
      "        self.pool = nn.MaxPool2d(2, 2)\n",
      "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
      "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
      "        self.fc2 = nn.Linear(120, 84)\n",
      "        self.fc3 = nn.Linear(84, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))\n",
      "        x = self.pool(F.relu(self.conv2(x)))\n",
      "        x = x.view(-1, 16 * 4 * 4)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = F.relu(self.fc2(x))\n",
      "        x = self.fc3(x)\n",
      "        return x\n",
      "\n",
      "def inference(\n",
      "    model_state_path: any,\n",
      "    inputs: any\n",
      ") -> any:\n",
      "    model = CNNClassifier()\n",
      "    model.load_state_dict(torch.load(model_state_path))\n",
      "    predictions = []\n",
      "    inputs = torch.tensor(np.array(inputs, dtype=np.float32))\n",
      "    with torch.no_grad():\n",
      "        model.eval()\n",
      "        outputs = model(inputs)\n",
      "        preds = torch.max(outputs, 1)[1]\n",
      "        predictions.extend(preds.tolist())\n",
      "    return predictions\n",
      "created_preds = inference(\n",
      "    model_state_path = 'artifacts/cloud-hpc-model-1.pth',\n",
      "    inputs = sample_inputs\n",
      ")\n",
      "created_preds\n",
      "sample_labels\n",
      "## TIMES\n",
      "training_time = get_object(\n",
      "    storage_client = storage_client,\n",
      "    bucket_name = storage_names[-2],\n",
      "    object_name = 'root',\n",
      "    path_replacers = {\n",
      "        'name': 'TIMES'\n",
      "    },\n",
      "    path_names = [\n",
      "        'ray-jobs'\n",
      "    ]\n",
      ")\n",
      "training_time_data = training_time['data']\n",
      "with open('times/training-1.json', 'w') as f:\n",
      "    json.dump(training_time_data, f, indent = 4)\n",
      "component_time = get_object(\n",
      "    storage_client = storage_client,\n",
      "    bucket_name = storage_names[-2],\n",
      "    object_name = 'root',\n",
      "    path_replacers = {\n",
      "        'name': 'TIMES'\n",
      "    },\n",
      "    path_names = [\n",
      "        'components'\n",
      "    ]\n",
      ")\n",
      "component_time = component_time['data']\n",
      "with open('times/components-1.json', 'w') as f:\n",
      "    json.dump(component_time, f, indent = 4)\n",
      "## LOGS\n",
      "response = requests.get(\n",
      "    url = 'http://127.0.0.1:6500/general/logs/frontend'\n",
      ")\n",
      "logs = json.loads(response.text)['logs']\n",
      "with open('logs/forwarder-frontend-1.txt', 'w') as f:\n",
      "    for line in logs:\n",
      "        f.write(line + '\\n')\n",
      "response = requests.get(\n",
      "    url = 'http://127.0.0.1:6500/general/logs/backend'\n",
      ")\n",
      "logs = json.loads(response.text)['logs']\n",
      "with open('logs/forwarder-backend-1.txt', 'w') as f:\n",
      "    for line in logs:\n",
      "        f.write(line + '\\n')\n",
      "response = requests.get(\n",
      "    url = 'http://127.0.0.1:6600/general/logs/frontend'\n",
      ")\n",
      "logs = json.loads(response.text)['logs']\n",
      "with open('logs/submitter-frontend-1.txt', 'w') as f:\n",
      "    for line in logs:\n",
      "        f.write(line + '\\n')\n",
      "response = requests.get(\n",
      "    url = 'http://127.0.0.1:6600/general/logs/backend'\n",
      ")\n",
      "logs = json.loads(response.text)['logs']\n",
      "with open('logs/submitter-backend-1.txt', 'w') as f:\n",
      "    for line in logs:\n",
      "        f.write(line + '\\n')\n",
      "## SACCT\n",
      "sacct_object = get_object(\n",
      "    storage_client = storage_client,\n",
      "    bucket_name = storage_names[1],\n",
      "    object_name = 'root',\n",
      "    path_replacers = {\n",
      "        'name': 'ARTIFACTS'\n",
      "    },\n",
      "    path_names = [\n",
      "        'SACCT',\n",
      "        '23'\n",
      "    ]\n",
      ")\n",
      "sacct_data = sacct_object['data']\n",
      "with open('artifacts/sacct-1.json', 'w') as f:\n",
      "    json.dump(sacct_data, f, indent = 4)\n",
      "## SEFF\n",
      "seff_object = get_object(\n",
      "    storage_client = storage_client,\n",
      "    bucket_name = storage_names[1],\n",
      "    object_name = 'root',\n",
      "    path_replacers = {\n",
      "        'name': 'ARTIFACTS'\n",
      "    },\n",
      "    path_names = [\n",
      "        'SEFF',\n",
      "        '23'\n",
      "    ]\n",
      ")\n",
      "seff_data = seff_object['data']\n",
      "with open('artifacts/seff-1.json', 'w') as f:\n",
      "    json.dump(seff_data , f, indent = 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notebook_cells = notebook_node.cells\n",
    "for cell in notebook_cells:\n",
    "    if cell.cell_type == 'markdown':\n",
    "        markdown_content = ''.join(cell.source)\n",
    "        print(markdown_content)\n",
    "    if cell.cell_type == 'code':\n",
    "        code_content = ''.join(cell.source)\n",
    "        print(code_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ad71b-3c7e-489f-8b7d-f66055428495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
