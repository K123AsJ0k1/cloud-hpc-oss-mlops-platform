{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76734520-9ec6-4cb9-bed5-477006377137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df5dcf8-2d82-40da-8914-9ce8aef3b84f",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7f7b3f-6c35-47f0-8ba4-f0853ab66d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import Config,RepositoryEnv\n",
    "env_path = '/home/sfniila/.ssh/.env'\n",
    "env_config = Config(RepositoryEnv(env_path))\n",
    "github_token = env_config.get('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4314a1-75b3-4a21-93ea-da45671295c9",
   "metadata": {},
   "source": [
    "## PyGitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b07a4fb-3e77-4418-900a-00355b551a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "def pygithub_get_repo_paths(\n",
    "    token: str,\n",
    "    owner: str, \n",
    "    name: str\n",
    ") -> any:\n",
    "    g = Github(token)\n",
    "    repo = g.get_repo(f\"{owner}/{name}\")\n",
    "    contents = repo.get_contents(\"\")\n",
    "    paths = []\n",
    "    while len(contents) > 0:\n",
    "      file_content = contents.pop(0)\n",
    "      if file_content.type == 'dir':\n",
    "        contents.extend(repo.get_contents(file_content.path))\n",
    "      else:\n",
    "        paths.append(file_content.path)\n",
    "    g.close()\n",
    "    return paths\n",
    "\n",
    "def pygithub_get_path_content(\n",
    "    token: str,\n",
    "    owner: str, \n",
    "    name: str, \n",
    "    path: str\n",
    ") -> any:\n",
    "    g = Github(token)\n",
    "    repo = g.get_repo(f\"{owner}/{name}\")\n",
    "    file_content = repo.get_contents(path)\n",
    "    content = file_content.decoded_content.decode('utf-8')\n",
    "    # fails on some yaml files\n",
    "    # deployment/kubeflow/manifests/contrib/ray/kuberay-operator/base/resources.yaml\n",
    "    # unsupported encoding: none\n",
    "    g.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d75007-8269-450f-a54b-c485a324949e",
   "metadata": {},
   "source": [
    "## Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b2f0cd3-106c-4b5d-9926-40dd191ee231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_github_repo_documents(\n",
    "    github_token: str,\n",
    "    repo_owner: str,\n",
    "    repo_name: str,\n",
    "    relevant_files: any,\n",
    "    store: bool\n",
    ") -> any:\n",
    "    storage_path = os.getcwd() + '/' + repo_owner + '-' + repo_name + '.txt' \n",
    "\n",
    "    repo_paths = []\n",
    "    if not os.path.exists(storage_path):\n",
    "        print('Getting github paths')\n",
    "        repo_paths = pygithub_get_repo_paths(\n",
    "            token = github_token,\n",
    "            owner = repo_owner, \n",
    "            name = repo_name\n",
    "        )\n",
    "        \n",
    "        if store:\n",
    "            print('Storing paths')\n",
    "            path_amount = len(repo_paths)\n",
    "            i = 0\n",
    "            with open(storage_path, 'w') as file:\n",
    "                for line in repo_paths:\n",
    "                    row = line \n",
    "                    if i < path_amount:\n",
    "                        row += '\\n'\n",
    "                    file.write(row)\n",
    "                    i += 1\n",
    "            print('Paths stored')\n",
    "    else:\n",
    "        print('Getting stored paths')\n",
    "        with open(storage_path, 'r') as file:\n",
    "            repo_paths = file.readlines()\n",
    "    print('Paths fetched')\n",
    "                \n",
    "    print('Filtering paths')\n",
    "    relevant_paths = []\n",
    "    for path in repo_paths:\n",
    "        path_split = path.split('/')\n",
    "        file_end = path_split[-1].split('.')[-1].rstrip()\n",
    "        if file_end in relevant_files:\n",
    "            relevant_paths.append(path.rstrip())\n",
    "    print('Paths filtered')\n",
    "\n",
    "    formatted_paths = {\n",
    "        'paths': relevant_paths\n",
    "    }\n",
    "\n",
    "    # consider how to store this to allas\n",
    "    return formatted_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee717787-4d21-4082-ad61-5b1973926cc9",
   "metadata": {},
   "source": [
    "Files of intrest\n",
    "- md = important to usesr\n",
    "- txt = isn't important to users\n",
    "- sh = isn't important to users\n",
    "- yaml = important to developers\n",
    "- py = important to users and developers\n",
    "- ipynb = important to users and developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9ccfa42-95dc-4c8a-9784-fa703937d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stored paths\n",
      "Paths fetched\n",
      "Filtering paths\n",
      "Paths filtered\n"
     ]
    }
   ],
   "source": [
    "repository_paths = get_github_repo_documents(\n",
    "    github_token = github_token,\n",
    "    repo_owner = 'K123AsJ0k1',\n",
    "    repo_name = 'cloud-hpc-oss-mlops-platform',\n",
    "    relevant_files = [\n",
    "        'md',\n",
    "        'yaml',\n",
    "        'py',\n",
    "        'ipynb'\n",
    "    ],\n",
    "    store = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70f6e8-ed91-45c9-afa5-cf598998ac17",
   "metadata": {},
   "source": [
    "# Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfc42e-02e2-4d94-b7f9-9782228cc44d",
   "metadata": {},
   "source": [
    "## Mongo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "56e4c444-5608-4dde-bf09-c0364a1b8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient as mc\n",
    "\n",
    "def mongo_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return isinstance(storage_client, mc.Connection)\n",
    "\n",
    "def mongo_setup_client(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    address: str,\n",
    "    port: str\n",
    ") -> any:\n",
    "    connection_prefix = 'mongodb://(username):(password)@(address):(port)/'\n",
    "    connection_address = connection_prefix.replace('(username)', username)\n",
    "    connection_address = connection_address.replace('(password)', password)\n",
    "    connection_address = connection_address.replace('(address)', address)\n",
    "    connection_address = connection_address.replace('(port)', port)\n",
    "    mongo_client = mc(\n",
    "        host = connection_address\n",
    "    )\n",
    "    return mongo_client\n",
    "\n",
    "def mongo_get_database(\n",
    "    mongo_client: any,\n",
    "    database_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        return database\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_check_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database_exists = database_name in mongo_client.list_database_names()\n",
    "        return database_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_list_databases(\n",
    "    mongo_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        databases = mongo_client.list_database_names()\n",
    "        return databases\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        mongo_client.drop_database(database_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_get_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collection = database[collection_name]\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def mongo_check_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        collection_exists = collection_name in database.list_collection_names()\n",
    "        return collection_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_update_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_many(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_list_collections(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collections = database.list_collection_names()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try: \n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        database.drop_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_create_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    document: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.insert_one(document)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_get_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    "):\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        document = collection.find_one(filter_query)\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None \n",
    "\n",
    "def mongo_list_documents(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any,\n",
    "    sorting_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        documents = list(collection.find(filter_query).sort(sorting_query))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_update_document(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_one(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_remove_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    ") -> bool:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.delete_one(filter_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116c18c-53f1-4222-986f-6e3fa00e65b2",
   "metadata": {},
   "source": [
    "## Markdown documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3ec230be-1fbe-4b25-90da-078055b830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def create_markdown_documents(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    \n",
    "    documents = []\n",
    "    document = ''\n",
    "    index = 1\n",
    "    for element in soup.descendants:\n",
    "        if element.name in ['h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            text = element.get_text(strip = True)\n",
    "            if not document == '':\n",
    "                document = document.replace('\\n', '')\n",
    "                if not len(document.split()) == 1:\n",
    "                    documents.append({\n",
    "                        'index': index,\n",
    "                        'sub-index': 0,\n",
    "                        'type': 'markdown',\n",
    "                        'data': document\n",
    "                    })\n",
    "                    index += 1\n",
    "                document = ''\n",
    "            document += text\n",
    "        elif element.name == 'p':\n",
    "            text = element.get_text(strip = True)\n",
    "            text = re.sub(code_block_pattern, '', text)\n",
    "            text = text.rstrip('\\n')\n",
    "            text = text.replace('\\nsh', '')\n",
    "            text = text.replace('\\nbash', '')\n",
    "            document += ' ' + text\n",
    "        elif element.name in ['ul', 'ol']:\n",
    "            text = ''\n",
    "            for li in element.find_all('li'):\n",
    "                item = li.get_text(strip=True)\n",
    "                if not '-' in item:\n",
    "                    text += '-' + item\n",
    "                    continue\n",
    "                text += item\n",
    "            document += ' ' + text\n",
    "            \n",
    "    documents.append({\n",
    "        'index': index,\n",
    "        'sub-index': 0,\n",
    "        'type': 'markdown',\n",
    "        'data': document\n",
    "    })\n",
    "    \n",
    "    formatted_documents = {\n",
    "        'text': documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d185d0f-0003-4958-a026-0b79c6c62333",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = repository_paths[7]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c4d08e0-6f5e-41f1-a465-e9829d6cb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_documents = create_markdown_documents(\n",
    "    markdown_text = example_data\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f93ac-63d8-4b9f-8467-e202571ce0aa",
   "metadata": {},
   "source": [
    "## YAML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b019e009-db94-47b4-9021-9ab915c437b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "def extract_yaml_values(\n",
    "    section: any,\n",
    "    path: str,\n",
    "    values: any\n",
    ") -> any:\n",
    "    for key, value in section.items():\n",
    "        if path == '':\n",
    "            current_path = key\n",
    "        else:\n",
    "            current_path = path + '/' + key\n",
    "        if isinstance(value, dict):\n",
    "            extract_yaml_values(\n",
    "                section = value,\n",
    "                path = current_path,\n",
    "                values = values\n",
    "            )\n",
    "        if isinstance(value, list):\n",
    "            number = 1\n",
    "            \n",
    "            for case in value:\n",
    "                base_path = current_path\n",
    "                if isinstance(case, dict):\n",
    "                   extract_yaml_values(\n",
    "                       section = case,\n",
    "                       path = current_path,\n",
    "                       values = values\n",
    "                   ) \n",
    "                   continue\n",
    "                base_path += '/' + str(number)\n",
    "                number += 1\n",
    "                values.append(base_path + '=' + str(case))\n",
    "        else:\n",
    "            if isinstance(value, dict):\n",
    "                continue\n",
    "            if isinstance(value, list):\n",
    "                continue\n",
    "            values.append(current_path + '=' + str(value))\n",
    "            \n",
    "    return values\n",
    "\n",
    "def create_yaml_documents(\n",
    "    yaml_text: any\n",
    ") -> any:\n",
    "    # has problems with some yaml files\n",
    "    # unsupported encoding: none\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
    "    # 'list' object has no attribute 'items'\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
    "    # 'NoneType' object has no attribute 'items'\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/base/metadata/overlays/db/kustomization.yaml\n",
    "    #   could not determine a constructor for the tag 'tag:yaml.org,2002:value'\n",
    "    # in \"<unicode string>\", line 41, column 18:\n",
    "    #      delimiter: =\n",
    "    yaml_data = list(yaml.safe_load_all(yaml_text))\n",
    "\n",
    "    documents = []\n",
    "    index = 1\n",
    "    for data in yaml_data:\n",
    "        yaml_values = extract_yaml_values(\n",
    "            section = data,\n",
    "            path = '',\n",
    "            values = []\n",
    "        )\n",
    "\n",
    "        previous_root = ''\n",
    "        document = ''\n",
    "        sub_index = 1\n",
    "        for value in yaml_values:\n",
    "            equal_split = value.split('=')\n",
    "            path_split = equal_split[0].split('/')\n",
    "            root = path_split[0]\n",
    "            if not root == previous_root:\n",
    "                if 0 < len(document):\n",
    "                    documents.append({\n",
    "                        'index': index,\n",
    "                        'sub-index': sub_index,\n",
    "                        'type': 'yaml',\n",
    "                        'data': document\n",
    "                    })\n",
    "                    sub_index += 1\n",
    "                    \n",
    "                previous_root = root\n",
    "                document = value\n",
    "            else:\n",
    "                document += value\n",
    "                \n",
    "        documents.append({\n",
    "            'index': index,\n",
    "            'sub-index': sub_index,\n",
    "            'type': 'yaml',\n",
    "            'data': document\n",
    "        })\n",
    "        index += 1\n",
    "\n",
    "    formatted_documents = {\n",
    "        'text': documents\n",
    "    }\n",
    "            \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "239ffb67-18ec-4a3e-a8ca-57fa1364ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_yaml = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'deployment/forwarder/celery/celery-deployment.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48880c03-8a81-46a5-b207-f0c583e5e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_documents = create_yaml_documents(\n",
    "    yaml_text = example_yaml\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10990a57-ea53-4e0e-9c53-d620811f5b95",
   "metadata": {},
   "source": [
    "## TreeSitter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "43c68780-ac71-4ede-84fb-713a7cf56da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "def tree_extract_imports(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(tree_extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def tree_extract_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(tree_extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "\n",
    "def tree_extract_code_and_dependencies(\n",
    "    node: any,\n",
    "    code_text: str\n",
    ") -> any:\n",
    "    codes = []\n",
    "    if not node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name')\n",
    "        if name is None:\n",
    "            code = code_text[start_byte:end_byte].decode('utf8')\n",
    "            if not 'def' in code:\n",
    "                dependencies = tree_extract_dependencies(node, code_text)\n",
    "                codes.append({\n",
    "                    'name': 'global',\n",
    "                    'code': code,\n",
    "                    'dependencies': dependencies\n",
    "                })\n",
    "    return codes\n",
    "\n",
    "def tree_extract_functions_and_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(tree_extract_functions_and_dependencies(child, code_text))\n",
    "    return functions\n",
    "\n",
    "def tree_get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def tree_get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions\n",
    "\n",
    "def tree_create_code_document(\n",
    "    code_imports: any,\n",
    "    code_functions: any,\n",
    "    function_item: any\n",
    ") -> any:\n",
    "    used_imports = tree_get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = tree_get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': function_item['name'],\n",
    "        'dependencies': function_item['dependencies'],\n",
    "        'code': function_item['code']\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "     \n",
    "def tree_format_code_document(\n",
    "    code_document: any\n",
    ") -> any:\n",
    "    formatted_document = ''\n",
    "    for doc_import in code_document['imports']:\n",
    "        formatted_document += doc_import + '\\n'\n",
    "\n",
    "    for doc_functions in code_document['functions']:\n",
    "        formatted_document += doc_functions + '\\n'\n",
    "\n",
    "    if 0 < len(code_document['dependencies']):\n",
    "        formatted_document += 'code dependencies\\n'\n",
    "\n",
    "        for doc_dependency in code_document['dependencies']:\n",
    "            formatted_document += doc_dependency + '\\n'\n",
    "\n",
    "    if code_document['name'] == 'global':\n",
    "        formatted_document += code_document['name'] + ' code\\n'\n",
    "    else:\n",
    "        formatted_document += 'function ' + code_document['name'] + ' code\\n'\n",
    "    \n",
    "    for line in code_document['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        doc_code = re.sub(r'#.*','', line)\n",
    "        if not bool(doc_code.strip()):\n",
    "            continue\n",
    "        formatted_document += doc_code + '\\n'    \n",
    "    return formatted_document\n",
    "\n",
    "def tree_create_python_code_and_function_documents(\n",
    "    code_document: any\n",
    "):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "   \n",
    "    tree = parser.parse(\n",
    "        bytes(\n",
    "            code_document,\n",
    "            \"utf8\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    code_imports = tree_extract_imports(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_global = tree_extract_code_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_functions = tree_extract_functions_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    initial_documents = []\n",
    "    for item in code_global:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    for item in code_functions:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    formatted_documents = []\n",
    "    seen_functions = []\n",
    "    for document in initial_documents:\n",
    "        if not document['name'] == 'global':\n",
    "            if document['name'] in seen_functions:\n",
    "                continue\n",
    "        \n",
    "        formatted_document = tree_format_code_document(\n",
    "            code_document = document\n",
    "        )\n",
    "\n",
    "        formatted_documents.append(formatted_document)\n",
    "        seen_functions.append(document['name'])\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46127d-3124-4243-a7fa-77dae3531c91",
   "metadata": {},
   "source": [
    "## Python documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "42502b34-833a-41a5-8eb8-d6b52bb98330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_python_documents(\n",
    "    python_text: any\n",
    "): \n",
    "    joined_code = ''.join(python_text)\n",
    "    block_code_documents = tree_create_python_code_and_function_documents(\n",
    "        code_document = joined_code\n",
    "    )\n",
    "\n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    code_doc_index = 0\n",
    "    for code_doc in block_code_documents:\n",
    "        row_split = code_doc.split('\\n')\n",
    "        for row in row_split:\n",
    "            if 'function' in row and 'code' in row:\n",
    "                # This causes problems with some documents\n",
    "                # list index out of range\n",
    "                function_name = row.split(' ')[1]\n",
    "                if not function_name in seen_function_names:\n",
    "                    seen_function_names.append(function_name)\n",
    "                else:\n",
    "                    del block_code_documents[code_doc_index]\n",
    "        code_doc_index += 1\n",
    "\n",
    "    if 0 < len(block_code_documents):\n",
    "        index = 1\n",
    "        for code_doc in block_code_documents:\n",
    "            code_documents.append({\n",
    "                'index': index,\n",
    "                'sub-index': 0,\n",
    "                'type': 'python',\n",
    "                'data': code_doc\n",
    "            })\n",
    "            index += 1\n",
    "        \n",
    "    formatted_documents = {\n",
    "        'code': code_documents\n",
    "    }\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7426485-07ce-46ea-8d2b-05b598f3c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_python = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'applications/article/submitter/backend/functions/platforms/flower.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e688f33c-7429-458b-8983-994918e8537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_documents = create_python_documents(\n",
    "    python_text = example_python\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0259e6-052b-4e49-b73b-bd95fd6d3afe",
   "metadata": {},
   "source": [
    "## Notebook documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "55ecc3b9-29af-42e9-acad-27f50bd8dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "\n",
    "def parse_jupyter_notebook_markdown_into_text(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    text = soup.get_text()\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    text = re.sub(code_block_pattern, '', text)\n",
    "    text = text.rstrip('\\n')\n",
    "    text = text.replace('\\nsh', '\\n')\n",
    "    text = text.replace('\\nbash', '\\n')\n",
    "    return text\n",
    "\n",
    "def extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_text: any\n",
    "): \n",
    "    notebook_documents = {\n",
    "        'markdown': [],\n",
    "        'code': []\n",
    "    }\n",
    "\n",
    "    notebook = nbformat.reads(notebook_text, as_version=2)\n",
    "    index = 1\n",
    "    for cell in notebook.worksheets[0].cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            notebook_documents['markdown'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "        if cell.cell_type == 'code':\n",
    "            notebook_documents['code'].append({\n",
    "                'id': index,\n",
    "                'data': cell.input\n",
    "            })\n",
    "            index += 1\n",
    "    \n",
    "    return notebook_documents\n",
    "\n",
    "def create_notebook_documents(\n",
    "    notebook_text: any\n",
    "):\n",
    "    notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "        notebook_text = notebook_text\n",
    "    )\n",
    "\n",
    "    markdown_documents = []\n",
    "    for block in notebook_documents['markdown']:\n",
    "        joined_text = ''.join(block['data'])\n",
    "        markdown_text = parse_jupyter_notebook_markdown_into_text(\n",
    "            markdown_text = joined_text\n",
    "        )\n",
    "        markdown_documents.append({\n",
    "            'index': block['id'],\n",
    "            'sub-index': 0,\n",
    "            'type': 'markdown',\n",
    "            'data': markdown_text\n",
    "        })\n",
    "        \n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    for block in notebook_documents['code']:\n",
    "        joined_code = ''.join(block['data'])\n",
    "        block_code_documents = tree_create_python_code_and_function_documents(\n",
    "            code_document = joined_code\n",
    "        )\n",
    "\n",
    "        code_doc_index = 0\n",
    "        for code_doc in block_code_documents:\n",
    "            row_split = code_doc.split('\\n')\n",
    "            for row in row_split:\n",
    "                if 'function' in row and 'code' in row:\n",
    "                    # This causes problems with some documents\n",
    "                    # list index out of range\n",
    "                    function_name = row.split(' ')[1]\n",
    "                    if not function_name in seen_function_names:\n",
    "                        seen_function_names.append(function_name)\n",
    "                    else:\n",
    "                        del block_code_documents[code_doc_index]\n",
    "            code_doc_index += 1\n",
    "        \n",
    "        if 0 < len(block_code_documents):\n",
    "            sub_indexes = False\n",
    "            if 1 < len(block_code_documents):\n",
    "                sub_indexes = True\n",
    "            index = 1\n",
    "            for code_doc in block_code_documents:\n",
    "                if sub_indexes:\n",
    "                    code_documents.append({\n",
    "                        'index': block['id'],\n",
    "                        'sub-index': index, \n",
    "                        'type': 'python',\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                else:\n",
    "                    code_documents.append({ \n",
    "                        'index': block['id'],\n",
    "                        'sub-index': 0,\n",
    "                        'type': 'python',\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                index += 1\n",
    "            \n",
    "    formatted_documents = {\n",
    "        'text': markdown_documents,\n",
    "        'code': code_documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a060a79-f17c-45c6-b81d-ad36d0c0965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_notebook = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'tutorials/cloud_hpc/Cloud-HPC-FMNIST.ipynb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2fb19ea7-3ddc-4fce-99e9-6d37c21f60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_documents = create_notebook_documents(\n",
    "    notebook_text = example_notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c944d67-e224-49b5-a01f-d348deb398ae",
   "metadata": {},
   "source": [
    "## Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb12557-3abf-4426-adfd-d42a4706067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = mongo_setup_client(\n",
    "    username = 'mongo123',\n",
    "    password = 'mongo456',\n",
    "    address = '127.0.0.1',\n",
    "    port = '27017'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fe860a31-582b-4fa7-b8bf-73e2ca5919ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_repository_path_documents(\n",
    "    mongo_client: any,\n",
    "    github_token: any,\n",
    "    repository_owner: str,\n",
    "    repository_name: str,\n",
    "    repository_path: str,\n",
    "    database_name: str,\n",
    "    collection_name: str\n",
    "):\n",
    "    collection_exists = mongo_check_collection(\n",
    "        mongo_client = mongo_client, \n",
    "        database_name = database_name, \n",
    "        collection_name = collection_name\n",
    "    )\n",
    "\n",
    "    if collection_exists:\n",
    "        return False\n",
    "\n",
    "    target_content = ''\n",
    "    try:\n",
    "        target_content = pygithub_get_path_content(\n",
    "            token = github_token,\n",
    "            owner = repository_owner, \n",
    "            name = repository_name, \n",
    "            path = repository_path\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(repository_path)\n",
    "        print(e)\n",
    "\n",
    "    if target_content == '':\n",
    "        return False\n",
    "\n",
    "    path_split = repository_path.split('/')\n",
    "    target_type = path_split[-1].split('.')[-1]\n",
    "    \n",
    "    target_documents = {}\n",
    "    if target_type == 'md':\n",
    "        try:\n",
    "            target_documents = create_markdown_documents(\n",
    "                markdown_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'yaml':\n",
    "        try:\n",
    "            target_documents = create_yaml_documents(\n",
    "                yaml_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'py':\n",
    "        try:\n",
    "            target_documents = create_python_documents(\n",
    "                python_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'ipynb':\n",
    "        try:\n",
    "            target_documents = create_notebook_documents(\n",
    "                notebook_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if 0 < len(target_documents):\n",
    "        for doc_type, docs in target_documents.items():\n",
    "            for document in docs:\n",
    "                result = mongo_create_document(\n",
    "                    mongo_client = mongo_client,\n",
    "                    database_name = database_name,\n",
    "                    collection_name = collection_name,\n",
    "                    document = document\n",
    "                )\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_github_storage_prefix(\n",
    "    repository_owner: str,\n",
    "    repository_name: str\n",
    ") -> str:\n",
    "    return repository_owner + '|' + repository_name + '|'\n",
    "\n",
    "def store_github_repository_documents(\n",
    "    mongo_client: any,\n",
    "    github_token: str,\n",
    "    repository_owner: str,\n",
    "    repository_name: str,\n",
    "    repository_paths: any\n",
    ") -> any:\n",
    "    print('Storing paths')\n",
    "    paths = repository_paths['paths']\n",
    "    for path in paths:\n",
    "        path_split = path.split('/')\n",
    "        document_database_name = get_github_storage_prefix(repository_owner, repository_name) + path_split[-1].split('.')[-1]\n",
    "        \n",
    "        document_collection_name = ''\n",
    "        for word in path_split[:-1]:\n",
    "            document_collection_name += word[:2] + '|'\n",
    "        document_collection_name += path_split[-1].split('.')[0]\n",
    "\n",
    "        stored = store_repository_path_documents(\n",
    "            mongo_client = mongo_client,\n",
    "            github_token = github_token,\n",
    "            repository_owner = repository_owner,\n",
    "            repository_name = repository_name,\n",
    "            repository_path = path,\n",
    "            database_name = document_database_name,\n",
    "            collection_name = document_collection_name\n",
    "        )\n",
    "    print('Paths stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e25a1e1c-3177-41b7-95c7-e0c10f11ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing paths\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG-Development.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG-pipeline.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG_Preprocessing.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/documents.py\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/python_parsing.py\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/platforms/tree.py\n",
      "list index out of range\n",
      "deployment/kubeflow/manifests/contrib/ray/kuberay-operator/base/resources.yaml\n",
      "unsupported encoding: none\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/eventing-core.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/in-memory-channel.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/mt-channel-broker.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-serving/base/upstream/net-istio.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-serving/base/upstream/serving-core.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-openshift/patches/service-serving-cert.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-openshift/patches/webhook-inject-cabundle.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-with-kubeflow/patches/enable-ui-authz-checks.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-with-kubeflow/patches/ui-rbac.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/openshift-pipelines-custom-task/pipelineloop-controller-patch.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/openshift-pipelines-custom-task/pipelineloop-webhook-patch.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/contrib/kserve/models-web-app/overlays/kubeflow/patches/web-app-vsvc.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/base/metadata/overlays/db/kustomization.yaml\n",
      "could not determine a constructor for the tag 'tag:yaml.org,2002:value'\n",
      "  in \"<unicode string>\", line 41, column 18:\n",
      "          delimiter: =\n",
      "                     ^\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-dashboard/tekton-dashboard-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/pipeline/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/pipeline/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-dashboard/tekton-dashboard-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "Paths stored\n"
     ]
    }
   ],
   "source": [
    "store_github_repository_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    github_token = github_token,\n",
    "    repository_owner = 'K123AsJ0k1', \n",
    "    repository_name = 'cloud-hpc-oss-mlops-platform', \n",
    "    repository_paths = repository_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4b4bc4d1-9cd2-489c-8167-027e82ea42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "    \n",
    "def get_stored_documents(\n",
    "    mongo_client: any,\n",
    "    database_prefix: str\n",
    ") -> any:\n",
    "    storage_structure = {}\n",
    "    database_list = mongo_list_databases(\n",
    "        mongo_client = mongo_client\n",
    "    )\n",
    "    for database_name in database_list:\n",
    "        if database_prefix in database_name:\n",
    "            collection_list = mongo_list_collections(\n",
    "                mongo_client = mongo_client,\n",
    "                database_name = database_name\n",
    "            )\n",
    "            storage_structure[database_name] = collection_list\n",
    "    \n",
    "    storage_documents = {}\n",
    "    for database_name, collections in storage_structure.items():\n",
    "        if not database_name in storage_documents:\n",
    "            storage_documents[database_name] = {}\n",
    "        for collection_name in collections:\n",
    "            collection_documents = mongo_list_documents(\n",
    "                mongo_client = mongo_client,\n",
    "                database_name = database_name,\n",
    "                collection_name = collection_name,\n",
    "                filter_query = {},\n",
    "                sorting_query = [\n",
    "                    ('index', ASCENDING),\n",
    "                    ('sub-index', ASCENDING)\n",
    "                ]\n",
    "            )\n",
    "            storage_documents[database_name][collection_name] = collection_documents\n",
    "            \n",
    "    return storage_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5ed0eee5-bdd0-46b5-8a25-c06f0f5eb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_documents = get_stored_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    database_prefix = get_github_storage_prefix('K123AsJ0k1', 'cloud-hpc-oss-mlops-platform')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d75151-e190-4ce3-9e84-4bbdc38ae474",
   "metadata": {},
   "source": [
    "## Storing Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef29494-0db0-4796-921c-82d9d47376dd",
   "metadata": {},
   "source": [
    "## Langchain Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "29be8d4a-44bd-418a-acee-c806074116c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def langchain_create_code_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    code_chunks = splitter.create_documents([document])\n",
    "    code_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return code_chunks\n",
    "\n",
    "def lanchain_create_text_chunks(\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False\n",
    "    )\n",
    "\n",
    "    text_chunks = splitter.create_documents([document])\n",
    "    text_chunks = [doc.page_content for doc in document_chunks]\n",
    "    return text_chunks\n",
    "\n",
    "def langchain_create_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9aea4c-9ef7-401c-bf94-e8c94c88bbbe",
   "metadata": {},
   "source": [
    "## Qdrant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3293c5e1-03be-4329-8322-9888680392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient as qc\n",
    "from qdrant_client import models\n",
    "\n",
    "def qdrant_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, qc.Connection)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_setup_client(\n",
    "    api_key: str,\n",
    "    address: str, \n",
    "    port: str\n",
    ") -> any:\n",
    "    try:\n",
    "        qdrant_client = qc(\n",
    "            host = address,\n",
    "            port = int(port),\n",
    "            api_key = api_key,\n",
    "            https = False\n",
    "        ) \n",
    "        return qdrant_client\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_create_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str,\n",
    "    configuration: any\n",
    ") -> any:\n",
    "    try:\n",
    "        result = qdrant_client.create_collection(\n",
    "            collection_name = collection_name,\n",
    "            vectors_config = configuration\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_get_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(\n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_list_collections(\n",
    "    qdrant_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_list = []\n",
    "        for description in collections.collections:\n",
    "            collection_list.append(description.name)\n",
    "        return collection_list\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_upsert_points(\n",
    "    qdrant_client: qc, \n",
    "    collection_name: str,\n",
    "    points: any\n",
    ") -> any:\n",
    "    try:\n",
    "        results = qdrant_client.upsert(\n",
    "            collection_name = collection_name, \n",
    "            points = points\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_search_data(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    scroll_filter: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.scroll(\n",
    "            collection_name = collection_name,\n",
    "            scroll_filter = scroll_filter,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "def qdrant_search_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    query_vector: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.search(\n",
    "            collection_name = collection_name,\n",
    "            query_vector = query_vector,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str, \n",
    "    vectors: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        results = qdrant_client.delete_vectors(\n",
    "            collection_name = collection_name,\n",
    "            vectors = vectors\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing document: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dc47b-c7cf-406c-9846-7c53c89f7a29",
   "metadata": {},
   "source": [
    "## Vector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "126bd9db-f520-4bcc-8c06-de3af348e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import uuid\n",
    "import re\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def create_document_packet(\n",
    "    document: any,\n",
    "    configuration: any,\n",
    ") -> any:\n",
    "    #print('Create packet')\n",
    "    document_type = document['type']\n",
    "    used_configuration = configuration[document_type]\n",
    "    \n",
    "    document_chunks = []\n",
    "    if document_type == 'python':\n",
    "        document_chunks = langchain_generate_code_document_chunks(\n",
    "            language = used_configuration['language'],\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    if document_type == 'text' or document_type == 'yaml' or document_type == 'markdown':\n",
    "        document_chunks = lanchain_generate_text_document_chunks(\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    # This needs to remove empty chunks\n",
    "    filtered_chunks = []\n",
    "    for chunk in document_chunks:\n",
    "        if chunk.strip():\n",
    "            filtered_chunks.append(chunk)\n",
    "        \n",
    "    vector_embedding = langchain_generate_document_chunk_embeddings(\n",
    "        model_name = used_configuration['model-name'],\n",
    "        document_chunks = filtered_chunks\n",
    "    )\n",
    "\n",
    "    packet = {\n",
    "        'chunks': filtered_chunks,\n",
    "        'embeddings': vector_embedding\n",
    "    }\n",
    "    \n",
    "    return packet\n",
    "\n",
    "def format_chunk(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    chunk = re.sub(r'[^\\w\\s]', '', document_chunk)\n",
    "    chunk = re.sub(r'\\s+', ' ', chunk) \n",
    "    chunk = chunk.strip()\n",
    "    chunk = chunk.lower()\n",
    "    # This helps to remove unique hashes for duplicates such as:\n",
    "    # task_id = task_id )\n",
    "    # task_id = task_id \n",
    "    # task_id = task_id )\n",
    "    return chunk\n",
    "\n",
    "def generate_chunk_hash(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    cleaned_chunk = format_chunk(\n",
    "        document_chunk = document_chunk\n",
    "    )\n",
    "    return hashlib.md5(cleaned_chunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "def create_vector_points(\n",
    "    qdrant_client: any,\n",
    "    document_database: any,\n",
    "    document_collection: any,\n",
    "    document_type: any,\n",
    "    document_id: str, \n",
    "    document_chunks: any,\n",
    "    document_embeddings: any,\n",
    "    vector_collection: any\n",
    "):\n",
    "    #print('Create points')\n",
    "    vector_points = []\n",
    "    vector_index = 0\n",
    "    added_hashes = []\n",
    "    for chunk in document_chunks:\n",
    "        vector_id = document_id + '-' + str(vector_index + 1)\n",
    "        vector_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, vector_id))\n",
    "\n",
    "        chunk_hash = generate_chunk_hash(\n",
    "            document_chunk = chunk\n",
    "        )\n",
    "        \n",
    "        existing_chunks = qdrant_search_data(\n",
    "            qdrant_client = qdrant_client,\n",
    "            collection_name = vector_collection,\n",
    "            scroll_filter = models.Filter(\n",
    "                must = [\n",
    "                    models.FieldCondition(\n",
    "                        key = 'chunk_hash',\n",
    "                        match = models.MatchValue(\n",
    "                            value = chunk_hash\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            limit = 1\n",
    "        )\n",
    "        # Removes duplicates\n",
    "        if len(existing_chunks[0]) == 0:\n",
    "            if not chunk_hash in added_hashes:\n",
    "                given_vector = document_embeddings[vector_index]\n",
    "\n",
    "                chunk_point = PointStruct(\n",
    "                    id = vector_uuid, \n",
    "                    vector = given_vector,\n",
    "                    payload = {\n",
    "                        'database': document_database,\n",
    "                        'collection': document_collection,\n",
    "                        'document': document_id,\n",
    "                        'type': document_type,\n",
    "                        'chunk': chunk,\n",
    "                        'chunk_hash': chunk_hash\n",
    "                    }\n",
    "                )\n",
    "                added_hashes.append(chunk_hash)\n",
    "                vector_points.append(chunk_point)\n",
    "        vector_index += 1\n",
    "    return vector_points\n",
    "\n",
    "def store_document_vectors(\n",
    "    qdrant_client: any,\n",
    "    document_database,\n",
    "    document_collection,\n",
    "    document: any,\n",
    "    configuration: any,\n",
    "    vector_collection: str\n",
    ") -> bool:\n",
    "    #print('Store vectors')\n",
    "    document_id = str(document['_id'])\n",
    "    document_type = document['type']\n",
    "    \n",
    "    document_packet = create_document_packet(\n",
    "        document = document,\n",
    "        configuration = configuration\n",
    "    )\n",
    "    \n",
    "    document_chunks = document_packet['chunks']\n",
    "    document_embeddings = document_packet['embeddings']\n",
    "    \n",
    "    if 0 == len(document_embeddings):\n",
    "        return False\n",
    "    \n",
    "    vector_collections = qdrant_list_collections(\n",
    "        qdrant_client = qdrant_client\n",
    "    )\n",
    "    \n",
    "    collection_created = None\n",
    "    if not vector_collection in vector_collections:\n",
    "        collection_configuration = VectorParams(\n",
    "            size = len(document_embeddings[0]), \n",
    "            distance = Distance.COSINE\n",
    "        )\n",
    "        collection_created = qdrant_create_collection(\n",
    "            qdrant_client = qdrant_client,\n",
    "            collection_name = vector_collection,\n",
    "            configuration = collection_configuration\n",
    "        )\n",
    "\n",
    "    #if collection_created is None:\n",
    "        #return False\n",
    "\n",
    "    #print('Creating points')\n",
    "    vector_points = create_vector_points(\n",
    "        qdrant_client = qdrant_client,\n",
    "        document_database = document_database,\n",
    "        document_collection = document_collection,\n",
    "        document_type = document_type,\n",
    "        document_id = document_id,\n",
    "        document_chunks = document_chunks,\n",
    "        document_embeddings = document_embeddings,\n",
    "        vector_collection = vector_collection\n",
    "    )\n",
    "\n",
    "    #print('Points created')\n",
    "    \n",
    "    if 0 == len(vector_points):\n",
    "        return False\n",
    "    \n",
    "    points_stored = qdrant_upsert_points(\n",
    "        qdrant_client = qdrant_client, \n",
    "        collection_name = vector_collection,\n",
    "        points = vector_points\n",
    "    )\n",
    "    #print('Stored')\n",
    "    return True\n",
    "\n",
    "def create_vectors(\n",
    "    qdrant_client: any,\n",
    "    configuration: any,\n",
    "    storage_documents: any\n",
    "):\n",
    "    print('Storing vectors')\n",
    "    for document_database, document_collections in storage_documents.items():\n",
    "        vector_collection = document_database.replace('|','-') + '-embeddings'\n",
    "        for document_collection, documents in document_collections.items():\n",
    "            for document in documents:\n",
    "                stored = store_document_vectors(\n",
    "                    qdrant_client = qdrant_client,\n",
    "                    document_database = document_database,\n",
    "                    document_collection = document_collection,\n",
    "                    document = document,\n",
    "                    configuration = configuration,\n",
    "                    vector_collection = vector_collection\n",
    "                )\n",
    "    print('Vectors stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a0c446d6-dca4-4d47-9498-1a9c2441f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "qdrant_client = qdrant_setup_client(\n",
    "    api_key = 'qdrant_key',\n",
    "    address = '127.0.0.1', \n",
    "    port = '6333'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e6ab5ac0-2963-42f7-a67f-d62380bdbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_configuration = {\n",
    "    'python': {\n",
    "        'language': Language.PYTHON,\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'markdown': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'yaml': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "42fdc095-6e46-4b77-96e4-6822a6ab7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing vectors\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[213], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvector_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_documents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstorage_documents\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[212], line 205\u001b[0m, in \u001b[0;36mcreate_vectors\u001b[0;34m(qdrant_client, configuration, storage_documents)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m document_collection, documents \u001b[38;5;129;01min\u001b[39;00m document_collections\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m--> 205\u001b[0m             stored \u001b[38;5;241m=\u001b[39m \u001b[43mstore_document_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdocument_database\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdocument_collection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvector_collection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvector_collection\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVectors stored\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[212], line 140\u001b[0m, in \u001b[0;36mstore_document_vectors\u001b[0;34m(qdrant_client, document_database, document_collection, document, configuration, vector_collection)\u001b[0m\n\u001b[1;32m    137\u001b[0m document_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(document[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    138\u001b[0m document_type \u001b[38;5;241m=\u001b[39m document[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 140\u001b[0m document_packet \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_document_packet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfiguration\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m document_chunks \u001b[38;5;241m=\u001b[39m document_packet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    146\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m document_packet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[212], line 36\u001b[0m, in \u001b[0;36mcreate_document_packet\u001b[0;34m(document, configuration)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m     34\u001b[0m         filtered_chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m---> 36\u001b[0m vector_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mlangchain_generate_document_chunk_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mused_configuration\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel-name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_chunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfiltered_chunks\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m packet \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m: filtered_chunks,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: vector_embedding\n\u001b[1;32m     44\u001b[0m }\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packet\n",
      "Cell \u001b[0;32mIn[164], line 46\u001b[0m, in \u001b[0;36mlangchain_generate_document_chunk_embeddings\u001b[0;34m(model_name, document_chunks)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlangchain_generate_document_chunk_embeddings\u001b[39m(\n\u001b[1;32m     40\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     41\u001b[0m     document_chunks: \u001b[38;5;28many\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28many\u001b[39m:\n\u001b[1;32m     43\u001b[0m     embedding_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(\n\u001b[1;32m     44\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     chunk_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_chunks\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunk_embeddings\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py:87\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     85\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:565\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 565\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mall_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m    566\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "create_vectors(\n",
    "    qdrant_client = qdrant_client,\n",
    "    configuration = vector_configuration,\n",
    "    storage_documents = storage_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07548d-e73d-49f4-afce-cb74d798974e",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fe4b7-f763-4d93-900b-23b1b22f1fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664663d3-39ac-4acb-bc1b-e741daebf1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
