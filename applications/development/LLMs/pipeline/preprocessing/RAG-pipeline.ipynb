{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76734520-9ec6-4cb9-bed5-477006377137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df5dcf8-2d82-40da-8914-9ce8aef3b84f",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7f7b3f-6c35-47f0-8ba4-f0853ab66d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import Config,RepositoryEnv\n",
    "env_path = '/home/sfniila/.ssh/.env'\n",
    "env_config = Config(RepositoryEnv(env_path))\n",
    "github_token = env_config.get('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4314a1-75b3-4a21-93ea-da45671295c9",
   "metadata": {},
   "source": [
    "## PyGitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b07a4fb-3e77-4418-900a-00355b551a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "def pygithub_get_repo_paths(\n",
    "    token: str,\n",
    "    owner: str, \n",
    "    name: str\n",
    ") -> any:\n",
    "    g = Github(token)\n",
    "    repo = g.get_repo(f\"{owner}/{name}\")\n",
    "    contents = repo.get_contents(\"\")\n",
    "    paths = []\n",
    "    while len(contents) > 0:\n",
    "      file_content = contents.pop(0)\n",
    "      if file_content.type == 'dir':\n",
    "        contents.extend(repo.get_contents(file_content.path))\n",
    "      else:\n",
    "        paths.append(file_content.path)\n",
    "    g.close()\n",
    "    return paths\n",
    "\n",
    "def pygithub_get_path_content(\n",
    "    token: str,\n",
    "    owner: str, \n",
    "    name: str, \n",
    "    path: str\n",
    ") -> any:\n",
    "    g = Github(token)\n",
    "    repo = g.get_repo(f\"{owner}/{name}\")\n",
    "    file_content = repo.get_contents(path)\n",
    "    content = file_content.decoded_content.decode('utf-8')\n",
    "    # fails on some yaml files\n",
    "    # deployment/kubeflow/manifests/contrib/ray/kuberay-operator/base/resources.yaml\n",
    "    # unsupported encoding: none\n",
    "    g.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d75007-8269-450f-a54b-c485a324949e",
   "metadata": {},
   "source": [
    "## Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2f0cd3-106c-4b5d-9926-40dd191ee231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_github_repo_documents(\n",
    "    github_token: str,\n",
    "    repo_owner: str,\n",
    "    repo_name: str,\n",
    "    relevant_files: any,\n",
    "    store: bool\n",
    ") -> any:\n",
    "    storage_path = os.getcwd() + '/' + repo_owner + '-' + repo_name + '.txt' \n",
    "\n",
    "    repo_paths = []\n",
    "    if not os.path.exists(storage_path):\n",
    "        print('Getting github paths')\n",
    "        repo_paths = pygithub_get_repo_paths(\n",
    "            token = github_token,\n",
    "            owner = repo_owner, \n",
    "            name = repo_name\n",
    "        )\n",
    "        \n",
    "        if store:\n",
    "            print('Storing paths')\n",
    "            path_amount = len(repo_paths)\n",
    "            i = 0\n",
    "            with open(storage_path, 'w') as file:\n",
    "                for line in repo_paths:\n",
    "                    row = line \n",
    "                    if i < path_amount:\n",
    "                        row += '\\n'\n",
    "                    file.write(row)\n",
    "                    i += 1\n",
    "            print('Paths stored')\n",
    "    else:\n",
    "        print('Getting stored paths')\n",
    "        with open(storage_path, 'r') as file:\n",
    "            repo_paths = file.readlines()\n",
    "    print('Paths fetched')\n",
    "                \n",
    "    print('Filtering paths')\n",
    "    relevant_paths = []\n",
    "    for path in repo_paths:\n",
    "        path_split = path.split('/')\n",
    "        file_end = path_split[-1].split('.')[-1].rstrip()\n",
    "        if file_end in relevant_files:\n",
    "            relevant_paths.append(path.rstrip())\n",
    "    print('Paths filtered')\n",
    "\n",
    "    formatted_paths = {\n",
    "        'paths': relevant_paths\n",
    "    }\n",
    "\n",
    "    # consider how to store this to allas\n",
    "    return formatted_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee717787-4d21-4082-ad61-5b1973926cc9",
   "metadata": {},
   "source": [
    "Files of intrest\n",
    "- md = important to usesr\n",
    "- txt = isn't important to users\n",
    "- sh = isn't important to users\n",
    "- yaml = important to developers\n",
    "- py = important to users and developers\n",
    "- ipynb = important to users and developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ccfa42-95dc-4c8a-9784-fa703937d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stored paths\n",
      "Paths fetched\n",
      "Filtering paths\n",
      "Paths filtered\n"
     ]
    }
   ],
   "source": [
    "repository_paths = get_github_repo_documents(\n",
    "    github_token = github_token,\n",
    "    repo_owner = 'K123AsJ0k1',\n",
    "    repo_name = 'cloud-hpc-oss-mlops-platform',\n",
    "    relevant_files = [\n",
    "        'md',\n",
    "        'yaml',\n",
    "        'py',\n",
    "        'ipynb'\n",
    "    ],\n",
    "    store = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70f6e8-ed91-45c9-afa5-cf598998ac17",
   "metadata": {},
   "source": [
    "# Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfc42e-02e2-4d94-b7f9-9782228cc44d",
   "metadata": {},
   "source": [
    "## Mongo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e4c444-5608-4dde-bf09-c0364a1b8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient as mc\n",
    "\n",
    "def mongo_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return isinstance(storage_client, mc.Connection)\n",
    "\n",
    "def mongo_setup_client(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    address: str,\n",
    "    port: str\n",
    ") -> any:\n",
    "    connection_prefix = 'mongodb://(username):(password)@(address):(port)/'\n",
    "    connection_address = connection_prefix.replace('(username)', username)\n",
    "    connection_address = connection_address.replace('(password)', password)\n",
    "    connection_address = connection_address.replace('(address)', address)\n",
    "    connection_address = connection_address.replace('(port)', port)\n",
    "    mongo_client = mc(\n",
    "        host = connection_address\n",
    "    )\n",
    "    return mongo_client\n",
    "\n",
    "def mongo_get_database(\n",
    "    mongo_client: any,\n",
    "    database_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        return database\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_check_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database_exists = database_name in mongo_client.list_database_names()\n",
    "        return database_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_list_databases(\n",
    "    mongo_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        databases = mongo_client.list_database_names()\n",
    "        return databases\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        mongo_client.drop_database(database_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_get_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collection = database[collection_name]\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def mongo_check_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        collection_exists = collection_name in database.list_collection_names()\n",
    "        return collection_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_update_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_many(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_list_collections(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collections = database.list_collection_names()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try: \n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        database.drop_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_create_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    document: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.insert_one(document)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_get_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    "):\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        document = collection.find_one(filter_query)\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None \n",
    "\n",
    "def mongo_list_documents(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any,\n",
    "    sorting_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        documents = list(collection.find(filter_query).sort(sorting_query))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_update_document(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_one(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_remove_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    ") -> bool:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.delete_one(filter_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116c18c-53f1-4222-986f-6e3fa00e65b2",
   "metadata": {},
   "source": [
    "## Markdown documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec230be-1fbe-4b25-90da-078055b830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def create_markdown_documents(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    \n",
    "    documents = []\n",
    "    document = ''\n",
    "    index = 1\n",
    "    for element in soup.descendants:\n",
    "        if element.name in ['h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            text = element.get_text(strip = True)\n",
    "            if not document == '':\n",
    "                document = document.replace('\\n', '')\n",
    "                if not len(document.split()) == 1:\n",
    "                    documents.append({\n",
    "                        'index': index,\n",
    "                        'sub-index': 0,\n",
    "                        'type': 'markdown',\n",
    "                        'data': document\n",
    "                    })\n",
    "                    index += 1\n",
    "                document = ''\n",
    "            document += text\n",
    "        elif element.name == 'p':\n",
    "            text = element.get_text(strip = True)\n",
    "            text = re.sub(code_block_pattern, '', text)\n",
    "            text = text.rstrip('\\n')\n",
    "            text = text.replace('\\nsh', '')\n",
    "            text = text.replace('\\nbash', '')\n",
    "            document += ' ' + text\n",
    "        elif element.name in ['ul', 'ol']:\n",
    "            text = ''\n",
    "            for li in element.find_all('li'):\n",
    "                item = li.get_text(strip=True)\n",
    "                if not '-' in item:\n",
    "                    text += '-' + item\n",
    "                    continue\n",
    "                text += item\n",
    "            document += ' ' + text\n",
    "            \n",
    "    documents.append({\n",
    "        'index': index,\n",
    "        'sub-index': 0,\n",
    "        'type': 'markdown',\n",
    "        'data': document\n",
    "    })\n",
    "    \n",
    "    formatted_documents = {\n",
    "        'text': documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d185d0f-0003-4958-a026-0b79c6c62333",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = repository_paths[7]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c4d08e0-6f5e-41f1-a465-e9829d6cb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_documents = create_markdown_documents(\n",
    "    markdown_text = example_data\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f93ac-63d8-4b9f-8467-e202571ce0aa",
   "metadata": {},
   "source": [
    "## YAML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b019e009-db94-47b4-9021-9ab915c437b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "def extract_yaml_values(\n",
    "    section: any,\n",
    "    path: str,\n",
    "    values: any\n",
    ") -> any:\n",
    "    for key, value in section.items():\n",
    "        if path == '':\n",
    "            current_path = key\n",
    "        else:\n",
    "            current_path = path + '/' + key\n",
    "        if isinstance(value, dict):\n",
    "            extract_yaml_values(\n",
    "                section = value,\n",
    "                path = current_path,\n",
    "                values = values\n",
    "            )\n",
    "        if isinstance(value, list):\n",
    "            number = 1\n",
    "            \n",
    "            for case in value:\n",
    "                base_path = current_path\n",
    "                if isinstance(case, dict):\n",
    "                   extract_yaml_values(\n",
    "                       section = case,\n",
    "                       path = current_path,\n",
    "                       values = values\n",
    "                   ) \n",
    "                   continue\n",
    "                base_path += '/' + str(number)\n",
    "                number += 1\n",
    "                values.append(base_path + '=' + str(case))\n",
    "        else:\n",
    "            if isinstance(value, dict):\n",
    "                continue\n",
    "            if isinstance(value, list):\n",
    "                continue\n",
    "            values.append(current_path + '=' + str(value))\n",
    "            \n",
    "    return values\n",
    "\n",
    "def create_yaml_documents(\n",
    "    yaml_text: any\n",
    ") -> any:\n",
    "    # has problems with some yaml files\n",
    "    # unsupported encoding: none\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
    "    # 'list' object has no attribute 'items'\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
    "    # 'NoneType' object has no attribute 'items'\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/base/metadata/overlays/db/kustomization.yaml\n",
    "    #   could not determine a constructor for the tag 'tag:yaml.org,2002:value'\n",
    "    # in \"<unicode string>\", line 41, column 18:\n",
    "    #      delimiter: =\n",
    "    yaml_data = list(yaml.safe_load_all(yaml_text))\n",
    "\n",
    "    documents = []\n",
    "    index = 1\n",
    "    for data in yaml_data:\n",
    "        yaml_values = extract_yaml_values(\n",
    "            section = data,\n",
    "            path = '',\n",
    "            values = []\n",
    "        )\n",
    "\n",
    "        previous_root = ''\n",
    "        document = ''\n",
    "        sub_index = 1\n",
    "        for value in yaml_values:\n",
    "            equal_split = value.split('=')\n",
    "            path_split = equal_split[0].split('/')\n",
    "            root = path_split[0]\n",
    "            if not root == previous_root:\n",
    "                if 0 < len(document):\n",
    "                    documents.append({\n",
    "                        'index': index,\n",
    "                        'sub-index': sub_index,\n",
    "                        'type': 'yaml',\n",
    "                        'data': document\n",
    "                    })\n",
    "                    sub_index += 1\n",
    "                    \n",
    "                previous_root = root\n",
    "                document = value\n",
    "            else:\n",
    "                document += value\n",
    "                \n",
    "        documents.append({\n",
    "            'index': index,\n",
    "            'sub-index': sub_index,\n",
    "            'type': 'yaml',\n",
    "            'data': document\n",
    "        })\n",
    "        index += 1\n",
    "\n",
    "    formatted_documents = {\n",
    "        'text': documents\n",
    "    }\n",
    "            \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "239ffb67-18ec-4a3e-a8ca-57fa1364ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_yaml = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'deployment/forwarder/celery/celery-deployment.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48880c03-8a81-46a5-b207-f0c583e5e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_documents = create_yaml_documents(\n",
    "    yaml_text = example_yaml\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10990a57-ea53-4e0e-9c53-d620811f5b95",
   "metadata": {},
   "source": [
    "## TreeSitter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c68780-ac71-4ede-84fb-713a7cf56da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "def tree_extract_imports(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(tree_extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def tree_extract_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(tree_extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "\n",
    "def tree_extract_code_and_dependencies(\n",
    "    node: any,\n",
    "    code_text: str\n",
    ") -> any:\n",
    "    codes = []\n",
    "    if not node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name')\n",
    "        if name is None:\n",
    "            code = code_text[start_byte:end_byte].decode('utf8')\n",
    "            if not 'def' in code:\n",
    "                dependencies = tree_extract_dependencies(node, code_text)\n",
    "                codes.append({\n",
    "                    'name': 'global',\n",
    "                    'code': code,\n",
    "                    'dependencies': dependencies\n",
    "                })\n",
    "    return codes\n",
    "\n",
    "def tree_extract_functions_and_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(tree_extract_functions_and_dependencies(child, code_text))\n",
    "    return functions\n",
    "\n",
    "def tree_get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def tree_get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions\n",
    "\n",
    "def tree_create_code_document(\n",
    "    code_imports: any,\n",
    "    code_functions: any,\n",
    "    function_item: any\n",
    ") -> any:\n",
    "    used_imports = tree_get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = tree_get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': function_item['name'],\n",
    "        'dependencies': function_item['dependencies'],\n",
    "        'code': function_item['code']\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "     \n",
    "def tree_format_code_document(\n",
    "    code_document: any\n",
    ") -> any:\n",
    "    formatted_document = ''\n",
    "    for doc_import in code_document['imports']:\n",
    "        formatted_document += doc_import + '\\n'\n",
    "\n",
    "    for doc_functions in code_document['functions']:\n",
    "        formatted_document += doc_functions + '\\n'\n",
    "\n",
    "    if 0 < len(code_document['dependencies']):\n",
    "        formatted_document += 'code dependencies\\n'\n",
    "\n",
    "        for doc_dependency in code_document['dependencies']:\n",
    "            formatted_document += doc_dependency + '\\n'\n",
    "\n",
    "    if code_document['name'] == 'global':\n",
    "        formatted_document += code_document['name'] + ' code\\n'\n",
    "    else:\n",
    "        formatted_document += 'function ' + code_document['name'] + ' code\\n'\n",
    "    \n",
    "    for line in code_document['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        doc_code = re.sub(r'#.*','', line)\n",
    "        if not bool(doc_code.strip()):\n",
    "            continue\n",
    "        formatted_document += doc_code + '\\n'    \n",
    "    return formatted_document\n",
    "\n",
    "def tree_create_python_code_and_function_documents(\n",
    "    code_document: any\n",
    "):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "   \n",
    "    tree = parser.parse(\n",
    "        bytes(\n",
    "            code_document,\n",
    "            \"utf8\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    code_imports = tree_extract_imports(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_global = tree_extract_code_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_functions = tree_extract_functions_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    initial_documents = []\n",
    "    for item in code_global:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    for item in code_functions:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    formatted_documents = []\n",
    "    seen_functions = []\n",
    "    for document in initial_documents:\n",
    "        if not document['name'] == 'global':\n",
    "            if document['name'] in seen_functions:\n",
    "                continue\n",
    "        \n",
    "        formatted_document = tree_format_code_document(\n",
    "            code_document = document\n",
    "        )\n",
    "\n",
    "        formatted_documents.append(formatted_document)\n",
    "        seen_functions.append(document['name'])\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46127d-3124-4243-a7fa-77dae3531c91",
   "metadata": {},
   "source": [
    "## Python documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42502b34-833a-41a5-8eb8-d6b52bb98330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_python_documents(\n",
    "    python_text: any\n",
    "): \n",
    "    joined_code = ''.join(python_text)\n",
    "    block_code_documents = tree_create_python_code_and_function_documents(\n",
    "        code_document = joined_code\n",
    "    )\n",
    "\n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    code_doc_index = 0\n",
    "    for code_doc in block_code_documents:\n",
    "        row_split = code_doc.split('\\n')\n",
    "        for row in row_split:\n",
    "            if 'function' in row and 'code' in row:\n",
    "                # This causes problems with some documents\n",
    "                # list index out of range\n",
    "                function_name = row.split(' ')[1]\n",
    "                if not function_name in seen_function_names:\n",
    "                    seen_function_names.append(function_name)\n",
    "                else:\n",
    "                    del block_code_documents[code_doc_index]\n",
    "        code_doc_index += 1\n",
    "\n",
    "    if 0 < len(block_code_documents):\n",
    "        index = 1\n",
    "        for code_doc in block_code_documents:\n",
    "            code_documents.append({\n",
    "                'index': index,\n",
    "                'sub-index': 0,\n",
    "                'type': 'python',\n",
    "                'data': code_doc\n",
    "            })\n",
    "            index += 1\n",
    "        \n",
    "    formatted_documents = {\n",
    "        'code': code_documents\n",
    "    }\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7426485-07ce-46ea-8d2b-05b598f3c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_python = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'applications/article/submitter/backend/functions/platforms/flower.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e688f33c-7429-458b-8983-994918e8537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_documents = create_python_documents(\n",
    "    python_text = example_python\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0259e6-052b-4e49-b73b-bd95fd6d3afe",
   "metadata": {},
   "source": [
    "## Notebook documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ecc3b9-29af-42e9-acad-27f50bd8dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "\n",
    "def parse_jupyter_notebook_markdown_into_text(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    text = soup.get_text()\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    text = re.sub(code_block_pattern, '', text)\n",
    "    text = text.rstrip('\\n')\n",
    "    text = text.replace('\\nsh', '\\n')\n",
    "    text = text.replace('\\nbash', '\\n')\n",
    "    return text\n",
    "\n",
    "def extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_text: any\n",
    "): \n",
    "    notebook_documents = {\n",
    "        'markdown': [],\n",
    "        'code': []\n",
    "    }\n",
    "\n",
    "    notebook = nbformat.reads(notebook_text, as_version=2)\n",
    "    index = 1\n",
    "    for cell in notebook.worksheets[0].cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            notebook_documents['markdown'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "        if cell.cell_type == 'code':\n",
    "            notebook_documents['code'].append({\n",
    "                'id': index,\n",
    "                'data': cell.input\n",
    "            })\n",
    "            index += 1\n",
    "    \n",
    "    return notebook_documents\n",
    "\n",
    "def create_notebook_documents(\n",
    "    notebook_text: any\n",
    "):\n",
    "    notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "        notebook_text = notebook_text\n",
    "    )\n",
    "\n",
    "    markdown_documents = []\n",
    "    for block in notebook_documents['markdown']:\n",
    "        joined_text = ''.join(block['data'])\n",
    "        markdown_text = parse_jupyter_notebook_markdown_into_text(\n",
    "            markdown_text = joined_text\n",
    "        )\n",
    "        markdown_documents.append({\n",
    "            'index': block['id'],\n",
    "            'sub-index': 0,\n",
    "            'type': 'markdown',\n",
    "            'data': markdown_text\n",
    "        })\n",
    "        \n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    for block in notebook_documents['code']:\n",
    "        joined_code = ''.join(block['data'])\n",
    "        block_code_documents = tree_create_python_code_and_function_documents(\n",
    "            code_document = joined_code\n",
    "        )\n",
    "\n",
    "        code_doc_index = 0\n",
    "        for code_doc in block_code_documents:\n",
    "            row_split = code_doc.split('\\n')\n",
    "            for row in row_split:\n",
    "                if 'function' in row and 'code' in row:\n",
    "                    # This causes problems with some documents\n",
    "                    # list index out of range\n",
    "                    function_name = row.split(' ')[1]\n",
    "                    if not function_name in seen_function_names:\n",
    "                        seen_function_names.append(function_name)\n",
    "                    else:\n",
    "                        del block_code_documents[code_doc_index]\n",
    "            code_doc_index += 1\n",
    "        \n",
    "        if 0 < len(block_code_documents):\n",
    "            sub_indexes = False\n",
    "            if 1 < len(block_code_documents):\n",
    "                sub_indexes = True\n",
    "            index = 1\n",
    "            for code_doc in block_code_documents:\n",
    "                if sub_indexes:\n",
    "                    code_documents.append({\n",
    "                        'index': block['id'],\n",
    "                        'sub-index': index, \n",
    "                        'type': 'python',\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                else:\n",
    "                    code_documents.append({ \n",
    "                        'index': block['id'],\n",
    "                        'sub-index': 0,\n",
    "                        'type': 'python',\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                index += 1\n",
    "            \n",
    "    formatted_documents = {\n",
    "        'text': markdown_documents,\n",
    "        'code': code_documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a060a79-f17c-45c6-b81d-ad36d0c0965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_notebook = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'tutorials/cloud_hpc/Cloud-HPC-FMNIST.ipynb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2fb19ea7-3ddc-4fce-99e9-6d37c21f60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_documents = create_notebook_documents(\n",
    "    notebook_text = example_notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c944d67-e224-49b5-a01f-d348deb398ae",
   "metadata": {},
   "source": [
    "## Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb12557-3abf-4426-adfd-d42a4706067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = mongo_setup_client(\n",
    "    username = 'mongo123',\n",
    "    password = 'mongo456',\n",
    "    address = '127.0.0.1',\n",
    "    port = '27017'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe860a31-582b-4fa7-b8bf-73e2ca5919ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_repository_path_documents(\n",
    "    mongo_client: any,\n",
    "    github_token: any,\n",
    "    repository_owner: str,\n",
    "    repository_name: str,\n",
    "    repository_path: str,\n",
    "    database_name: str,\n",
    "    collection_name: str\n",
    "):\n",
    "    collection_exists = mongo_check_collection(\n",
    "        mongo_client = mongo_client, \n",
    "        database_name = database_name, \n",
    "        collection_name = collection_name\n",
    "    )\n",
    "\n",
    "    if collection_exists:\n",
    "        return False\n",
    "\n",
    "    target_content = ''\n",
    "    try:\n",
    "        target_content = pygithub_get_path_content(\n",
    "            token = github_token,\n",
    "            owner = repository_owner, \n",
    "            name = repository_name, \n",
    "            path = repository_path\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(repository_path)\n",
    "        print(e)\n",
    "\n",
    "    if target_content == '':\n",
    "        return False\n",
    "\n",
    "    path_split = repository_path.split('/')\n",
    "    target_type = path_split[-1].split('.')[-1]\n",
    "    \n",
    "    target_documents = {}\n",
    "    if target_type == 'md':\n",
    "        try:\n",
    "            target_documents = create_markdown_documents(\n",
    "                markdown_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'yaml':\n",
    "        try:\n",
    "            target_documents = create_yaml_documents(\n",
    "                yaml_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'py':\n",
    "        try:\n",
    "            target_documents = create_python_documents(\n",
    "                python_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'ipynb':\n",
    "        try:\n",
    "            target_documents = create_notebook_documents(\n",
    "                notebook_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if 0 < len(target_documents):\n",
    "        for doc_type, docs in target_documents.items():\n",
    "            for document in docs:\n",
    "                result = mongo_create_document(\n",
    "                    mongo_client = mongo_client,\n",
    "                    database_name = database_name,\n",
    "                    collection_name = collection_name,\n",
    "                    document = document\n",
    "                )\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_github_storage_prefix(\n",
    "    repository_owner: str,\n",
    "    repository_name: str\n",
    ") -> str:\n",
    "    return repository_owner + '|' + repository_name + '|'\n",
    "\n",
    "def store_github_repository_documents(\n",
    "    mongo_client: any,\n",
    "    github_token: str,\n",
    "    repository_owner: str,\n",
    "    repository_name: str,\n",
    "    repository_paths: any\n",
    ") -> any:\n",
    "    print('Storing paths')\n",
    "    paths = repository_paths['paths']\n",
    "    for path in paths:\n",
    "        path_split = path.split('/')\n",
    "        document_database_name = get_github_storage_prefix(repository_owner, repository_name) + path_split[-1].split('.')[-1]\n",
    "        \n",
    "        document_collection_name = ''\n",
    "        for word in path_split[:-1]:\n",
    "            document_collection_name += word[:2] + '|'\n",
    "        document_collection_name += path_split[-1].split('.')[0]\n",
    "\n",
    "        stored = store_repository_path_documents(\n",
    "            mongo_client = mongo_client,\n",
    "            github_token = github_token,\n",
    "            repository_owner = repository_owner,\n",
    "            repository_name = repository_name,\n",
    "            repository_path = path,\n",
    "            database_name = document_database_name,\n",
    "            collection_name = document_collection_name\n",
    "        )\n",
    "    print('Paths stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e25a1e1c-3177-41b7-95c7-e0c10f11ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing paths\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG-Development.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG-pipeline.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG_Preprocessing.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/documents.py\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/python_parsing.py\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/platforms/tree.py\n",
      "list index out of range\n",
      "deployment/kubeflow/manifests/contrib/ray/kuberay-operator/base/resources.yaml\n",
      "unsupported encoding: none\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/eventing-core.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/in-memory-channel.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/mt-channel-broker.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-serving/base/upstream/net-istio.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-serving/base/upstream/serving-core.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-openshift/patches/service-serving-cert.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-openshift/patches/webhook-inject-cabundle.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-with-kubeflow/patches/enable-ui-authz-checks.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-with-kubeflow/patches/ui-rbac.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/openshift-pipelines-custom-task/pipelineloop-controller-patch.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/openshift-pipelines-custom-task/pipelineloop-webhook-patch.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/contrib/kserve/models-web-app/overlays/kubeflow/patches/web-app-vsvc.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/base/metadata/overlays/db/kustomization.yaml\n",
      "could not determine a constructor for the tag 'tag:yaml.org,2002:value'\n",
      "  in \"<unicode string>\", line 41, column 18:\n",
      "          delimiter: =\n",
      "                     ^\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-dashboard/tekton-dashboard-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/pipeline/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/pipeline/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-dashboard/tekton-dashboard-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "Paths stored\n"
     ]
    }
   ],
   "source": [
    "store_github_repository_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    github_token = github_token,\n",
    "    repository_owner = 'K123AsJ0k1', \n",
    "    repository_name = 'cloud-hpc-oss-mlops-platform', \n",
    "    repository_paths = repository_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b4bc4d1-9cd2-489c-8167-027e82ea42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "    \n",
    "def get_stored_documents(\n",
    "    mongo_client: any,\n",
    "    database_prefix: str\n",
    ") -> any:\n",
    "    storage_structure = {}\n",
    "    database_list = mongo_list_databases(\n",
    "        mongo_client = mongo_client\n",
    "    )\n",
    "    for database_name in database_list:\n",
    "        if database_prefix in database_name:\n",
    "            collection_list = mongo_list_collections(\n",
    "                mongo_client = mongo_client,\n",
    "                database_name = database_name\n",
    "            )\n",
    "            storage_structure[database_name] = collection_list\n",
    "    \n",
    "    storage_documents = {}\n",
    "    for database_name, collections in storage_structure.items():\n",
    "        if not database_name in storage_documents:\n",
    "            storage_documents[database_name] = {}\n",
    "        for collection_name in collections:\n",
    "            collection_documents = mongo_list_documents(\n",
    "                mongo_client = mongo_client,\n",
    "                database_name = database_name,\n",
    "                collection_name = collection_name,\n",
    "                filter_query = {},\n",
    "                sorting_query = [\n",
    "                    ('index', ASCENDING),\n",
    "                    ('sub-index', ASCENDING)\n",
    "                ]\n",
    "            )\n",
    "            storage_documents[database_name][collection_name] = collection_documents\n",
    "            \n",
    "    return storage_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed0eee5-bdd0-46b5-8a25-c06f0f5eb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_documents = get_stored_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    database_prefix = get_github_storage_prefix('K123AsJ0k1', 'cloud-hpc-oss-mlops-platform')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea094ed-9324-48ce-b7c3-df51219b44b9",
   "metadata": {},
   "source": [
    "## MinIO Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "922a6270-3c62-4dec-b4d6-ed7b2ef2c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "\n",
    "def create_bucket(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str\n",
    ") -> bool:\n",
    "    MINIO_CLIENT = minio_client \n",
    "    try:\n",
    "        MINIO_CLIENT.make_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO bucket creation error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "    \n",
    "def check_bucket(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        status = MINIO_CLIENT.bucket_exists(bucket_name = bucket_name)\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO bucket checking error')\n",
    "        logger.error(e)\n",
    "        return False \n",
    "       \n",
    "def delete_bucket(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        MINIO_CLIENT.remove_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO bucket deletion error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "# Works\n",
    "def create_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool: \n",
    "    # Be aware that MinIO objects have a size limit of 1GB, \n",
    "    # which might result to large header error\n",
    "    MINIO_CLIENT = minio_client\n",
    "    \n",
    "    pickled_data = pickle.dumps(data)\n",
    "    length = len(pickled_data)\n",
    "    buffer = io.BytesIO()\n",
    "    buffer.write(pickled_data)\n",
    "    buffer.seek(0)\n",
    "    try:\n",
    "        MINIO_CLIENT.put_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path + '.pkl',\n",
    "            data = buffer,\n",
    "            length = length,\n",
    "            metadata = metadata\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO object creation error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "# Works\n",
    "def check_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        object_info = MINIO_CLIENT.stat_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path + '.pkl'\n",
    "        )      \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False \n",
    "# Works\n",
    "def delete_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        MINIO_CLIENT.remove_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path + '.pkl'\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO object deletion error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "# Works\n",
    "def update_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool:  \n",
    "    remove = delete_object(logger,minio_client,bucket_name, object_path)\n",
    "    if remove:\n",
    "        create = create_object(logger,minio_client, bucket_name, object_path, data, metadata)\n",
    "        if create:\n",
    "            return True\n",
    "    return False\n",
    "# works\n",
    "def create_or_update_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any, \n",
    "    metadata: dict\n",
    ") -> any:\n",
    "    bucket_status = check_bucket(logger,minio_client,bucket_name)\n",
    "    if not bucket_status:\n",
    "        creation_status = create_bucket(logger,minio_client,bucket_name)\n",
    "        if not creation_status:\n",
    "            return None\n",
    "    object_status = check_object(logger,minio_client,bucket_name, object_path)\n",
    "    if not object_status:\n",
    "        return create_object(logger,minio_client,bucket_name, object_path, data, metadata)\n",
    "    else:\n",
    "        return update_object(logger,minio_client,bucket_name, object_path, data, metadata)\n",
    "# Works\n",
    "def get_object_data_and_metadata(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> dict:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    \n",
    "    try:\n",
    "        given_object_info = MINIO_CLIENT.stat_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path + '.pkl'\n",
    "        )\n",
    "        # There seems to be some kind of a limit\n",
    "        # with the amount of request a client \n",
    "        # can make, which is why this variable\n",
    "        # is set here to give more time got the client\n",
    "        # to complete the request\n",
    "        given_metadata = given_object_info.metadata\n",
    "        \n",
    "        given_object_data = MINIO_CLIENT.get_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path + '.pkl'\n",
    "        )\n",
    "        given_pickled_data = given_object_data.data\n",
    "        \n",
    "        try:\n",
    "            given_data = pickle.loads(given_pickled_data)\n",
    "            relevant_metadata = {} \n",
    "            for key, value in given_metadata.items():\n",
    "                if 'x-amz-meta' in key:\n",
    "                    key_name = key[11:]\n",
    "                    relevant_metadata[key_name] = value\n",
    "            return {'data': given_data, 'metadata': relevant_metadata}\n",
    "        except Exception as e:\n",
    "            logger.error('MinIO object pickle decoding error')\n",
    "            logger.error(e)\n",
    "            return None \n",
    "    except Exception as e:\n",
    "        logger.error('MinIO object fetching error')\n",
    "        logger.error(e)\n",
    "        return None\n",
    "# Works\n",
    "def get_object_list(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str,\n",
    "    path_prefix: str\n",
    ") -> dict:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        objects = MINIO_CLIENT.list_objects(bucket_name = bucket_name, prefix = path_prefix, recursive = True)\n",
    "        object_dict = {}\n",
    "        for obj in objects:\n",
    "            object_name = obj.object_name\n",
    "            object_info = MINIO_CLIENT.stat_object(\n",
    "                bucket_name = bucket_name,\n",
    "                object_name = object_name\n",
    "            )\n",
    "            given_metadata = {} \n",
    "            for key, value in object_info.metadata.items():\n",
    "                if 'X-Amz-Meta' in key:\n",
    "                    key_name = key[11:]\n",
    "                    given_metadata[key_name] = value\n",
    "            object_dict[obj.object_name] = given_metadata\n",
    "        return object_dict\n",
    "    except Exception as e:\n",
    "        return None  \n",
    "# Works\n",
    "def delete_objects(\n",
    "    logger: any,\n",
    "    minio_client: any, \n",
    "    bucket_name: str,\n",
    "    path_prefix: str\n",
    "):\n",
    "    objects = get_object_list(logger,minio_client,bucket_name, path_prefix)\n",
    "    for object_name in objects.keys():\n",
    "        pkl_split = object_name.split('.')[0]\n",
    "        delete_object(logger,minio_client,bucket_name,pkl_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13ee1a63-b54f-46f9-a7f8-fa1d4540e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "from minio import Minio\n",
    "\n",
    "def is_minio_client(\n",
    "    storage_client: any\n",
    ") -> bool:\n",
    "    return isinstance(storage_client, Minio)\n",
    "\n",
    "def setup_minio(\n",
    "    endpoint: str,\n",
    "    username: str,\n",
    "    password: str\n",
    ") -> any:\n",
    "    minio_client = Minio(\n",
    "        endpoint = endpoint, \n",
    "        access_key = username, \n",
    "        secret_key = password,\n",
    "        secure = False\n",
    "    )\n",
    "    return minio_client\n",
    "\n",
    "def pickle_data(\n",
    "    data: any\n",
    ") -> any:\n",
    "    pickled_data = pickle.dumps(data)\n",
    "    length = len(pickled_data)\n",
    "    buffer = io.BytesIO()\n",
    "    buffer.write(pickled_data)\n",
    "    buffer.seek(0)\n",
    "    return buffer, length\n",
    "\n",
    "def unpickle_data(\n",
    "    pickled: any\n",
    ") -> any:\n",
    "    return pickle.loads(pickled)\n",
    "\n",
    "def minio_create_bucket(\n",
    "    minio_client: any,\n",
    "    bucket_name: str\n",
    ") -> bool: \n",
    "    try:\n",
    "        minio_client.make_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO bucket creation error')\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def minio_check_bucket(\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    try:\n",
    "        status = minio_client.bucket_exists(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        print('MinIO bucket checking error')\n",
    "        print(e)\n",
    "        return False \n",
    "       \n",
    "def minio_delete_bucket(\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    try:\n",
    "        minio_client.remove_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO bucket deletion error')\n",
    "        print(e)\n",
    "        return False\n",
    "# Works\n",
    "def minio_create_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool: \n",
    "    # Be aware that MinIO objects have a size limit of 1GB, \n",
    "    # which might result to large header error    \n",
    "    #length = len(data)\n",
    "\n",
    "    try:\n",
    "        buffer, length = pickle_data(\n",
    "            data = data\n",
    "        )\n",
    "\n",
    "        minio_client.put_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path,\n",
    "            data = buffer,\n",
    "            length = length,\n",
    "            metadata = metadata\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO object creation error')\n",
    "        print(e)\n",
    "        return False\n",
    "# Works\n",
    "def minio_check_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> any: \n",
    "    try:\n",
    "        object_info = minio_client.stat_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path\n",
    "        )      \n",
    "        return object_info\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "# Works\n",
    "def minio_delete_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    try:\n",
    "        minio_client.remove_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO object deletion error')\n",
    "        print(e)\n",
    "        return False\n",
    "# Works\n",
    "def minio_update_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool:  \n",
    "    remove = minio_delete_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = bucket_name,\n",
    "        object_path = object_path\n",
    "    )\n",
    "    if remove:\n",
    "        return minio_create_object(\n",
    "            minio_client = minio_client, \n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            data = data,\n",
    "            metadata = metadata\n",
    "        )\n",
    "    return False\n",
    "# works\n",
    "def minio_create_or_update_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any, \n",
    "    metadata: dict\n",
    ") -> bool:\n",
    "    bucket_status = minio_check_bucket(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = bucket_name\n",
    "    )\n",
    "    if not bucket_status:\n",
    "        creation_status = minio_create_bucket(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        if not creation_status:\n",
    "            return False\n",
    "    object_status = minio_check_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = bucket_name, \n",
    "        object_path = object_path\n",
    "    )\n",
    "    if not object_status:\n",
    "        return minio_create_object(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            data = data, \n",
    "            metadata = metadata\n",
    "        )\n",
    "    else:\n",
    "        return minio_update_object(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            data = data, \n",
    "            metadata = metadata\n",
    "        )\n",
    "# Works\n",
    "def minio_get_object_list(\n",
    "    minio_client: any,\n",
    "    bucket_name: str,\n",
    "    path_prefix: str\n",
    ") -> any:\n",
    "    try:\n",
    "        objects = minio_client.list_objects(\n",
    "            bucket_name = bucket_name, \n",
    "            prefix = path_prefix, \n",
    "            recursive = True\n",
    "        )\n",
    "        return objects\n",
    "    except Exception as e:\n",
    "        return None  \n",
    "    \n",
    "def minio_get_object_data_and_metadata(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> any:\n",
    "    try:\n",
    "        given_object_data = minio_client.get_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path\n",
    "        )\n",
    "        \n",
    "        # There seems to be some kind of a limit\n",
    "        # with the amount of request a client \n",
    "        # can make, which is why this variable\n",
    "        # is set here to give more time got the client\n",
    "        # to complete the request\n",
    "\n",
    "        given_data = unpickle_data(\n",
    "            pickled = given_object_data.data\n",
    "        )\n",
    "        \n",
    "        #given_data = given_object_data.data\n",
    "\n",
    "        given_object_info = minio_client.stat_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path\n",
    "        )\n",
    "        \n",
    "        given_metadata = given_object_info.metadata\n",
    "        \n",
    "        return {'data': given_data, 'metadata': given_metadata}\n",
    "    except Exception as e:\n",
    "        print('MinIO object fetching error')\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d75151-e190-4ce3-9e84-4bbdc38ae474",
   "metadata": {},
   "source": [
    "## Storing Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef29494-0db0-4796-921c-82d9d47376dd",
   "metadata": {},
   "source": [
    "## Langchain Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29be8d4a-44bd-418a-acee-c806074116c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def langchain_create_code_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    code_chunks = splitter.create_documents([document])\n",
    "    code_chunks = [doc.page_content for doc in code_chunks]\n",
    "    return code_chunks\n",
    "\n",
    "def lanchain_create_text_chunks(\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False\n",
    "    )\n",
    "\n",
    "    text_chunks = splitter.create_documents([document])\n",
    "    text_chunks = [doc.page_content for doc in text_chunks]\n",
    "    return text_chunks\n",
    "\n",
    "def langchain_create_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9aea4c-9ef7-401c-bf94-e8c94c88bbbe",
   "metadata": {},
   "source": [
    "## Qdrant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3293c5e1-03be-4329-8322-9888680392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient as qc\n",
    "from qdrant_client import models\n",
    "\n",
    "def qdrant_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, qc.Connection)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_setup_client(\n",
    "    api_key: str,\n",
    "    address: str, \n",
    "    port: str\n",
    ") -> any:\n",
    "    try:\n",
    "        qdrant_client = qc(\n",
    "            host = address,\n",
    "            port = int(port),\n",
    "            api_key = api_key,\n",
    "            https = False\n",
    "        ) \n",
    "        return qdrant_client\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_create_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str,\n",
    "    configuration: any\n",
    ") -> any:\n",
    "    try:\n",
    "        result = qdrant_client.create_collection(\n",
    "            collection_name = collection_name,\n",
    "            vectors_config = configuration\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_get_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(\n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_list_collections(\n",
    "    qdrant_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_list = []\n",
    "        for description in collections.collections:\n",
    "            collection_list.append(description.name)\n",
    "        return collection_list\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_upsert_points(\n",
    "    qdrant_client: qc, \n",
    "    collection_name: str,\n",
    "    points: any\n",
    ") -> any:\n",
    "    try:\n",
    "        results = qdrant_client.upsert(\n",
    "            collection_name = collection_name, \n",
    "            points = points\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_search_data(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    scroll_filter: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.scroll(\n",
    "            collection_name = collection_name,\n",
    "            scroll_filter = scroll_filter,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "def qdrant_search_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    query_vector: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.search(\n",
    "            collection_name = collection_name,\n",
    "            query_vector = query_vector,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str, \n",
    "    vectors: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        results = qdrant_client.delete_vectors(\n",
    "            collection_name = collection_name,\n",
    "            vectors = vectors\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing document: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dc47b-c7cf-406c-9846-7c53c89f7a29",
   "metadata": {},
   "source": [
    "## Vector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "126bd9db-f520-4bcc-8c06-de3af348e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import uuid\n",
    "import re\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def create_document_packet(\n",
    "    document: any,\n",
    "    configuration: any,\n",
    ") -> any:\n",
    "    document_type = document['type']\n",
    "    used_configuration = configuration[document_type]\n",
    "    \n",
    "    document_chunks = []\n",
    "    if document_type == 'python':\n",
    "        document_chunks = langchain_create_code_chunks(\n",
    "            language = used_configuration['language'],\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    if document_type == 'text' or document_type == 'yaml' or document_type == 'markdown':\n",
    "        document_chunks = lanchain_create_text_chunks(\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    # This needs to remove empty chunks\n",
    "    filtered_chunks = []\n",
    "    for chunk in document_chunks:\n",
    "        if chunk.strip():\n",
    "            filtered_chunks.append(chunk)\n",
    "        \n",
    "    vector_embedding = langchain_create_chunk_embeddings(\n",
    "        model_name = used_configuration['model-name'],\n",
    "        chunks = filtered_chunks\n",
    "    )\n",
    "\n",
    "    packet = {\n",
    "        'chunks': filtered_chunks,\n",
    "        'embeddings': vector_embedding\n",
    "    }\n",
    "    \n",
    "    return packet\n",
    "\n",
    "def format_chunk(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    chunk = re.sub(r'[^\\w\\s]', '', document_chunk)\n",
    "    chunk = re.sub(r'\\s+', ' ', chunk) \n",
    "    chunk = chunk.strip()\n",
    "    chunk = chunk.lower()\n",
    "    # This helps to remove unique hashes for duplicates such as:\n",
    "    # task_id = task_id )\n",
    "    # task_id = task_id \n",
    "    # task_id = task_id )\n",
    "    return chunk\n",
    "\n",
    "def generate_chunk_hash(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    cleaned_chunk = format_chunk(\n",
    "        document_chunk = document_chunk\n",
    "    )\n",
    "    return hashlib.md5(cleaned_chunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "def generate_document_vectors(\n",
    "    qdrant_client: any,\n",
    "    document_database: any,\n",
    "    document_collection: any,\n",
    "    document_type: any,\n",
    "    document_id: str, \n",
    "    document_chunks: any,\n",
    "    document_embeddings: any,\n",
    "    vector_collection: any\n",
    "):\n",
    "    vector_points = []\n",
    "    vector_index = 0\n",
    "    added_hashes = []\n",
    "    for chunk in document_chunks:\n",
    "        vector_id = document_id + '-' + str(vector_index + 1)\n",
    "        vector_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, vector_id))\n",
    "\n",
    "        chunk_hash = generate_chunk_hash(\n",
    "            document_chunk = chunk\n",
    "        )\n",
    "        \n",
    "        existing_chunks = qdrant_search_data(\n",
    "            qdrant_client = qdrant_client,\n",
    "            collection_name = vector_collection,\n",
    "            scroll_filter = models.Filter(\n",
    "                must = [\n",
    "                    models.FieldCondition(\n",
    "                        key = 'chunk_hash',\n",
    "                        match = models.MatchValue(\n",
    "                            value = chunk_hash\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            limit = 1\n",
    "        )\n",
    "        # Removes duplicates\n",
    "        if len(existing_chunks[0]) == 0:\n",
    "            if not chunk_hash in added_hashes:\n",
    "                given_vector = document_embeddings[vector_index]\n",
    "\n",
    "                chunk_point = PointStruct(\n",
    "                    id = vector_uuid, \n",
    "                    vector = given_vector,\n",
    "                    payload = {\n",
    "                        'database': document_database,\n",
    "                        'collection': document_collection,\n",
    "                        'document': document_id,\n",
    "                        'type': document_type,\n",
    "                        'chunk': chunk,\n",
    "                        'chunk_hash': chunk_hash\n",
    "                    }\n",
    "                )\n",
    "                added_hashes.append(chunk_hash)\n",
    "                vector_points.append(chunk_point)\n",
    "        vector_index += 1\n",
    "    return vector_points\n",
    "\n",
    "def create_document_vectors(\n",
    "    qdrant_client: any,\n",
    "    document_database,\n",
    "    document_collection,\n",
    "    document: any,\n",
    "    configuration: any,\n",
    "    vector_collection: str\n",
    ") -> bool:\n",
    "    document_id = str(document['_id'])\n",
    "    document_type = document['type']\n",
    "\n",
    "    document_packet = {}\n",
    "    try:\n",
    "        document_packet = create_document_packet(\n",
    "            document = document,\n",
    "            configuration = configuration\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(document_database,document_collection,document_id)\n",
    "        print(e)\n",
    "\n",
    "    if 0 == len(document_packet):\n",
    "        return []\n",
    "        \n",
    "    document_chunks = document_packet['chunks']\n",
    "    document_embeddings = document_packet['embeddings']\n",
    "    \n",
    "    if 0 == len(document_embeddings):\n",
    "        return []\n",
    "    \n",
    "    vector_collections = qdrant_list_collections(\n",
    "        qdrant_client = qdrant_client\n",
    "    )\n",
    "    \n",
    "    collection_created = None\n",
    "    if not vector_collection in vector_collections:\n",
    "        collection_configuration = VectorParams(\n",
    "            size = len(document_embeddings[0]), \n",
    "            distance = Distance.COSINE\n",
    "        )\n",
    "        collection_created = qdrant_create_collection(\n",
    "            qdrant_client = qdrant_client,\n",
    "            collection_name = vector_collection,\n",
    "            configuration = collection_configuration\n",
    "        )\n",
    "\n",
    "    vector_points = generate_document_vectors(\n",
    "        qdrant_client = qdrant_client,\n",
    "        document_database = document_database,\n",
    "        document_collection = document_collection,\n",
    "        document_type = document_type,\n",
    "        document_id = document_id,\n",
    "        document_chunks = document_chunks,\n",
    "        document_embeddings = document_embeddings,\n",
    "        vector_collection = vector_collection\n",
    "    )\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def store_vectors(\n",
    "    qdrant_client: any,\n",
    "    configuration: any,\n",
    "    storage_documents: any\n",
    "):\n",
    "    print('Storing vectors')\n",
    "    # create progress logs\n",
    "\n",
    "    used_object_bucket = 'llm-rag'\n",
    "    used_object_path = 'vector-documents'\n",
    "    \n",
    "    identities_exists = minio_check_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = used_object_bucket, \n",
    "        object_path = used_object_path\n",
    "    )\n",
    "\n",
    "    document_identities = []\n",
    "    if not len(identities_exists) == 0:\n",
    "        document_identities = minio_get_object_data_and_metadata(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = used_object_bucket, \n",
    "            object_path = used_object_path\n",
    "        )['data']\n",
    "    \n",
    "    amount_of_databases = len(storage_documents)\n",
    "    database_index = 1\n",
    "    for document_database, document_collections in storage_documents.items():\n",
    "        vector_collection = document_database.replace('|','-') + '-embeddings'\n",
    "        amount_of_collections = len(document_collections)\n",
    "        collection_index = 1\n",
    "        database_vectors = []\n",
    "        for document_collection, documents in document_collections.items():\n",
    "            amount_of_documents = len(documents)\n",
    "            for document in documents:\n",
    "                document_identity = document_database + '-' + document_collection + '-' + str(document['_id'])\n",
    "\n",
    "                if document_identity in document_identities:\n",
    "                    continue\n",
    "                    \n",
    "                document_vectors = create_document_vectors(\n",
    "                    qdrant_client = qdrant_client,\n",
    "                    document_database = document_database,\n",
    "                    document_collection = document_collection,\n",
    "                    document = document,\n",
    "                    configuration = configuration,\n",
    "                    vector_collection = vector_collection\n",
    "                )\n",
    "                database_vectors.extend(document_vectors)\n",
    "                document_identities.append(document_identity)\n",
    "            print('Collections: ' + str(collection_index) + '|' + str(amount_of_collections))\n",
    "            collection_index += 1\n",
    "        points_stored = qdrant_upsert_points(\n",
    "            qdrant_client = qdrant_client, \n",
    "            collection_name = vector_collection,\n",
    "            points = database_vectors\n",
    "        )\n",
    "        print('Databases: ' + str(database_index) + '|' + str(amount_of_databases))\n",
    "        database_index += 1\n",
    "\n",
    "    minio_create_or_update_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = used_object_bucket, \n",
    "        object_path = used_object_path\n",
    "        data = document_identities, \n",
    "        metadata = {}\n",
    "    )\n",
    "    \n",
    "    print('Vectors stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb0c0f3a-0b85-4ead-b075-f5a2dd61bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = setup_minio(\n",
    "    endpoint = '127.0.0.1:9000',\n",
    "    username = 'minio123',\n",
    "    password = 'minio456'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0c446d6-dca4-4d47-9498-1a9c2441f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "qdrant_client = qdrant_setup_client(\n",
    "    api_key = 'qdrant_key',\n",
    "    address = '127.0.0.1', \n",
    "    port = '6333'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6ab5ac0-2963-42f7-a67f-d62380bdbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_configuration = {\n",
    "    'python': {\n",
    "        'language': Language.PYTHON,\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'markdown': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'yaml': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624bf65-0fa4-4a7f-80a8-070567bd2568",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_vectors(\n",
    "    qdrant_client = qdrant_client,\n",
    "    configuration = vector_configuration,\n",
    "    storage_documents = stored_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07548d-e73d-49f4-afce-cb74d798974e",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a989b4b-c898-433d-809a-ebf091e24516",
   "metadata": {},
   "source": [
    "## SpaCy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "664663d3-39ac-4acb-bc1b-e741daebf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_document_keywords(\n",
    "    document: str\n",
    "):\n",
    "    doc = nlp(document.lower())\n",
    "    \n",
    "    keywords = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop               \n",
    "        and not token.is_punct              \n",
    "        and not token.is_space              \n",
    "        and len(token) > 1                  \n",
    "    ]\n",
    "    \n",
    "    keywords = list(set(keywords))\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8d0c9-7e6a-477e-bd1e-f5a3a5925a6c",
   "metadata": {},
   "source": [
    "## Meili Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b06b314-ccbc-41cf-8160-2bb92286e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meilisearch as ms\n",
    "\n",
    "def meili_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, ms.Connection)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "def meili_setup_client(\n",
    "    host: str, \n",
    "    api_key: str\n",
    ") -> any:\n",
    "    try:\n",
    "        meili_client = ms.Client(\n",
    "            url = host, \n",
    "            api_key = api_key\n",
    "        )\n",
    "        return meili_client \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_get_index( \n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_check_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        meili_client.get_index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def meili_remove_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        response = meili_client.index(\n",
    "            index_name = index_name\n",
    "        ).delete()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_list_indexes(\n",
    "    meili_client: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        indexes = meili_client.get_indexes()\n",
    "        return indexes\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_add_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    documents: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.add_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_set_filterable(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    attributes: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_filterable_attributes(attributes)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_search_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    query: any, \n",
    "    options: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.search(\n",
    "            query,\n",
    "            options\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_update_documents(\n",
    "    meili_client, \n",
    "    index_name, \n",
    "    documents\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_delete_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    ids: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.delete_documents(\n",
    "            document_ids = ids\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b71ef-af43-4e45-a4d2-84e41acccb87",
   "metadata": {},
   "source": [
    "## Keyword Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "72b4bef1-d26c-44a3-a6b6-c1249a4caf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def generate_keyword_uuid(\n",
    "    document_id: str,\n",
    "    document_index: int\n",
    ") -> str:\n",
    "    keyword_id = document_id + '-' + str(document_index + 1)\n",
    "    keyword_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, keyword_id))\n",
    "    return keyword_uuid\n",
    "\n",
    "def create_document_keywords(\n",
    "    document_database: str,\n",
    "    document_collection: str,\n",
    "    document: any,\n",
    "    document_index: int\n",
    ") -> any:\n",
    "    document_id = str(document['_id'])\n",
    "    document_data = document['data']\n",
    "    document_type = document['type']\n",
    "    \n",
    "    document_keywords = get_document_keywords(\n",
    "        document = document_data\n",
    "    )\n",
    "    \n",
    "    keyword_uuid = generate_keyword_uuid(\n",
    "        document_id = document_id,\n",
    "        document_index = document_index\n",
    "    ) \n",
    "\n",
    "    payload = {\n",
    "        'id': keyword_uuid,\n",
    "        'database': document_database,\n",
    "        'collection': document_collection,\n",
    "        'document': document_id,\n",
    "        'type': document_type,\n",
    "        'keywords': document_keywords\n",
    "    }\n",
    "\n",
    "    return payload\n",
    "    \n",
    "def store_keywords(\n",
    "    minio_client: any,\n",
    "    meili_client: any,\n",
    "    storage_documents: any\n",
    "):\n",
    "\n",
    "    used_object_bucket = 'llm-rag'\n",
    "    used_object_path = 'search-documents'\n",
    "    \n",
    "    identities_exists = minio_check_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = used_object_bucket, \n",
    "        object_path = used_object_path\n",
    "    )\n",
    "\n",
    "    document_identities = []\n",
    "    if not len(identities_exists) == 0:\n",
    "        document_identities = minio_get_object_data_and_metadata(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = used_object_bucket, \n",
    "            object_path = used_object_path\n",
    "        )['data']\n",
    "\n",
    "    amount_of_databases = len(storage_documents)\n",
    "    database_index = 1\n",
    "    for document_database, collections in storage_documents.items():\n",
    "        keyword_collection = document_database.replace('|','-') + '-keywords'\n",
    "        database_keywords = []\n",
    "        collection_index = 1\n",
    "        amount_of_collections = len(collections)\n",
    "        for document_collection, documents in collections.items():\n",
    "            document_index = 1\n",
    "            for document in documents:\n",
    "                document_identity = document_database + '-' + document_collection + '-' + str(document['_id'])\n",
    "\n",
    "                if document_identity in document_identities:\n",
    "                    continue\n",
    "\n",
    "                document_keywords = create_document_keywords(\n",
    "                    document_database = document_database,\n",
    "                    document_collection = document_collection,\n",
    "                    document = document,\n",
    "                    document_index = document_index\n",
    "                )\n",
    "\n",
    "                database_keywords.append(document_keywords)\n",
    "                \n",
    "                document_identities.append(document_identity)\n",
    "            print('Collections: ' + str(collection_index) + '|' + str(amount_of_collections))\n",
    "            collection_index += 1\n",
    "        #print(database_keywords)\n",
    "        stored = meili_add_documents(\n",
    "            meili_client = meili_client,\n",
    "            index_name = keyword_collection,\n",
    "            documents = database_keywords\n",
    "        )\n",
    "        print('Databases: ' + str(database_index) + '|' + str(amount_of_databases))\n",
    "        database_index += 1\n",
    "        \n",
    "    minio_create_or_update_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = used_object_bucket, \n",
    "        object_path = used_object_path,\n",
    "        data = document_identities, \n",
    "        metadata = {}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5fbc769-527e-48a4-b952-b767ab4c10a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=36, index_uid='K123AsJ0k1-cloud-hpc-oss-mlops-platform-ipynb-keywords', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 11, 1, 13, 51, 22, 24182))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('K123AsJ0k1-cloud-hpc-oss-mlops-platform-ipynb-keywords').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61da8311-8864-46fe-b533-61908700ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "meili_client = meili_setup_client(\n",
    "    host = 'http://127.0.0.1:7700', \n",
    "    api_key = 'meili_key'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a6d8d0f-d008-4013-b352-4960815777f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections: 1|16\n",
      "Collections: 2|16\n",
      "Collections: 3|16\n",
      "Collections: 4|16\n",
      "Collections: 5|16\n",
      "Collections: 6|16\n",
      "Collections: 7|16\n",
      "Collections: 8|16\n",
      "Collections: 9|16\n",
      "Collections: 10|16\n",
      "Collections: 11|16\n",
      "Collections: 12|16\n",
      "Collections: 13|16\n",
      "Collections: 14|16\n",
      "Collections: 15|16\n",
      "Collections: 16|16\n",
      "Databases: 1|4\n",
      "Collections: 1|84\n",
      "Collections: 2|84\n",
      "Collections: 3|84\n",
      "Collections: 4|84\n",
      "Collections: 5|84\n",
      "Collections: 6|84\n",
      "Collections: 7|84\n",
      "Collections: 8|84\n",
      "Collections: 9|84\n",
      "Collections: 10|84\n",
      "Collections: 11|84\n",
      "Collections: 12|84\n",
      "Collections: 13|84\n",
      "Collections: 14|84\n",
      "Collections: 15|84\n",
      "Collections: 16|84\n",
      "Collections: 17|84\n",
      "Collections: 18|84\n",
      "Collections: 19|84\n",
      "Collections: 20|84\n",
      "Collections: 21|84\n",
      "Collections: 22|84\n",
      "Collections: 23|84\n",
      "Collections: 24|84\n",
      "Collections: 25|84\n",
      "Collections: 26|84\n",
      "Collections: 27|84\n",
      "Collections: 28|84\n",
      "Collections: 29|84\n",
      "Collections: 30|84\n",
      "Collections: 31|84\n",
      "Collections: 32|84\n",
      "Collections: 33|84\n",
      "Collections: 34|84\n",
      "Collections: 35|84\n",
      "Collections: 36|84\n",
      "Collections: 37|84\n",
      "Collections: 38|84\n",
      "Collections: 39|84\n",
      "Collections: 40|84\n",
      "Collections: 41|84\n",
      "Collections: 42|84\n",
      "Collections: 43|84\n",
      "Collections: 44|84\n",
      "Collections: 45|84\n",
      "Collections: 46|84\n",
      "Collections: 47|84\n",
      "Collections: 48|84\n",
      "Collections: 49|84\n",
      "Collections: 50|84\n",
      "Collections: 51|84\n",
      "Collections: 52|84\n",
      "Collections: 53|84\n",
      "Collections: 54|84\n",
      "Collections: 55|84\n",
      "Collections: 56|84\n",
      "Collections: 57|84\n",
      "Collections: 58|84\n",
      "Collections: 59|84\n",
      "Collections: 60|84\n",
      "Collections: 61|84\n",
      "Collections: 62|84\n",
      "Collections: 63|84\n",
      "Collections: 64|84\n",
      "Collections: 65|84\n",
      "Collections: 66|84\n",
      "Collections: 67|84\n",
      "Collections: 68|84\n",
      "Collections: 69|84\n",
      "Collections: 70|84\n",
      "Collections: 71|84\n",
      "Collections: 72|84\n",
      "Collections: 73|84\n",
      "Collections: 74|84\n",
      "Collections: 75|84\n",
      "Collections: 76|84\n",
      "Collections: 77|84\n",
      "Collections: 78|84\n",
      "Collections: 79|84\n",
      "Collections: 80|84\n",
      "Collections: 81|84\n",
      "Collections: 82|84\n",
      "Collections: 83|84\n",
      "Collections: 84|84\n",
      "Databases: 2|4\n",
      "Collections: 1|336\n",
      "Collections: 2|336\n",
      "Collections: 3|336\n",
      "Collections: 4|336\n",
      "Collections: 5|336\n",
      "Collections: 6|336\n",
      "Collections: 7|336\n",
      "Collections: 8|336\n",
      "Collections: 9|336\n",
      "Collections: 10|336\n",
      "Collections: 11|336\n",
      "Collections: 12|336\n",
      "Collections: 13|336\n",
      "Collections: 14|336\n",
      "Collections: 15|336\n",
      "Collections: 16|336\n",
      "Collections: 17|336\n",
      "Collections: 18|336\n",
      "Collections: 19|336\n",
      "Collections: 20|336\n",
      "Collections: 21|336\n",
      "Collections: 22|336\n",
      "Collections: 23|336\n",
      "Collections: 24|336\n",
      "Collections: 25|336\n",
      "Collections: 26|336\n",
      "Collections: 27|336\n",
      "Collections: 28|336\n",
      "Collections: 29|336\n",
      "Collections: 30|336\n",
      "Collections: 31|336\n",
      "Collections: 32|336\n",
      "Collections: 33|336\n",
      "Collections: 34|336\n",
      "Collections: 35|336\n",
      "Collections: 36|336\n",
      "Collections: 37|336\n",
      "Collections: 38|336\n",
      "Collections: 39|336\n",
      "Collections: 40|336\n",
      "Collections: 41|336\n",
      "Collections: 42|336\n",
      "Collections: 43|336\n",
      "Collections: 44|336\n",
      "Collections: 45|336\n",
      "Collections: 46|336\n",
      "Collections: 47|336\n",
      "Collections: 48|336\n",
      "Collections: 49|336\n",
      "Collections: 50|336\n",
      "Collections: 51|336\n",
      "Collections: 52|336\n",
      "Collections: 53|336\n",
      "Collections: 54|336\n",
      "Collections: 55|336\n",
      "Collections: 56|336\n",
      "Collections: 57|336\n",
      "Collections: 58|336\n",
      "Collections: 59|336\n",
      "Collections: 60|336\n",
      "Collections: 61|336\n",
      "Collections: 62|336\n",
      "Collections: 63|336\n",
      "Collections: 64|336\n",
      "Collections: 65|336\n",
      "Collections: 66|336\n",
      "Collections: 67|336\n",
      "Collections: 68|336\n",
      "Collections: 69|336\n",
      "Collections: 70|336\n",
      "Collections: 71|336\n",
      "Collections: 72|336\n",
      "Collections: 73|336\n",
      "Collections: 74|336\n",
      "Collections: 75|336\n",
      "Collections: 76|336\n",
      "Collections: 77|336\n",
      "Collections: 78|336\n",
      "Collections: 79|336\n",
      "Collections: 80|336\n",
      "Collections: 81|336\n",
      "Collections: 82|336\n",
      "Collections: 83|336\n",
      "Collections: 84|336\n",
      "Collections: 85|336\n",
      "Collections: 86|336\n",
      "Collections: 87|336\n",
      "Collections: 88|336\n",
      "Collections: 89|336\n",
      "Collections: 90|336\n",
      "Collections: 91|336\n",
      "Collections: 92|336\n",
      "Collections: 93|336\n",
      "Collections: 94|336\n",
      "Collections: 95|336\n",
      "Collections: 96|336\n",
      "Collections: 97|336\n",
      "Collections: 98|336\n",
      "Collections: 99|336\n",
      "Collections: 100|336\n",
      "Collections: 101|336\n",
      "Collections: 102|336\n",
      "Collections: 103|336\n",
      "Collections: 104|336\n",
      "Collections: 105|336\n",
      "Collections: 106|336\n",
      "Collections: 107|336\n",
      "Collections: 108|336\n",
      "Collections: 109|336\n",
      "Collections: 110|336\n",
      "Collections: 111|336\n",
      "Collections: 112|336\n",
      "Collections: 113|336\n",
      "Collections: 114|336\n",
      "Collections: 115|336\n",
      "Collections: 116|336\n",
      "Collections: 117|336\n",
      "Collections: 118|336\n",
      "Collections: 119|336\n",
      "Collections: 120|336\n",
      "Collections: 121|336\n",
      "Collections: 122|336\n",
      "Collections: 123|336\n",
      "Collections: 124|336\n",
      "Collections: 125|336\n",
      "Collections: 126|336\n",
      "Collections: 127|336\n",
      "Collections: 128|336\n",
      "Collections: 129|336\n",
      "Collections: 130|336\n",
      "Collections: 131|336\n",
      "Collections: 132|336\n",
      "Collections: 133|336\n",
      "Collections: 134|336\n",
      "Collections: 135|336\n",
      "Collections: 136|336\n",
      "Collections: 137|336\n",
      "Collections: 138|336\n",
      "Collections: 139|336\n",
      "Collections: 140|336\n",
      "Collections: 141|336\n",
      "Collections: 142|336\n",
      "Collections: 143|336\n",
      "Collections: 144|336\n",
      "Collections: 145|336\n",
      "Collections: 146|336\n",
      "Collections: 147|336\n",
      "Collections: 148|336\n",
      "Collections: 149|336\n",
      "Collections: 150|336\n",
      "Collections: 151|336\n",
      "Collections: 152|336\n",
      "Collections: 153|336\n",
      "Collections: 154|336\n",
      "Collections: 155|336\n",
      "Collections: 156|336\n",
      "Collections: 157|336\n",
      "Collections: 158|336\n",
      "Collections: 159|336\n",
      "Collections: 160|336\n",
      "Collections: 161|336\n",
      "Collections: 162|336\n",
      "Collections: 163|336\n",
      "Collections: 164|336\n",
      "Collections: 165|336\n",
      "Collections: 166|336\n",
      "Collections: 167|336\n",
      "Collections: 168|336\n",
      "Collections: 169|336\n",
      "Collections: 170|336\n",
      "Collections: 171|336\n",
      "Collections: 172|336\n",
      "Collections: 173|336\n",
      "Collections: 174|336\n",
      "Collections: 175|336\n",
      "Collections: 176|336\n",
      "Collections: 177|336\n",
      "Collections: 178|336\n",
      "Collections: 179|336\n",
      "Collections: 180|336\n",
      "Collections: 181|336\n",
      "Collections: 182|336\n",
      "Collections: 183|336\n",
      "Collections: 184|336\n",
      "Collections: 185|336\n",
      "Collections: 186|336\n",
      "Collections: 187|336\n",
      "Collections: 188|336\n",
      "Collections: 189|336\n",
      "Collections: 190|336\n",
      "Collections: 191|336\n",
      "Collections: 192|336\n",
      "Collections: 193|336\n",
      "Collections: 194|336\n",
      "Collections: 195|336\n",
      "Collections: 196|336\n",
      "Collections: 197|336\n",
      "Collections: 198|336\n",
      "Collections: 199|336\n",
      "Collections: 200|336\n",
      "Collections: 201|336\n",
      "Collections: 202|336\n",
      "Collections: 203|336\n",
      "Collections: 204|336\n",
      "Collections: 205|336\n",
      "Collections: 206|336\n",
      "Collections: 207|336\n",
      "Collections: 208|336\n",
      "Collections: 209|336\n",
      "Collections: 210|336\n",
      "Collections: 211|336\n",
      "Collections: 212|336\n",
      "Collections: 213|336\n",
      "Collections: 214|336\n",
      "Collections: 215|336\n",
      "Collections: 216|336\n",
      "Collections: 217|336\n",
      "Collections: 218|336\n",
      "Collections: 219|336\n",
      "Collections: 220|336\n",
      "Collections: 221|336\n",
      "Collections: 222|336\n",
      "Collections: 223|336\n",
      "Collections: 224|336\n",
      "Collections: 225|336\n",
      "Collections: 226|336\n",
      "Collections: 227|336\n",
      "Collections: 228|336\n",
      "Collections: 229|336\n",
      "Collections: 230|336\n",
      "Collections: 231|336\n",
      "Collections: 232|336\n",
      "Collections: 233|336\n",
      "Collections: 234|336\n",
      "Collections: 235|336\n",
      "Collections: 236|336\n",
      "Collections: 237|336\n",
      "Collections: 238|336\n",
      "Collections: 239|336\n",
      "Collections: 240|336\n",
      "Collections: 241|336\n",
      "Collections: 242|336\n",
      "Collections: 243|336\n",
      "Collections: 244|336\n",
      "Collections: 245|336\n",
      "Collections: 246|336\n",
      "Collections: 247|336\n",
      "Collections: 248|336\n",
      "Collections: 249|336\n",
      "Collections: 250|336\n",
      "Collections: 251|336\n",
      "Collections: 252|336\n",
      "Collections: 253|336\n",
      "Collections: 254|336\n",
      "Collections: 255|336\n",
      "Collections: 256|336\n",
      "Collections: 257|336\n",
      "Collections: 258|336\n",
      "Collections: 259|336\n",
      "Collections: 260|336\n",
      "Collections: 261|336\n",
      "Collections: 262|336\n",
      "Collections: 263|336\n",
      "Collections: 264|336\n",
      "Collections: 265|336\n",
      "Collections: 266|336\n",
      "Collections: 267|336\n",
      "Collections: 268|336\n",
      "Collections: 269|336\n",
      "Collections: 270|336\n",
      "Collections: 271|336\n",
      "Collections: 272|336\n",
      "Collections: 273|336\n",
      "Collections: 274|336\n",
      "Collections: 275|336\n",
      "Collections: 276|336\n",
      "Collections: 277|336\n",
      "Collections: 278|336\n",
      "Collections: 279|336\n",
      "Collections: 280|336\n",
      "Collections: 281|336\n",
      "Collections: 282|336\n",
      "Collections: 283|336\n",
      "Collections: 284|336\n",
      "Collections: 285|336\n",
      "Collections: 286|336\n",
      "Collections: 287|336\n",
      "Collections: 288|336\n",
      "Collections: 289|336\n",
      "Collections: 290|336\n",
      "Collections: 291|336\n",
      "Collections: 292|336\n",
      "Collections: 293|336\n",
      "Collections: 294|336\n",
      "Collections: 295|336\n",
      "Collections: 296|336\n",
      "Collections: 297|336\n",
      "Collections: 298|336\n",
      "Collections: 299|336\n",
      "Collections: 300|336\n",
      "Collections: 301|336\n",
      "Collections: 302|336\n",
      "Collections: 303|336\n",
      "Collections: 304|336\n",
      "Collections: 305|336\n",
      "Collections: 306|336\n",
      "Collections: 307|336\n",
      "Collections: 308|336\n",
      "Collections: 309|336\n",
      "Collections: 310|336\n",
      "Collections: 311|336\n",
      "Collections: 312|336\n",
      "Collections: 313|336\n",
      "Collections: 314|336\n",
      "Collections: 315|336\n",
      "Collections: 316|336\n",
      "Collections: 317|336\n",
      "Collections: 318|336\n",
      "Collections: 319|336\n",
      "Collections: 320|336\n",
      "Collections: 321|336\n",
      "Collections: 322|336\n",
      "Collections: 323|336\n",
      "Collections: 324|336\n",
      "Collections: 325|336\n",
      "Collections: 326|336\n",
      "Collections: 327|336\n",
      "Collections: 328|336\n",
      "Collections: 329|336\n",
      "Collections: 330|336\n",
      "Collections: 331|336\n",
      "Collections: 332|336\n",
      "Collections: 333|336\n",
      "Collections: 334|336\n",
      "Collections: 335|336\n",
      "Collections: 336|336\n",
      "Databases: 3|4\n",
      "Collections: 1|1534\n",
      "Collections: 2|1534\n",
      "Collections: 3|1534\n",
      "Collections: 4|1534\n",
      "Collections: 5|1534\n",
      "Collections: 6|1534\n",
      "Collections: 7|1534\n",
      "Collections: 8|1534\n",
      "Collections: 9|1534\n",
      "Collections: 10|1534\n",
      "Collections: 11|1534\n",
      "Collections: 12|1534\n",
      "Collections: 13|1534\n",
      "Collections: 14|1534\n",
      "Collections: 15|1534\n",
      "Collections: 16|1534\n",
      "Collections: 17|1534\n",
      "Collections: 18|1534\n",
      "Collections: 19|1534\n",
      "Collections: 20|1534\n",
      "Collections: 21|1534\n",
      "Collections: 22|1534\n",
      "Collections: 23|1534\n",
      "Collections: 24|1534\n",
      "Collections: 25|1534\n",
      "Collections: 26|1534\n",
      "Collections: 27|1534\n",
      "Collections: 28|1534\n",
      "Collections: 29|1534\n",
      "Collections: 30|1534\n",
      "Collections: 31|1534\n",
      "Collections: 32|1534\n",
      "Collections: 33|1534\n",
      "Collections: 34|1534\n",
      "Collections: 35|1534\n",
      "Collections: 36|1534\n",
      "Collections: 37|1534\n",
      "Collections: 38|1534\n",
      "Collections: 39|1534\n",
      "Collections: 40|1534\n",
      "Collections: 41|1534\n",
      "Collections: 42|1534\n",
      "Collections: 43|1534\n",
      "Collections: 44|1534\n",
      "Collections: 45|1534\n",
      "Collections: 46|1534\n",
      "Collections: 47|1534\n",
      "Collections: 48|1534\n",
      "Collections: 49|1534\n",
      "Collections: 50|1534\n",
      "Collections: 51|1534\n",
      "Collections: 52|1534\n",
      "Collections: 53|1534\n",
      "Collections: 54|1534\n",
      "Collections: 55|1534\n",
      "Collections: 56|1534\n",
      "Collections: 57|1534\n",
      "Collections: 58|1534\n",
      "Collections: 59|1534\n",
      "Collections: 60|1534\n",
      "Collections: 61|1534\n",
      "Collections: 62|1534\n",
      "Collections: 63|1534\n",
      "Collections: 64|1534\n",
      "Collections: 65|1534\n",
      "Collections: 66|1534\n",
      "Collections: 67|1534\n",
      "Collections: 68|1534\n",
      "Collections: 69|1534\n",
      "Collections: 70|1534\n",
      "Collections: 71|1534\n",
      "Collections: 72|1534\n",
      "Collections: 73|1534\n",
      "Collections: 74|1534\n",
      "Collections: 75|1534\n",
      "Collections: 76|1534\n",
      "Collections: 77|1534\n",
      "Collections: 78|1534\n",
      "Collections: 79|1534\n",
      "Collections: 80|1534\n",
      "Collections: 81|1534\n",
      "Collections: 82|1534\n",
      "Collections: 83|1534\n",
      "Collections: 84|1534\n",
      "Collections: 85|1534\n",
      "Collections: 86|1534\n",
      "Collections: 87|1534\n",
      "Collections: 88|1534\n",
      "Collections: 89|1534\n",
      "Collections: 90|1534\n",
      "Collections: 91|1534\n",
      "Collections: 92|1534\n",
      "Collections: 93|1534\n",
      "Collections: 94|1534\n",
      "Collections: 95|1534\n",
      "Collections: 96|1534\n",
      "Collections: 97|1534\n",
      "Collections: 98|1534\n",
      "Collections: 99|1534\n",
      "Collections: 100|1534\n",
      "Collections: 101|1534\n",
      "Collections: 102|1534\n",
      "Collections: 103|1534\n",
      "Collections: 104|1534\n",
      "Collections: 105|1534\n",
      "Collections: 106|1534\n",
      "Collections: 107|1534\n",
      "Collections: 108|1534\n",
      "Collections: 109|1534\n",
      "Collections: 110|1534\n",
      "Collections: 111|1534\n",
      "Collections: 112|1534\n",
      "Collections: 113|1534\n",
      "Collections: 114|1534\n",
      "Collections: 115|1534\n",
      "Collections: 116|1534\n",
      "Collections: 117|1534\n",
      "Collections: 118|1534\n",
      "Collections: 119|1534\n",
      "Collections: 120|1534\n",
      "Collections: 121|1534\n",
      "Collections: 122|1534\n",
      "Collections: 123|1534\n",
      "Collections: 124|1534\n",
      "Collections: 125|1534\n",
      "Collections: 126|1534\n",
      "Collections: 127|1534\n",
      "Collections: 128|1534\n",
      "Collections: 129|1534\n",
      "Collections: 130|1534\n",
      "Collections: 131|1534\n",
      "Collections: 132|1534\n",
      "Collections: 133|1534\n",
      "Collections: 134|1534\n",
      "Collections: 135|1534\n",
      "Collections: 136|1534\n",
      "Collections: 137|1534\n",
      "Collections: 138|1534\n",
      "Collections: 139|1534\n",
      "Collections: 140|1534\n",
      "Collections: 141|1534\n",
      "Collections: 142|1534\n",
      "Collections: 143|1534\n",
      "Collections: 144|1534\n",
      "Collections: 145|1534\n",
      "Collections: 146|1534\n",
      "Collections: 147|1534\n",
      "Collections: 148|1534\n",
      "Collections: 149|1534\n",
      "Collections: 150|1534\n",
      "Collections: 151|1534\n",
      "Collections: 152|1534\n",
      "Collections: 153|1534\n",
      "Collections: 154|1534\n",
      "Collections: 155|1534\n",
      "Collections: 156|1534\n",
      "Collections: 157|1534\n",
      "Collections: 158|1534\n",
      "Collections: 159|1534\n",
      "Collections: 160|1534\n",
      "Collections: 161|1534\n",
      "Collections: 162|1534\n",
      "Collections: 163|1534\n",
      "Collections: 164|1534\n",
      "Collections: 165|1534\n",
      "Collections: 166|1534\n",
      "Collections: 167|1534\n",
      "Collections: 168|1534\n",
      "Collections: 169|1534\n",
      "Collections: 170|1534\n",
      "Collections: 171|1534\n",
      "Collections: 172|1534\n",
      "Collections: 173|1534\n",
      "Collections: 174|1534\n",
      "Collections: 175|1534\n",
      "Collections: 176|1534\n",
      "Collections: 177|1534\n",
      "Collections: 178|1534\n",
      "Collections: 179|1534\n",
      "Collections: 180|1534\n",
      "Collections: 181|1534\n",
      "Collections: 182|1534\n",
      "Collections: 183|1534\n",
      "Collections: 184|1534\n",
      "Collections: 185|1534\n",
      "Collections: 186|1534\n",
      "Collections: 187|1534\n",
      "Collections: 188|1534\n",
      "Collections: 189|1534\n",
      "Collections: 190|1534\n",
      "Collections: 191|1534\n",
      "Collections: 192|1534\n",
      "Collections: 193|1534\n",
      "Collections: 194|1534\n",
      "Collections: 195|1534\n",
      "Collections: 196|1534\n",
      "Collections: 197|1534\n",
      "Collections: 198|1534\n",
      "Collections: 199|1534\n",
      "Collections: 200|1534\n",
      "Collections: 201|1534\n",
      "Collections: 202|1534\n",
      "Collections: 203|1534\n",
      "Collections: 204|1534\n",
      "Collections: 205|1534\n",
      "Collections: 206|1534\n",
      "Collections: 207|1534\n",
      "Collections: 208|1534\n",
      "Collections: 209|1534\n",
      "Collections: 210|1534\n",
      "Collections: 211|1534\n",
      "Collections: 212|1534\n",
      "Collections: 213|1534\n",
      "Collections: 214|1534\n",
      "Collections: 215|1534\n",
      "Collections: 216|1534\n",
      "Collections: 217|1534\n",
      "Collections: 218|1534\n",
      "Collections: 219|1534\n",
      "Collections: 220|1534\n",
      "Collections: 221|1534\n",
      "Collections: 222|1534\n",
      "Collections: 223|1534\n",
      "Collections: 224|1534\n",
      "Collections: 225|1534\n",
      "Collections: 226|1534\n",
      "Collections: 227|1534\n",
      "Collections: 228|1534\n",
      "Collections: 229|1534\n",
      "Collections: 230|1534\n",
      "Collections: 231|1534\n",
      "Collections: 232|1534\n",
      "Collections: 233|1534\n",
      "Collections: 234|1534\n",
      "Collections: 235|1534\n",
      "Collections: 236|1534\n",
      "Collections: 237|1534\n",
      "Collections: 238|1534\n",
      "Collections: 239|1534\n",
      "Collections: 240|1534\n",
      "Collections: 241|1534\n",
      "Collections: 242|1534\n",
      "Collections: 243|1534\n",
      "Collections: 244|1534\n",
      "Collections: 245|1534\n",
      "Collections: 246|1534\n",
      "Collections: 247|1534\n",
      "Collections: 248|1534\n",
      "Collections: 249|1534\n",
      "Collections: 250|1534\n",
      "Collections: 251|1534\n",
      "Collections: 252|1534\n",
      "Collections: 253|1534\n",
      "Collections: 254|1534\n",
      "Collections: 255|1534\n",
      "Collections: 256|1534\n",
      "Collections: 257|1534\n",
      "Collections: 258|1534\n",
      "Collections: 259|1534\n",
      "Collections: 260|1534\n",
      "Collections: 261|1534\n",
      "Collections: 262|1534\n",
      "Collections: 263|1534\n",
      "Collections: 264|1534\n",
      "Collections: 265|1534\n",
      "Collections: 266|1534\n",
      "Collections: 267|1534\n",
      "Collections: 268|1534\n",
      "Collections: 269|1534\n",
      "Collections: 270|1534\n",
      "Collections: 271|1534\n",
      "Collections: 272|1534\n",
      "Collections: 273|1534\n",
      "Collections: 274|1534\n",
      "Collections: 275|1534\n",
      "Collections: 276|1534\n",
      "Collections: 277|1534\n",
      "Collections: 278|1534\n",
      "Collections: 279|1534\n",
      "Collections: 280|1534\n",
      "Collections: 281|1534\n",
      "Collections: 282|1534\n",
      "Collections: 283|1534\n",
      "Collections: 284|1534\n",
      "Collections: 285|1534\n",
      "Collections: 286|1534\n",
      "Collections: 287|1534\n",
      "Collections: 288|1534\n",
      "Collections: 289|1534\n",
      "Collections: 290|1534\n",
      "Collections: 291|1534\n",
      "Collections: 292|1534\n",
      "Collections: 293|1534\n",
      "Collections: 294|1534\n",
      "Collections: 295|1534\n",
      "Collections: 296|1534\n",
      "Collections: 297|1534\n",
      "Collections: 298|1534\n",
      "Collections: 299|1534\n",
      "Collections: 300|1534\n",
      "Collections: 301|1534\n",
      "Collections: 302|1534\n",
      "Collections: 303|1534\n",
      "Collections: 304|1534\n",
      "Collections: 305|1534\n",
      "Collections: 306|1534\n",
      "Collections: 307|1534\n",
      "Collections: 308|1534\n",
      "Collections: 309|1534\n",
      "Collections: 310|1534\n",
      "Collections: 311|1534\n",
      "Collections: 312|1534\n",
      "Collections: 313|1534\n",
      "Collections: 314|1534\n",
      "Collections: 315|1534\n",
      "Collections: 316|1534\n",
      "Collections: 317|1534\n",
      "Collections: 318|1534\n",
      "Collections: 319|1534\n",
      "Collections: 320|1534\n",
      "Collections: 321|1534\n",
      "Collections: 322|1534\n",
      "Collections: 323|1534\n",
      "Collections: 324|1534\n",
      "Collections: 325|1534\n",
      "Collections: 326|1534\n",
      "Collections: 327|1534\n",
      "Collections: 328|1534\n",
      "Collections: 329|1534\n",
      "Collections: 330|1534\n",
      "Collections: 331|1534\n",
      "Collections: 332|1534\n",
      "Collections: 333|1534\n",
      "Collections: 334|1534\n",
      "Collections: 335|1534\n",
      "Collections: 336|1534\n",
      "Collections: 337|1534\n",
      "Collections: 338|1534\n",
      "Collections: 339|1534\n",
      "Collections: 340|1534\n",
      "Collections: 341|1534\n",
      "Collections: 342|1534\n",
      "Collections: 343|1534\n",
      "Collections: 344|1534\n",
      "Collections: 345|1534\n",
      "Collections: 346|1534\n",
      "Collections: 347|1534\n",
      "Collections: 348|1534\n",
      "Collections: 349|1534\n",
      "Collections: 350|1534\n",
      "Collections: 351|1534\n",
      "Collections: 352|1534\n",
      "Collections: 353|1534\n",
      "Collections: 354|1534\n",
      "Collections: 355|1534\n",
      "Collections: 356|1534\n",
      "Collections: 357|1534\n",
      "Collections: 358|1534\n",
      "Collections: 359|1534\n",
      "Collections: 360|1534\n",
      "Collections: 361|1534\n",
      "Collections: 362|1534\n",
      "Collections: 363|1534\n",
      "Collections: 364|1534\n",
      "Collections: 365|1534\n",
      "Collections: 366|1534\n",
      "Collections: 367|1534\n",
      "Collections: 368|1534\n",
      "Collections: 369|1534\n",
      "Collections: 370|1534\n",
      "Collections: 371|1534\n",
      "Collections: 372|1534\n",
      "Collections: 373|1534\n",
      "Collections: 374|1534\n",
      "Collections: 375|1534\n",
      "Collections: 376|1534\n",
      "Collections: 377|1534\n",
      "Collections: 378|1534\n",
      "Collections: 379|1534\n",
      "Collections: 380|1534\n",
      "Collections: 381|1534\n",
      "Collections: 382|1534\n",
      "Collections: 383|1534\n",
      "Collections: 384|1534\n",
      "Collections: 385|1534\n",
      "Collections: 386|1534\n",
      "Collections: 387|1534\n",
      "Collections: 388|1534\n",
      "Collections: 389|1534\n",
      "Collections: 390|1534\n",
      "Collections: 391|1534\n",
      "Collections: 392|1534\n",
      "Collections: 393|1534\n",
      "Collections: 394|1534\n",
      "Collections: 395|1534\n",
      "Collections: 396|1534\n",
      "Collections: 397|1534\n",
      "Collections: 398|1534\n",
      "Collections: 399|1534\n",
      "Collections: 400|1534\n",
      "Collections: 401|1534\n",
      "Collections: 402|1534\n",
      "Collections: 403|1534\n",
      "Collections: 404|1534\n",
      "Collections: 405|1534\n",
      "Collections: 406|1534\n",
      "Collections: 407|1534\n",
      "Collections: 408|1534\n",
      "Collections: 409|1534\n",
      "Collections: 410|1534\n",
      "Collections: 411|1534\n",
      "Collections: 412|1534\n",
      "Collections: 413|1534\n",
      "Collections: 414|1534\n",
      "Collections: 415|1534\n",
      "Collections: 416|1534\n",
      "Collections: 417|1534\n",
      "Collections: 418|1534\n",
      "Collections: 419|1534\n",
      "Collections: 420|1534\n",
      "Collections: 421|1534\n",
      "Collections: 422|1534\n",
      "Collections: 423|1534\n",
      "Collections: 424|1534\n",
      "Collections: 425|1534\n",
      "Collections: 426|1534\n",
      "Collections: 427|1534\n",
      "Collections: 428|1534\n",
      "Collections: 429|1534\n",
      "Collections: 430|1534\n",
      "Collections: 431|1534\n",
      "Collections: 432|1534\n",
      "Collections: 433|1534\n",
      "Collections: 434|1534\n",
      "Collections: 435|1534\n",
      "Collections: 436|1534\n",
      "Collections: 437|1534\n",
      "Collections: 438|1534\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1409376 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstore_keywords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminio_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mminio_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeili_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmeili_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_documents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstored_documents\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 79\u001b[0m, in \u001b[0;36mstore_keywords\u001b[0;34m(minio_client, meili_client, storage_documents)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_identity \u001b[38;5;129;01min\u001b[39;00m document_identities:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m document_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_document_keywords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_database\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_collection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_index\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m database_keywords\u001b[38;5;241m.\u001b[39mappend(document_keywords)\n\u001b[1;32m     88\u001b[0m document_identities\u001b[38;5;241m.\u001b[39mappend(document_identity)\n",
      "Cell \u001b[0;32mIn[68], line 21\u001b[0m, in \u001b[0;36mcreate_document_keywords\u001b[0;34m(document_database, document_collection, document, document_index)\u001b[0m\n\u001b[1;32m     18\u001b[0m document_data \u001b[38;5;241m=\u001b[39m document[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m document_type \u001b[38;5;241m=\u001b[39m document[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m document_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mget_document_keywords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_data\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m keyword_uuid \u001b[38;5;241m=\u001b[39m generate_keyword_uuid(\n\u001b[1;32m     26\u001b[0m     document_id \u001b[38;5;241m=\u001b[39m document_id,\n\u001b[1;32m     27\u001b[0m     document_index \u001b[38;5;241m=\u001b[39m document_index\n\u001b[1;32m     28\u001b[0m ) \n\u001b[1;32m     30\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: keyword_uuid,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m'\u001b[39m: document_database,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m: document_keywords\n\u001b[1;32m     37\u001b[0m }\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mget_document_keywords\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_document_keywords\u001b[39m(\n\u001b[1;32m      6\u001b[0m     document: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      7\u001b[0m ):\n\u001b[0;32m----> 8\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m         token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop               \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m                  \n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(keywords))\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/spacy/language.py:1040\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1025\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/spacy/language.py:1131\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/spacy/language.py:1120\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1121\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1409376 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "store_keywords(\n",
    "    minio_client = minio_client,\n",
    "    meili_client = meili_client,\n",
    "    storage_documents = stored_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a484c6-7032-4878-8f8a-b5d466089bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88b4c8f0-1390-4c4a-a6c9-21411d839c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskInfo(task_uid=32, index_uid='llm-rag-code-keywords', status='enqueued', type='indexDeletion', enqueued_at=datetime.datetime(2024, 11, 1, 13, 29, 58, 264698))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meili_client.index('llm-rag-code-keywords').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340932c-f57d-464c-9d0e-0baa4c20077f",
   "metadata": {},
   "source": [
    "## Hybrid Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd2a9e-4a99-45b2-83cb-da2f9895c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prompt(\n",
    "    prompt: str\n",
    ") -> any:\n",
    "    prompt = prompt.lower()\n",
    "    prompt = re.sub(r'\\s+', ' ', prompt)\n",
    "    prompt = re.sub(r'[^\\w\\s]', '', prompt)\n",
    "    return prompt.strip()\n",
    "\n",
    "def generate_prompt_embedding_query(\n",
    "    model_name: str,\n",
    "    prompt: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    embedding = embedding_model.embed_documents(\n",
    "        texts = [prompt]\n",
    "    )\n",
    "    return embedding[0]\n",
    "\n",
    "def spacy_find_keywords(\n",
    "    text: str\n",
    "):\n",
    "    formatted = nlp(text.lower())\n",
    "    \n",
    "    keywords = [\n",
    "        token.lemma_ for token in formatted\n",
    "        if not token.is_stop               \n",
    "        and not token.is_punct              \n",
    "        and not token.is_space              \n",
    "        and len(token) > 1                  \n",
    "    ]\n",
    "    \n",
    "    keywords = list(set(keywords))\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def generate_prompt_keyword_query(\n",
    "    prompt: any\n",
    ") -> any:\n",
    "    keywords = spacy_find_keywords(\n",
    "        text = prompt\n",
    "    )\n",
    "    keyword_query = ' OR '.join([f'keywords = \"{keyword}\"' for keyword in keywords])\n",
    "    return keyword_query\n",
    "\n",
    "def calculate_keyword_score(\n",
    "    keyword_query: str,\n",
    "    keyword_list: any\n",
    ") -> any:\n",
    "    match = 0\n",
    "    asked_keywords = keyword_query.split('OR')\n",
    "    for asked_keyword in asked_keywords:\n",
    "        formatted = asked_keyword.replace('keywords =', '')\n",
    "        formatted = formatted.replace('\"', '')\n",
    "        formatted = formatted.replace(' ', '')\n",
    "        \n",
    "        if formatted in keyword_list:\n",
    "            match += 1\n",
    "            \n",
    "    query_length = len(asked_keywords)\n",
    "    keyword_length = len(keyword_list)\n",
    "\n",
    "    if match == 0:\n",
    "        return 0.0\n",
    "\n",
    "    normalized = match / ((query_length * keyword_length) ** 0.5)\n",
    "    return normalized\n",
    "    \n",
    "def vector_search_collection(\n",
    "    vector_client: any,\n",
    "    search_client: any,\n",
    "    prompt: str,\n",
    "    top_k: int\n",
    "):\n",
    "\n",
    "    cleaned_prompt = clean_prompt(\n",
    "        prompt = prompt\n",
    "    )\n",
    "\n",
    "    prompt_embedding_query = generate_prompt_embedding_query(\n",
    "        model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        prompt = cleaned_prompt\n",
    "    )\n",
    "\n",
    "    prompt_keyword_query = generate_prompt_keyword_query(\n",
    "        prompt = cleaned_prompt\n",
    "    )\n",
    "\n",
    "    qdrant_collections = [\n",
    "        'llm-rag-code-embeddings',\n",
    "        'llm-rag-workflows-embeddings'\n",
    "    ]\n",
    "    \n",
    "    recommeded_cases = []\n",
    "    for collection in qdrant_collections:\n",
    "        results = qdrant_search_vectors(\n",
    "            qdrant_client = vector_client,  \n",
    "            collection_name = collection,\n",
    "            query_vector = prompt_embedding_query,\n",
    "            limit = top_k\n",
    "        ) \n",
    "        \n",
    "        for result in results:\n",
    "            res_database = result.payload['database']\n",
    "            res_collection = result.payload['collection']\n",
    "            res_document = result.payload['document']\n",
    "            res_type = result.payload['type']\n",
    "            res_score = result.score\n",
    "            \n",
    "            res_case = {\n",
    "                'source': 'vector',\n",
    "                'database': res_database,\n",
    "                'collection': res_collection,\n",
    "                'document': res_document,\n",
    "                'type': res_type,\n",
    "                'score': res_score\n",
    "            }\n",
    "            \n",
    "            recommeded_cases.append(res_case)\n",
    "            \n",
    "    meili_collections = [\n",
    "        'llm-rag-code-keywords',\n",
    "        'llm-rag-workflows-keywords'\n",
    "    ]\n",
    "\n",
    "    recommeded_keyword_cases = []\n",
    "    for index in meili_collections:\n",
    "        results = meili_search_documents(\n",
    "            meili_client = search_client, \n",
    "            index_name = index, \n",
    "            query = \"\", \n",
    "            options = {\n",
    "                'filter': prompt_keyword_query,\n",
    "                'attributesToRetrieve': ['database','collection','document', 'keywords'],\n",
    "                'limit': top_k\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for result in results['hits']:\n",
    "            res_database = result['database']\n",
    "            res_collection = result['collection']\n",
    "            res_document = result['document']\n",
    "            res_keywords = result['keywords']\n",
    "            \n",
    "            res_score = calculate_keyword_score(\n",
    "                keyword_query = prompt_keyword_query,\n",
    "                keyword_list = res_keywords\n",
    "            )\n",
    "\n",
    "            res_case = {\n",
    "                'source': 'search',\n",
    "                'database': res_database,\n",
    "                'collection': res_collection,\n",
    "                'document': res_document,\n",
    "                'type': res_type,\n",
    "                'score': res_score\n",
    "            }\n",
    "\n",
    "            recommeded_cases.append(res_case)\n",
    "\n",
    "    return recommeded_cases\n",
    "\n",
    "def get_top_document_metadata(\n",
    "    collection: str,\n",
    "    alpha: float\n",
    ") -> any:\n",
    "    df = pd.DataFrame(collection)\n",
    "    ids_with_both = df.groupby('document')['source'].nunique()\n",
    "    ids_with_both = ids_with_both[ids_with_both > 1].index\n",
    "    filtered_df = df[df['document'].isin(ids_with_both)]\n",
    "\n",
    "    matched_documents = []\n",
    "    for index_i, row_i in filtered_df[filtered_df['source'] == 'vector'].iterrows():\n",
    "        vector_source = row_i['source']\n",
    "        vector_database = row_i['database']\n",
    "        vector_collection = row_i['collection']\n",
    "        vector_id = row_i['document']\n",
    "        vector_type = row_i['type']\n",
    "        vector_score = row_i['score']\n",
    "        \n",
    "        for index_j, row_j in filtered_df[filtered_df['source'] == 'search'].iterrows():\n",
    "            search_source = row_j['source']\n",
    "            search_database = row_j['database']\n",
    "            search_collection = row_j['collection']\n",
    "            search_id = row_j['document']\n",
    "            search_type = row_j['type']\n",
    "            search_score = row_j['score']\n",
    "            \n",
    "            if vector_database == search_database:\n",
    "                if vector_collection == search_collection:\n",
    "                    if vector_type == search_type:\n",
    "                        if vector_id == search_id:\n",
    "                            hybrid_score = vector_score * alpha + search_score * (1-alpha)\n",
    "    \n",
    "                            matched_documents.append({\n",
    "                                'source': 'hybrid',\n",
    "                                'database': search_database,\n",
    "                                'collection': search_collection,\n",
    "                                'document': search_id,\n",
    "                                'score': hybrid_score\n",
    "                            })\n",
    "    \n",
    "    match_df = pd.DataFrame(matched_documents)\n",
    "    print(match_df)\n",
    "    return match_df.nlargest(1, 'score').values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9dd18-5f6a-4869-a606-b8ed73cbdde9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
