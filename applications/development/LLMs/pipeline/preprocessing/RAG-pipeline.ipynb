{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76734520-9ec6-4cb9-bed5-477006377137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df5dcf8-2d82-40da-8914-9ce8aef3b84f",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7f7b3f-6c35-47f0-8ba4-f0853ab66d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import Config,RepositoryEnv\n",
    "env_path = '/home/sfniila/.ssh/.env'\n",
    "env_config = Config(RepositoryEnv(env_path))\n",
    "github_token = env_config.get('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4314a1-75b3-4a21-93ea-da45671295c9",
   "metadata": {},
   "source": [
    "## PyGitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b07a4fb-3e77-4418-900a-00355b551a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "def pygithub_get_repo_paths(\n",
    "    token: str,\n",
    "    owner: str, \n",
    "    name: str\n",
    ") -> any:\n",
    "    g = Github(token)\n",
    "    repo = g.get_repo(f\"{owner}/{name}\")\n",
    "    contents = repo.get_contents(\"\")\n",
    "    paths = []\n",
    "    while len(contents) > 0:\n",
    "      file_content = contents.pop(0)\n",
    "      if file_content.type == 'dir':\n",
    "        contents.extend(repo.get_contents(file_content.path))\n",
    "      else:\n",
    "        paths.append(file_content.path)\n",
    "    g.close()\n",
    "    return paths\n",
    "\n",
    "def pygithub_get_path_content(\n",
    "    token: str,\n",
    "    owner: str, \n",
    "    name: str, \n",
    "    path: str\n",
    ") -> any:\n",
    "    g = Github(token)\n",
    "    repo = g.get_repo(f\"{owner}/{name}\")\n",
    "    file_content = repo.get_contents(path)\n",
    "    content = file_content.decoded_content.decode('utf-8')\n",
    "    # fails on some yaml files\n",
    "    # deployment/kubeflow/manifests/contrib/ray/kuberay-operator/base/resources.yaml\n",
    "    # unsupported encoding: none\n",
    "    g.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d75007-8269-450f-a54b-c485a324949e",
   "metadata": {},
   "source": [
    "## Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2f0cd3-106c-4b5d-9926-40dd191ee231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_github_repo_documents(\n",
    "    github_token: str,\n",
    "    repo_owner: str,\n",
    "    repo_name: str,\n",
    "    relevant_files: any,\n",
    "    store: bool\n",
    ") -> any:\n",
    "    storage_path = os.getcwd() + '/' + repo_owner + '-' + repo_name + '.txt' \n",
    "\n",
    "    repo_paths = []\n",
    "    if not os.path.exists(storage_path):\n",
    "        print('Getting github paths')\n",
    "        repo_paths = pygithub_get_repo_paths(\n",
    "            token = github_token,\n",
    "            owner = repo_owner, \n",
    "            name = repo_name\n",
    "        )\n",
    "        \n",
    "        if store:\n",
    "            print('Storing paths')\n",
    "            path_amount = len(repo_paths)\n",
    "            i = 0\n",
    "            with open(storage_path, 'w') as file:\n",
    "                for line in repo_paths:\n",
    "                    row = line \n",
    "                    if i < path_amount:\n",
    "                        row += '\\n'\n",
    "                    file.write(row)\n",
    "                    i += 1\n",
    "            print('Paths stored')\n",
    "    else:\n",
    "        print('Getting stored paths')\n",
    "        with open(storage_path, 'r') as file:\n",
    "            repo_paths = file.readlines()\n",
    "    print('Paths fetched')\n",
    "                \n",
    "    print('Filtering paths')\n",
    "    relevant_paths = []\n",
    "    for path in repo_paths:\n",
    "        path_split = path.split('/')\n",
    "        file_end = path_split[-1].split('.')[-1].rstrip()\n",
    "        if file_end in relevant_files:\n",
    "            relevant_paths.append(path.rstrip())\n",
    "    print('Paths filtered')\n",
    "\n",
    "    formatted_paths = {\n",
    "        'paths': relevant_paths\n",
    "    }\n",
    "\n",
    "    # consider how to store this to allas\n",
    "    return formatted_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee717787-4d21-4082-ad61-5b1973926cc9",
   "metadata": {},
   "source": [
    "Files of intrest\n",
    "- md = important to usesr\n",
    "- txt = isn't important to users\n",
    "- sh = isn't important to users\n",
    "- yaml = important to developers\n",
    "- py = important to users and developers\n",
    "- ipynb = important to users and developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ccfa42-95dc-4c8a-9784-fa703937d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stored paths\n",
      "Paths fetched\n",
      "Filtering paths\n",
      "Paths filtered\n"
     ]
    }
   ],
   "source": [
    "repository_paths = get_github_repo_documents(\n",
    "    github_token = github_token,\n",
    "    repo_owner = 'K123AsJ0k1',\n",
    "    repo_name = 'cloud-hpc-oss-mlops-platform',\n",
    "    relevant_files = [\n",
    "        'md',\n",
    "        'yaml',\n",
    "        'py',\n",
    "        'ipynb'\n",
    "    ],\n",
    "    store = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70f6e8-ed91-45c9-afa5-cf598998ac17",
   "metadata": {},
   "source": [
    "# Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfc42e-02e2-4d94-b7f9-9782228cc44d",
   "metadata": {},
   "source": [
    "## Mongo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e4c444-5608-4dde-bf09-c0364a1b8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient as mc\n",
    "\n",
    "def mongo_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    return isinstance(storage_client, mc.Connection)\n",
    "\n",
    "def mongo_setup_client(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    address: str,\n",
    "    port: str\n",
    ") -> any:\n",
    "    connection_prefix = 'mongodb://(username):(password)@(address):(port)/'\n",
    "    connection_address = connection_prefix.replace('(username)', username)\n",
    "    connection_address = connection_address.replace('(password)', password)\n",
    "    connection_address = connection_address.replace('(address)', address)\n",
    "    connection_address = connection_address.replace('(port)', port)\n",
    "    mongo_client = mc(\n",
    "        host = connection_address\n",
    "    )\n",
    "    return mongo_client\n",
    "\n",
    "def mongo_get_database(\n",
    "    mongo_client: any,\n",
    "    database_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        return database\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_check_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database_exists = database_name in mongo_client.list_database_names()\n",
    "        return database_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_list_databases(\n",
    "    mongo_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        databases = mongo_client.list_database_names()\n",
    "        return databases\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_database(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        mongo_client.drop_database(database_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_get_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collection = database[collection_name]\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def mongo_check_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_client[database_name]\n",
    "        collection_exists = collection_name in database.list_collection_names()\n",
    "        return collection_exists\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_update_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_many(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_list_collections(\n",
    "    mongo_client: any, \n",
    "    database_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        collections = database.list_collection_names()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_remove_collection(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try: \n",
    "        database = mongo_get_database(\n",
    "            mongo_client = mongo_client,\n",
    "            database_name = database_name\n",
    "        )\n",
    "        database.drop_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def mongo_create_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    document: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.insert_one(document)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_get_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    "):\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        document = collection.find_one(filter_query)\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None \n",
    "\n",
    "def mongo_list_documents(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any,\n",
    "    sorting_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        documents = list(collection.find(filter_query).sort(sorting_query))\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def mongo_update_document(\n",
    "    mongo_client: any, \n",
    "    database_name: any, \n",
    "    collection_name: any, \n",
    "    filter_query: any, \n",
    "    update_query: any\n",
    ") -> any:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.update_one(filter_query, update_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mongo_remove_document(\n",
    "    mongo_client: any, \n",
    "    database_name: str, \n",
    "    collection_name: str, \n",
    "    filter_query: any\n",
    ") -> bool:\n",
    "    try: \n",
    "        collection = mongo_get_collection(\n",
    "            mongo_client = mongo_client, \n",
    "            database_name = database_name, \n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        result = collection.delete_one(filter_query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116c18c-53f1-4222-986f-6e3fa00e65b2",
   "metadata": {},
   "source": [
    "## Markdown documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec230be-1fbe-4b25-90da-078055b830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def create_markdown_documents(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    \n",
    "    documents = []\n",
    "    document = ''\n",
    "    index = 1\n",
    "    for element in soup.descendants:\n",
    "        if element.name in ['h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            text = element.get_text(strip = True)\n",
    "            if not document == '':\n",
    "                document = document.replace('\\n', '')\n",
    "                if not len(document.split()) == 1:\n",
    "                    documents.append({\n",
    "                        'index': index,\n",
    "                        'sub-index': 0,\n",
    "                        'type': 'markdown',\n",
    "                        'data': document\n",
    "                    })\n",
    "                    index += 1\n",
    "                document = ''\n",
    "            document += text\n",
    "        elif element.name == 'p':\n",
    "            text = element.get_text(strip = True)\n",
    "            text = re.sub(code_block_pattern, '', text)\n",
    "            text = text.rstrip('\\n')\n",
    "            text = text.replace('\\nsh', '')\n",
    "            text = text.replace('\\nbash', '')\n",
    "            document += ' ' + text\n",
    "        elif element.name in ['ul', 'ol']:\n",
    "            text = ''\n",
    "            for li in element.find_all('li'):\n",
    "                item = li.get_text(strip=True)\n",
    "                if not '-' in item:\n",
    "                    text += '-' + item\n",
    "                    continue\n",
    "                text += item\n",
    "            document += ' ' + text\n",
    "            \n",
    "    documents.append({\n",
    "        'index': index,\n",
    "        'sub-index': 0,\n",
    "        'type': 'markdown',\n",
    "        'data': document\n",
    "    })\n",
    "    \n",
    "    formatted_documents = {\n",
    "        'text': documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d185d0f-0003-4958-a026-0b79c6c62333",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = repository_paths[7]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c4d08e0-6f5e-41f1-a465-e9829d6cb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_documents = create_markdown_documents(\n",
    "    markdown_text = example_data\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f93ac-63d8-4b9f-8467-e202571ce0aa",
   "metadata": {},
   "source": [
    "## YAML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b019e009-db94-47b4-9021-9ab915c437b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "def extract_yaml_values(\n",
    "    section: any,\n",
    "    path: str,\n",
    "    values: any\n",
    ") -> any:\n",
    "    for key, value in section.items():\n",
    "        if path == '':\n",
    "            current_path = key\n",
    "        else:\n",
    "            current_path = path + '/' + key\n",
    "        if isinstance(value, dict):\n",
    "            extract_yaml_values(\n",
    "                section = value,\n",
    "                path = current_path,\n",
    "                values = values\n",
    "            )\n",
    "        if isinstance(value, list):\n",
    "            number = 1\n",
    "            \n",
    "            for case in value:\n",
    "                base_path = current_path\n",
    "                if isinstance(case, dict):\n",
    "                   extract_yaml_values(\n",
    "                       section = case,\n",
    "                       path = current_path,\n",
    "                       values = values\n",
    "                   ) \n",
    "                   continue\n",
    "                base_path += '/' + str(number)\n",
    "                number += 1\n",
    "                values.append(base_path + '=' + str(case))\n",
    "        else:\n",
    "            if isinstance(value, dict):\n",
    "                continue\n",
    "            if isinstance(value, list):\n",
    "                continue\n",
    "            values.append(current_path + '=' + str(value))\n",
    "            \n",
    "    return values\n",
    "\n",
    "def create_yaml_documents(\n",
    "    yaml_text: any\n",
    ") -> any:\n",
    "    # has problems with some yaml files\n",
    "    # unsupported encoding: none\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
    "    # 'list' object has no attribute 'items'\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
    "    # 'NoneType' object has no attribute 'items'\n",
    "    # deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/base/metadata/overlays/db/kustomization.yaml\n",
    "    #   could not determine a constructor for the tag 'tag:yaml.org,2002:value'\n",
    "    # in \"<unicode string>\", line 41, column 18:\n",
    "    #      delimiter: =\n",
    "    yaml_data = list(yaml.safe_load_all(yaml_text))\n",
    "\n",
    "    documents = []\n",
    "    index = 1\n",
    "    for data in yaml_data:\n",
    "        yaml_values = extract_yaml_values(\n",
    "            section = data,\n",
    "            path = '',\n",
    "            values = []\n",
    "        )\n",
    "\n",
    "        previous_root = ''\n",
    "        document = ''\n",
    "        sub_index = 1\n",
    "        for value in yaml_values:\n",
    "            equal_split = value.split('=')\n",
    "            path_split = equal_split[0].split('/')\n",
    "            root = path_split[0]\n",
    "            if not root == previous_root:\n",
    "                if 0 < len(document):\n",
    "                    documents.append({\n",
    "                        'index': index,\n",
    "                        'sub-index': sub_index,\n",
    "                        'type': 'yaml',\n",
    "                        'data': document\n",
    "                    })\n",
    "                    sub_index += 1\n",
    "                    \n",
    "                previous_root = root\n",
    "                document = value\n",
    "            else:\n",
    "                document += value\n",
    "                \n",
    "        documents.append({\n",
    "            'index': index,\n",
    "            'sub-index': sub_index,\n",
    "            'type': 'yaml',\n",
    "            'data': document\n",
    "        })\n",
    "        index += 1\n",
    "\n",
    "    formatted_documents = {\n",
    "        'text': documents\n",
    "    }\n",
    "            \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "239ffb67-18ec-4a3e-a8ca-57fa1364ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_yaml = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'deployment/forwarder/celery/celery-deployment.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48880c03-8a81-46a5-b207-f0c583e5e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_documents = create_yaml_documents(\n",
    "    yaml_text = example_yaml\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10990a57-ea53-4e0e-9c53-d620811f5b95",
   "metadata": {},
   "source": [
    "## TreeSitter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43c68780-ac71-4ede-84fb-713a7cf56da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "import re\n",
    "\n",
    "def tree_extract_imports(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    imports = []\n",
    "    if node.type == 'import_statement' or node.type == 'import_from_statement':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        imports.append(code_text[start_byte:end_byte].decode('utf8'))\n",
    "    for child in node.children:\n",
    "        imports.extend(tree_extract_imports(child, code_text))\n",
    "    return imports\n",
    "\n",
    "def tree_extract_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    dependencies = []\n",
    "    for child in node.children:\n",
    "        if child.type == 'call':\n",
    "            dependency_name = child.child_by_field_name('function').text.decode('utf8')\n",
    "            dependencies.append(dependency_name)\n",
    "        dependencies.extend(tree_extract_dependencies(child, code_text))\n",
    "    return dependencies\n",
    "\n",
    "def tree_extract_code_and_dependencies(\n",
    "    node: any,\n",
    "    code_text: str\n",
    ") -> any:\n",
    "    codes = []\n",
    "    if not node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name')\n",
    "        if name is None:\n",
    "            code = code_text[start_byte:end_byte].decode('utf8')\n",
    "            if not 'def' in code:\n",
    "                dependencies = tree_extract_dependencies(node, code_text)\n",
    "                codes.append({\n",
    "                    'name': 'global',\n",
    "                    'code': code,\n",
    "                    'dependencies': dependencies\n",
    "                })\n",
    "    return codes\n",
    "\n",
    "def tree_extract_functions_and_dependencies(\n",
    "    node: any, \n",
    "    code_text: str\n",
    ") -> any:\n",
    "    functions = []\n",
    "    if node.type == 'function_definition':\n",
    "        start_byte = node.start_byte\n",
    "        end_byte = node.end_byte\n",
    "        name = node.child_by_field_name('name').text.decode('utf8')\n",
    "        code = code_text[start_byte:end_byte].decode('utf8')\n",
    "        dependencies = tree_extract_dependencies(node, code_text)\n",
    "        functions.append({\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'dependencies': dependencies\n",
    "        })\n",
    "    for child in node.children:\n",
    "        functions.extend(tree_extract_functions_and_dependencies(child, code_text))\n",
    "    return functions\n",
    "\n",
    "def tree_get_used_imports(\n",
    "    general_imports: any,\n",
    "    function_dependencies: any\n",
    ") -> any:\n",
    "    parsed_imports = {}\n",
    "    for code_import in general_imports:\n",
    "        import_factors = code_import.split('import')[-1].replace(' ', '')\n",
    "        import_factors = import_factors.split(',')\n",
    "    \n",
    "        for factor in import_factors:\n",
    "            if not factor in parsed_imports:\n",
    "                parsed_imports[factor] = code_import.split('import')[0] + 'import ' + factor\n",
    "            \n",
    "    relevant_imports = {}\n",
    "    for dependency in function_dependencies:\n",
    "        initial_term = dependency.split('.')[0]\n",
    "    \n",
    "        if not initial_term in relevant_imports:\n",
    "            if initial_term in parsed_imports:\n",
    "                relevant_imports[initial_term] = parsed_imports[initial_term]\n",
    "    \n",
    "    used_imports = []\n",
    "    for name, code in relevant_imports.items():\n",
    "        used_imports.append(code)\n",
    "\n",
    "    return used_imports\n",
    "\n",
    "def tree_get_used_functions(\n",
    "    general_functions: any,\n",
    "    function_dependencies: any\n",
    "): \n",
    "    used_functions = []\n",
    "    for related_function_name in function_dependencies:\n",
    "        for function in general_functions:\n",
    "            if function['name'] == related_function_name:\n",
    "                used_functions.append('from ice import ' + function['name'])\n",
    "    return used_functions\n",
    "\n",
    "def tree_create_code_document(\n",
    "    code_imports: any,\n",
    "    code_functions: any,\n",
    "    function_item: any\n",
    ") -> any:\n",
    "    used_imports = tree_get_used_imports(\n",
    "        general_imports = code_imports,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "\n",
    "    used_functions = tree_get_used_functions(\n",
    "        general_functions = code_functions,\n",
    "        function_dependencies = function_item['dependencies']\n",
    "    )\n",
    "    \n",
    "    document = {\n",
    "        'imports': used_imports,\n",
    "        'functions': used_functions,\n",
    "        'name': function_item['name'],\n",
    "        'dependencies': function_item['dependencies'],\n",
    "        'code': function_item['code']\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "     \n",
    "def tree_format_code_document(\n",
    "    code_document: any\n",
    ") -> any:\n",
    "    formatted_document = ''\n",
    "    for doc_import in code_document['imports']:\n",
    "        formatted_document += doc_import + '\\n'\n",
    "\n",
    "    for doc_functions in code_document['functions']:\n",
    "        formatted_document += doc_functions + '\\n'\n",
    "\n",
    "    if 0 < len(code_document['dependencies']):\n",
    "        formatted_document += 'code dependencies\\n'\n",
    "\n",
    "        for doc_dependency in code_document['dependencies']:\n",
    "            formatted_document += doc_dependency + '\\n'\n",
    "\n",
    "    if code_document['name'] == 'global':\n",
    "        formatted_document += code_document['name'] + ' code\\n'\n",
    "    else:\n",
    "        formatted_document += 'function ' + code_document['name'] + ' code\\n'\n",
    "    \n",
    "    for line in code_document['code'].splitlines():\n",
    "        if not bool(line.strip()):\n",
    "            continue\n",
    "        doc_code = re.sub(r'#.*','', line)\n",
    "        if not bool(doc_code.strip()):\n",
    "            continue\n",
    "        formatted_document += doc_code + '\\n'    \n",
    "    return formatted_document\n",
    "\n",
    "def tree_create_python_code_and_function_documents(\n",
    "    code_document: any\n",
    "):\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "   \n",
    "    tree = parser.parse(\n",
    "        bytes(\n",
    "            code_document,\n",
    "            \"utf8\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    code_imports = tree_extract_imports(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_global = tree_extract_code_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    code_functions = tree_extract_functions_and_dependencies(\n",
    "        root_node, \n",
    "        bytes(\n",
    "            code_document, \n",
    "            'utf8'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    initial_documents = []\n",
    "    for item in code_global:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    for item in code_functions:\n",
    "        document = tree_create_code_document(\n",
    "            code_imports = code_imports,\n",
    "            code_functions = code_functions,\n",
    "            function_item = item\n",
    "        )  \n",
    "        initial_documents.append(document)\n",
    "\n",
    "    formatted_documents = []\n",
    "    seen_functions = []\n",
    "    for document in initial_documents:\n",
    "        if not document['name'] == 'global':\n",
    "            if document['name'] in seen_functions:\n",
    "                continue\n",
    "        \n",
    "        formatted_document = tree_format_code_document(\n",
    "            code_document = document\n",
    "        )\n",
    "\n",
    "        formatted_documents.append(formatted_document)\n",
    "        seen_functions.append(document['name'])\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46127d-3124-4243-a7fa-77dae3531c91",
   "metadata": {},
   "source": [
    "## Python documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42502b34-833a-41a5-8eb8-d6b52bb98330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_python_documents(\n",
    "    python_text: any\n",
    "): \n",
    "    joined_code = ''.join(python_text)\n",
    "    block_code_documents = tree_create_python_code_and_function_documents(\n",
    "        code_document = joined_code\n",
    "    )\n",
    "\n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    code_doc_index = 0\n",
    "    for code_doc in block_code_documents:\n",
    "        row_split = code_doc.split('\\n')\n",
    "        for row in row_split:\n",
    "            if 'function' in row and 'code' in row:\n",
    "                # This causes problems with some documents\n",
    "                # list index out of range\n",
    "                function_name = row.split(' ')[1]\n",
    "                if not function_name in seen_function_names:\n",
    "                    seen_function_names.append(function_name)\n",
    "                else:\n",
    "                    del block_code_documents[code_doc_index]\n",
    "        code_doc_index += 1\n",
    "\n",
    "    if 0 < len(block_code_documents):\n",
    "        index = 1\n",
    "        for code_doc in block_code_documents:\n",
    "            code_documents.append({\n",
    "                'index': index,\n",
    "                'sub-index': 0,\n",
    "                'type': 'python',\n",
    "                'data': code_doc\n",
    "            })\n",
    "            index += 1\n",
    "        \n",
    "    formatted_documents = {\n",
    "        'code': code_documents\n",
    "    }\n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7426485-07ce-46ea-8d2b-05b598f3c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_python = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'applications/article/submitter/backend/functions/platforms/flower.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e688f33c-7429-458b-8983-994918e8537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_documents = create_python_documents(\n",
    "    python_text = example_python\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0259e6-052b-4e49-b73b-bd95fd6d3afe",
   "metadata": {},
   "source": [
    "## Notebook documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55ecc3b9-29af-42e9-acad-27f50bd8dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "\n",
    "def parse_jupyter_notebook_markdown_into_text(\n",
    "    markdown_text: any\n",
    ") -> any:\n",
    "    html = markdown.markdown(markdown_text)\n",
    "    soup = BeautifulSoup(html, features='html.parser')\n",
    "    text = soup.get_text()\n",
    "    code_block_pattern = re.compile(r\"```\")\n",
    "    text = re.sub(code_block_pattern, '', text)\n",
    "    text = text.rstrip('\\n')\n",
    "    text = text.replace('\\nsh', '\\n')\n",
    "    text = text.replace('\\nbash', '\\n')\n",
    "    return text\n",
    "\n",
    "def extract_jupyter_notebook_markdown_and_code(\n",
    "    notebook_text: any\n",
    "): \n",
    "    notebook_documents = {\n",
    "        'markdown': [],\n",
    "        'code': []\n",
    "    }\n",
    "\n",
    "    notebook = nbformat.reads(notebook_text, as_version=2)\n",
    "    index = 1\n",
    "    for cell in notebook.worksheets[0].cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            notebook_documents['markdown'].append({\n",
    "                'id': index,\n",
    "                'data': cell.source\n",
    "            })\n",
    "            index += 1\n",
    "        if cell.cell_type == 'code':\n",
    "            notebook_documents['code'].append({\n",
    "                'id': index,\n",
    "                'data': cell.input\n",
    "            })\n",
    "            index += 1\n",
    "    \n",
    "    return notebook_documents\n",
    "\n",
    "def create_notebook_documents(\n",
    "    notebook_text: any\n",
    "):\n",
    "    notebook_documents = extract_jupyter_notebook_markdown_and_code(\n",
    "        notebook_text = notebook_text\n",
    "    )\n",
    "\n",
    "    markdown_documents = []\n",
    "    for block in notebook_documents['markdown']:\n",
    "        joined_text = ''.join(block['data'])\n",
    "        markdown_text = parse_jupyter_notebook_markdown_into_text(\n",
    "            markdown_text = joined_text\n",
    "        )\n",
    "        markdown_documents.append({\n",
    "            'index': block['id'],\n",
    "            'sub-index': 0,\n",
    "            'type': 'markdown',\n",
    "            'data': markdown_text\n",
    "        })\n",
    "        \n",
    "    code_documents = []\n",
    "    seen_function_names = []\n",
    "    for block in notebook_documents['code']:\n",
    "        joined_code = ''.join(block['data'])\n",
    "        block_code_documents = tree_create_python_code_and_function_documents(\n",
    "            code_document = joined_code\n",
    "        )\n",
    "\n",
    "        code_doc_index = 0\n",
    "        for code_doc in block_code_documents:\n",
    "            row_split = code_doc.split('\\n')\n",
    "            for row in row_split:\n",
    "                if 'function' in row and 'code' in row:\n",
    "                    # This causes problems with some documents\n",
    "                    # list index out of range\n",
    "                    function_name = row.split(' ')[1]\n",
    "                    if not function_name in seen_function_names:\n",
    "                        seen_function_names.append(function_name)\n",
    "                    else:\n",
    "                        del block_code_documents[code_doc_index]\n",
    "            code_doc_index += 1\n",
    "        \n",
    "        if 0 < len(block_code_documents):\n",
    "            sub_indexes = False\n",
    "            if 1 < len(block_code_documents):\n",
    "                sub_indexes = True\n",
    "            index = 1\n",
    "            for code_doc in block_code_documents:\n",
    "                if sub_indexes:\n",
    "                    code_documents.append({\n",
    "                        'index': block['id'],\n",
    "                        'sub-index': index, \n",
    "                        'type': 'python',\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                else:\n",
    "                    code_documents.append({ \n",
    "                        'index': block['id'],\n",
    "                        'sub-index': 0,\n",
    "                        'type': 'python',\n",
    "                        'data': code_doc\n",
    "                    })\n",
    "                index += 1\n",
    "            \n",
    "    formatted_documents = {\n",
    "        'text': markdown_documents,\n",
    "        'code': code_documents\n",
    "    }\n",
    "    \n",
    "    return formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a060a79-f17c-45c6-b81d-ad36d0c0965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_notebook = pygithub_get_path_content(\n",
    "    token = github_token,\n",
    "    owner = 'K123AsJ0k1', \n",
    "    name = 'cloud-hpc-oss-mlops-platform', \n",
    "    path = 'tutorials/cloud_hpc/Cloud-HPC-FMNIST.ipynb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2fb19ea7-3ddc-4fce-99e9-6d37c21f60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_documents = create_notebook_documents(\n",
    "    notebook_text = example_notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c944d67-e224-49b5-a01f-d348deb398ae",
   "metadata": {},
   "source": [
    "## Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb12557-3abf-4426-adfd-d42a4706067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = mongo_setup_client(\n",
    "    username = 'mongo123',\n",
    "    password = 'mongo456',\n",
    "    address = '127.0.0.1',\n",
    "    port = '27017'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe860a31-582b-4fa7-b8bf-73e2ca5919ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_repository_path_documents(\n",
    "    mongo_client: any,\n",
    "    github_token: any,\n",
    "    repository_owner: str,\n",
    "    repository_name: str,\n",
    "    repository_path: str,\n",
    "    database_name: str,\n",
    "    collection_name: str\n",
    "):\n",
    "    collection_exists = mongo_check_collection(\n",
    "        mongo_client = mongo_client, \n",
    "        database_name = database_name, \n",
    "        collection_name = collection_name\n",
    "    )\n",
    "\n",
    "    if collection_exists:\n",
    "        return False\n",
    "\n",
    "    target_content = ''\n",
    "    try:\n",
    "        target_content = pygithub_get_path_content(\n",
    "            token = github_token,\n",
    "            owner = repository_owner, \n",
    "            name = repository_name, \n",
    "            path = repository_path\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(repository_path)\n",
    "        print(e)\n",
    "\n",
    "    if target_content == '':\n",
    "        return False\n",
    "\n",
    "    path_split = repository_path.split('/')\n",
    "    target_type = path_split[-1].split('.')[-1]\n",
    "    \n",
    "    target_documents = {}\n",
    "    if target_type == 'md':\n",
    "        try:\n",
    "            target_documents = create_markdown_documents(\n",
    "                markdown_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'yaml':\n",
    "        try:\n",
    "            target_documents = create_yaml_documents(\n",
    "                yaml_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'py':\n",
    "        try:\n",
    "            target_documents = create_python_documents(\n",
    "                python_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if target_type == 'ipynb':\n",
    "        try:\n",
    "            target_documents = create_notebook_documents(\n",
    "                notebook_text = target_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(repository_path)\n",
    "            print(e)\n",
    "    if 0 < len(target_documents):\n",
    "        for doc_type, docs in target_documents.items():\n",
    "            for document in docs:\n",
    "                result = mongo_create_document(\n",
    "                    mongo_client = mongo_client,\n",
    "                    database_name = database_name,\n",
    "                    collection_name = collection_name,\n",
    "                    document = document\n",
    "                )\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_github_storage_prefix(\n",
    "    repository_owner: str,\n",
    "    repository_name: str\n",
    ") -> str:\n",
    "    return repository_owner + '|' + repository_name + '|'\n",
    "\n",
    "def store_github_repository_documents(\n",
    "    mongo_client: any,\n",
    "    github_token: str,\n",
    "    repository_owner: str,\n",
    "    repository_name: str,\n",
    "    repository_paths: any\n",
    ") -> any:\n",
    "    print('Storing paths')\n",
    "    paths = repository_paths['paths']\n",
    "    for path in paths:\n",
    "        path_split = path.split('/')\n",
    "        document_database_name = get_github_storage_prefix(repository_owner, repository_name) + path_split[-1].split('.')[-1]\n",
    "        \n",
    "        document_collection_name = ''\n",
    "        for word in path_split[:-1]:\n",
    "            document_collection_name += word[:2] + '|'\n",
    "        document_collection_name += path_split[-1].split('.')[0]\n",
    "\n",
    "        stored = store_repository_path_documents(\n",
    "            mongo_client = mongo_client,\n",
    "            github_token = github_token,\n",
    "            repository_owner = repository_owner,\n",
    "            repository_name = repository_name,\n",
    "            repository_path = path,\n",
    "            database_name = document_database_name,\n",
    "            collection_name = document_collection_name\n",
    "        )\n",
    "    print('Paths stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e25a1e1c-3177-41b7-95c7-e0c10f11ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing paths\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG-Development.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG-pipeline.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/RAG_Preprocessing.ipynb\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/documents.py\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/python_parsing.py\n",
      "list index out of range\n",
      "applications/development/LLMs/pipeline/preprocessing/scripts/platforms/tree.py\n",
      "list index out of range\n",
      "deployment/kubeflow/manifests/contrib/ray/kuberay-operator/base/resources.yaml\n",
      "unsupported encoding: none\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/eventing-core.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/in-memory-channel.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-eventing/base/upstream/mt-channel-broker.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-serving/base/upstream/net-istio.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/common/knative/knative-serving/base/upstream/serving-core.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-openshift/patches/service-serving-cert.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-openshift/patches/webhook-inject-cabundle.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-with-kubeflow/patches/enable-ui-authz-checks.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/katib/upstream/installs/katib-with-kubeflow/patches/ui-rbac.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/openshift-pipelines-custom-task/pipelineloop-controller-patch.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/openshift-pipelines-custom-task/pipelineloop-webhook-patch.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/contrib/kserve/models-web-app/overlays/kubeflow/patches/web-app-vsvc.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/base/metadata/overlays/db/kustomization.yaml\n",
      "could not determine a constructor for the tag 'tag:yaml.org,2002:value'\n",
      "  in \"<unicode string>\", line 41, column 18:\n",
      "          delimiter: =\n",
      "                     ^\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-dashboard/tekton-dashboard-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/pipeline/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/pipeline/upstream/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/argo/upstream/manifests/namespace-install/overlays/argo-server-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/argo/upstream/manifests/namespace-install/overlays/workflow-controller-deployment.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-dashboard/tekton-dashboard-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-controller.yaml\n",
      "'list' object has no attribute 'items'\n",
      "deployment/kubeflow/manifests/apps/kfp-tekton/upstream/v1/third-party/tekton/upstream/manifests/base/tektoncd-install/tekton-release.yaml\n",
      "'NoneType' object has no attribute 'items'\n",
      "Paths stored\n"
     ]
    }
   ],
   "source": [
    "store_github_repository_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    github_token = github_token,\n",
    "    repository_owner = 'K123AsJ0k1', \n",
    "    repository_name = 'cloud-hpc-oss-mlops-platform', \n",
    "    repository_paths = repository_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b4bc4d1-9cd2-489c-8167-027e82ea42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING, DESCENDING\n",
    "    \n",
    "def get_stored_documents(\n",
    "    mongo_client: any,\n",
    "    database_prefix: str\n",
    ") -> any:\n",
    "    storage_structure = {}\n",
    "    database_list = mongo_list_databases(\n",
    "        mongo_client = mongo_client\n",
    "    )\n",
    "    for database_name in database_list:\n",
    "        if database_prefix in database_name:\n",
    "            collection_list = mongo_list_collections(\n",
    "                mongo_client = mongo_client,\n",
    "                database_name = database_name\n",
    "            )\n",
    "            storage_structure[database_name] = collection_list\n",
    "    \n",
    "    storage_documents = {}\n",
    "    for database_name, collections in storage_structure.items():\n",
    "        if not database_name in storage_documents:\n",
    "            storage_documents[database_name] = {}\n",
    "        for collection_name in collections:\n",
    "            collection_documents = mongo_list_documents(\n",
    "                mongo_client = mongo_client,\n",
    "                database_name = database_name,\n",
    "                collection_name = collection_name,\n",
    "                filter_query = {},\n",
    "                sorting_query = [\n",
    "                    ('index', ASCENDING),\n",
    "                    ('sub-index', ASCENDING)\n",
    "                ]\n",
    "            )\n",
    "            storage_documents[database_name][collection_name] = collection_documents\n",
    "            \n",
    "    return storage_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed0eee5-bdd0-46b5-8a25-c06f0f5eb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_documents = get_stored_documents(\n",
    "    mongo_client = mongo_client,\n",
    "    database_prefix = get_github_storage_prefix('K123AsJ0k1', 'cloud-hpc-oss-mlops-platform')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea094ed-9324-48ce-b7c3-df51219b44b9",
   "metadata": {},
   "source": [
    "## MinIO Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "922a6270-3c62-4dec-b4d6-ed7b2ef2c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "\n",
    "def create_bucket(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str\n",
    ") -> bool:\n",
    "    MINIO_CLIENT = minio_client \n",
    "    try:\n",
    "        MINIO_CLIENT.make_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO bucket creation error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "    \n",
    "def check_bucket(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        status = MINIO_CLIENT.bucket_exists(bucket_name = bucket_name)\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO bucket checking error')\n",
    "        logger.error(e)\n",
    "        return False \n",
    "       \n",
    "def delete_bucket(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        MINIO_CLIENT.remove_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO bucket deletion error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "# Works\n",
    "def create_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool: \n",
    "    # Be aware that MinIO objects have a size limit of 1GB, \n",
    "    # which might result to large header error\n",
    "    MINIO_CLIENT = minio_client\n",
    "    \n",
    "    pickled_data = pickle.dumps(data)\n",
    "    length = len(pickled_data)\n",
    "    buffer = io.BytesIO()\n",
    "    buffer.write(pickled_data)\n",
    "    buffer.seek(0)\n",
    "    try:\n",
    "        MINIO_CLIENT.put_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path + '.pkl',\n",
    "            data = buffer,\n",
    "            length = length,\n",
    "            metadata = metadata\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO object creation error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "# Works\n",
    "def check_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        object_info = MINIO_CLIENT.stat_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path + '.pkl'\n",
    "        )      \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False \n",
    "# Works\n",
    "def delete_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        MINIO_CLIENT.remove_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path + '.pkl'\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error('MinIO object deletion error')\n",
    "        logger.error(e)\n",
    "        return False\n",
    "# Works\n",
    "def update_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool:  \n",
    "    remove = delete_object(logger,minio_client,bucket_name, object_path)\n",
    "    if remove:\n",
    "        create = create_object(logger,minio_client, bucket_name, object_path, data, metadata)\n",
    "        if create:\n",
    "            return True\n",
    "    return False\n",
    "# works\n",
    "def create_or_update_object(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any, \n",
    "    metadata: dict\n",
    ") -> any:\n",
    "    bucket_status = check_bucket(logger,minio_client,bucket_name)\n",
    "    if not bucket_status:\n",
    "        creation_status = create_bucket(logger,minio_client,bucket_name)\n",
    "        if not creation_status:\n",
    "            return None\n",
    "    object_status = check_object(logger,minio_client,bucket_name, object_path)\n",
    "    if not object_status:\n",
    "        return create_object(logger,minio_client,bucket_name, object_path, data, metadata)\n",
    "    else:\n",
    "        return update_object(logger,minio_client,bucket_name, object_path, data, metadata)\n",
    "# Works\n",
    "def get_object_data_and_metadata(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> dict:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    \n",
    "    try:\n",
    "        given_object_info = MINIO_CLIENT.stat_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path + '.pkl'\n",
    "        )\n",
    "        # There seems to be some kind of a limit\n",
    "        # with the amount of request a client \n",
    "        # can make, which is why this variable\n",
    "        # is set here to give more time got the client\n",
    "        # to complete the request\n",
    "        given_metadata = given_object_info.metadata\n",
    "        \n",
    "        given_object_data = MINIO_CLIENT.get_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path + '.pkl'\n",
    "        )\n",
    "        given_pickled_data = given_object_data.data\n",
    "        \n",
    "        try:\n",
    "            given_data = pickle.loads(given_pickled_data)\n",
    "            relevant_metadata = {} \n",
    "            for key, value in given_metadata.items():\n",
    "                if 'x-amz-meta' in key:\n",
    "                    key_name = key[11:]\n",
    "                    relevant_metadata[key_name] = value\n",
    "            return {'data': given_data, 'metadata': relevant_metadata}\n",
    "        except Exception as e:\n",
    "            logger.error('MinIO object pickle decoding error')\n",
    "            logger.error(e)\n",
    "            return None \n",
    "    except Exception as e:\n",
    "        logger.error('MinIO object fetching error')\n",
    "        logger.error(e)\n",
    "        return None\n",
    "# Works\n",
    "def get_object_list(\n",
    "    logger: any,\n",
    "    minio_client: any,\n",
    "    bucket_name: str,\n",
    "    path_prefix: str\n",
    ") -> dict:\n",
    "    MINIO_CLIENT = minio_client\n",
    "    try:\n",
    "        objects = MINIO_CLIENT.list_objects(bucket_name = bucket_name, prefix = path_prefix, recursive = True)\n",
    "        object_dict = {}\n",
    "        for obj in objects:\n",
    "            object_name = obj.object_name\n",
    "            object_info = MINIO_CLIENT.stat_object(\n",
    "                bucket_name = bucket_name,\n",
    "                object_name = object_name\n",
    "            )\n",
    "            given_metadata = {} \n",
    "            for key, value in object_info.metadata.items():\n",
    "                if 'X-Amz-Meta' in key:\n",
    "                    key_name = key[11:]\n",
    "                    given_metadata[key_name] = value\n",
    "            object_dict[obj.object_name] = given_metadata\n",
    "        return object_dict\n",
    "    except Exception as e:\n",
    "        return None  \n",
    "# Works\n",
    "def delete_objects(\n",
    "    logger: any,\n",
    "    minio_client: any, \n",
    "    bucket_name: str,\n",
    "    path_prefix: str\n",
    "):\n",
    "    objects = get_object_list(logger,minio_client,bucket_name, path_prefix)\n",
    "    for object_name in objects.keys():\n",
    "        pkl_split = object_name.split('.')[0]\n",
    "        delete_object(logger,minio_client,bucket_name,pkl_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee1a63-b54f-46f9-a7f8-fa1d4540e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "\n",
    "def is_minio_client(\n",
    "    storage_client: any\n",
    ") -> bool:\n",
    "    return isinstance(storage_client, Minio)\n",
    "\n",
    "def setup_minio(\n",
    "    endpoint: str,\n",
    "    username: str,\n",
    "    password: str\n",
    ") -> any:\n",
    "    minio_client = Minio(\n",
    "        endpoint = endpoint, \n",
    "        access_key = username, \n",
    "        secret_key = password,\n",
    "        secure = False\n",
    "    )\n",
    "    return minio_client\n",
    "\n",
    "def minio_create_bucket(\n",
    "    minio_client: any,\n",
    "    bucket_name: str\n",
    ") -> bool: \n",
    "    try:\n",
    "        minio_client.make_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO bucket creation error')\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def minio_check_bucket(\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    try:\n",
    "        status = minio_client.bucket_exists(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        print('MinIO bucket checking error')\n",
    "        print(e)\n",
    "        return False \n",
    "       \n",
    "def minio_delete_bucket(\n",
    "    minio_client: any,\n",
    "    bucket_name:str\n",
    ") -> bool:\n",
    "    try:\n",
    "        minio_client.remove_bucket(\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO bucket deletion error')\n",
    "        print(e)\n",
    "        return False\n",
    "# Works\n",
    "def minio_create_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool: \n",
    "    # Be aware that MinIO objects have a size limit of 1GB, \n",
    "    # which might result to large header error    \n",
    "    length = len(data)\n",
    "    try:\n",
    "        minio_client.put_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path,\n",
    "            data = data,\n",
    "            length = length,\n",
    "            metadata = metadata\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO object creation error')\n",
    "        print(e)\n",
    "        return False\n",
    "# Works\n",
    "def minio_check_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> any: \n",
    "    try:\n",
    "        object_info = minio_client.stat_object(\n",
    "            bucket_name = bucket_name,\n",
    "            object_name = object_path\n",
    "        )      \n",
    "        return object_info\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "# Works\n",
    "def minio_delete_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> bool: \n",
    "    try:\n",
    "        minio_client.remove_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('MinIO object deletion error')\n",
    "        print(e)\n",
    "        return False\n",
    "# Works\n",
    "def minio_update_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any,\n",
    "    metadata: dict\n",
    ") -> bool:  \n",
    "    remove = minio_delete_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = bucket_name,\n",
    "        object_path = object_path\n",
    "    )\n",
    "    if remove:\n",
    "        return minio_create_object(\n",
    "            minio_client = minio_client, \n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            data = data,\n",
    "            metadata = metadata\n",
    "        )\n",
    "    return False\n",
    "# works\n",
    "def minio_create_or_update_object(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str, \n",
    "    data: any, \n",
    "    metadata: dict\n",
    ") -> bool:\n",
    "    bucket_status = minio_check_bucket(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = bucket_name\n",
    "    )\n",
    "    if not bucket_status:\n",
    "        creation_status = minio_create_bucket(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = bucket_name\n",
    "        )\n",
    "        if not creation_status:\n",
    "            return False\n",
    "    object_status = minio_check_object(\n",
    "        minio_client = minio_client,\n",
    "        bucket_name = bucket_name, \n",
    "        object_path = object_path\n",
    "    )\n",
    "    if not object_status:\n",
    "        return minio_create_object(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            data = data, \n",
    "            metadata = metadata\n",
    "        )\n",
    "    else:\n",
    "        return minio_update_object(\n",
    "            minio_client = minio_client,\n",
    "            bucket_name = bucket_name, \n",
    "            object_path = object_path, \n",
    "            data = data, \n",
    "            metadata = metadata\n",
    "        )\n",
    "# Works\n",
    "def minio_get_object_list(\n",
    "    minio_client: any,\n",
    "    bucket_name: str,\n",
    "    path_prefix: str\n",
    ") -> any:\n",
    "    try:\n",
    "        objects = minio_client.list_objects(\n",
    "            bucket_name = bucket_name, \n",
    "            prefix = path_prefix, \n",
    "            recursive = True\n",
    "        )\n",
    "        return objects\n",
    "    except Exception as e:\n",
    "        return None  \n",
    "    \n",
    "def minio_get_object_data_and_metadata(\n",
    "    minio_client: any,\n",
    "    bucket_name: str, \n",
    "    object_path: str\n",
    ") -> any:\n",
    "    try:\n",
    "        given_object_data = minio_client.get_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path\n",
    "        )\n",
    "        \n",
    "        # There seems to be some kind of a limit\n",
    "        # with the amount of request a client \n",
    "        # can make, which is why this variable\n",
    "        # is set here to give more time got the client\n",
    "        # to complete the request\n",
    "        given_data = given_object_data.data\n",
    "\n",
    "        given_object_info = minio_client.stat_object(\n",
    "            bucket_name = bucket_name, \n",
    "            object_name = object_path\n",
    "        )\n",
    "        \n",
    "        given_metadata = given_object_info.metadata\n",
    "        \n",
    "        return {'data': given_data, 'metadata': given_metadata}\n",
    "    except Exception as e:\n",
    "        print.error('MinIO object fetching error')\n",
    "        print.error(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d75151-e190-4ce3-9e84-4bbdc38ae474",
   "metadata": {},
   "source": [
    "## Storing Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef29494-0db0-4796-921c-82d9d47376dd",
   "metadata": {},
   "source": [
    "## Langchain Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29be8d4a-44bd-418a-acee-c806074116c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def langchain_create_code_chunks(\n",
    "    language: any,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language = language,\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "\n",
    "    code_chunks = splitter.create_documents([document])\n",
    "    code_chunks = [doc.page_content for doc in code_chunks]\n",
    "    return code_chunks\n",
    "\n",
    "def lanchain_create_text_chunks(\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    document: any\n",
    ") -> any:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False\n",
    "    )\n",
    "\n",
    "    text_chunks = splitter.create_documents([document])\n",
    "    text_chunks = [doc.page_content for doc in text_chunks]\n",
    "    return text_chunks\n",
    "\n",
    "def langchain_create_chunk_embeddings(\n",
    "    model_name: str,\n",
    "    chunks: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    chunk_embeddings = embedding_model.embed_documents(\n",
    "        texts = chunks\n",
    "    )\n",
    "    return chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9aea4c-9ef7-401c-bf94-e8c94c88bbbe",
   "metadata": {},
   "source": [
    "## Qdrant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3293c5e1-03be-4329-8322-9888680392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient as qc\n",
    "from qdrant_client import models\n",
    "\n",
    "def qdrant_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, qc.Connection)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_setup_client(\n",
    "    api_key: str,\n",
    "    address: str, \n",
    "    port: str\n",
    ") -> any:\n",
    "    try:\n",
    "        qdrant_client = qc(\n",
    "            host = address,\n",
    "            port = int(port),\n",
    "            api_key = api_key,\n",
    "            https = False\n",
    "        ) \n",
    "        return qdrant_client\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_create_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str,\n",
    "    configuration: any\n",
    ") -> any:\n",
    "    try:\n",
    "        result = qdrant_client.create_collection(\n",
    "            collection_name = collection_name,\n",
    "            vectors_config = configuration\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_get_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(\n",
    "            collection_name = collection_name\n",
    "        )\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def qdrant_list_collections(\n",
    "    qdrant_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections()\n",
    "        collection_list = []\n",
    "        for description in collections.collections:\n",
    "            collection_list.append(description.name)\n",
    "        return collection_list\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_collection(\n",
    "    qdrant_client: any, \n",
    "    collection_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def qdrant_upsert_points(\n",
    "    qdrant_client: qc, \n",
    "    collection_name: str,\n",
    "    points: any\n",
    ") -> any:\n",
    "    try:\n",
    "        results = qdrant_client.upsert(\n",
    "            collection_name = collection_name, \n",
    "            points = points\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def qdrant_search_data(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    scroll_filter: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.scroll(\n",
    "            collection_name = collection_name,\n",
    "            scroll_filter = scroll_filter,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "def qdrant_search_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str,\n",
    "    query_vector: any,\n",
    "    limit: str\n",
    ") -> any:\n",
    "    try:\n",
    "        hits = qdrant_client.search(\n",
    "            collection_name = collection_name,\n",
    "            query_vector = query_vector,\n",
    "            limit = limit\n",
    "        )\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def qdrant_remove_vectors(\n",
    "    qdrant_client: qc,  \n",
    "    collection_name: str, \n",
    "    vectors: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        results = qdrant_client.delete_vectors(\n",
    "            collection_name = collection_name,\n",
    "            vectors = vectors\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing document: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dc47b-c7cf-406c-9846-7c53c89f7a29",
   "metadata": {},
   "source": [
    "## Vector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "126bd9db-f520-4bcc-8c06-de3af348e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import uuid\n",
    "import re\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def create_document_packet(\n",
    "    document: any,\n",
    "    configuration: any,\n",
    ") -> any:\n",
    "    #print('Create packet')\n",
    "    document_type = document['type']\n",
    "    used_configuration = configuration[document_type]\n",
    "    \n",
    "    document_chunks = []\n",
    "    if document_type == 'python':\n",
    "        document_chunks = langchain_create_code_chunks(\n",
    "            language = used_configuration['language'],\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    if document_type == 'text' or document_type == 'yaml' or document_type == 'markdown':\n",
    "        document_chunks = lanchain_create_text_chunks(\n",
    "            chunk_size = used_configuration['chunk-size'],\n",
    "            chunk_overlap = used_configuration['chunk-overlap'],\n",
    "            document = document['data']\n",
    "        )\n",
    "    # This needs to remove empty chunks\n",
    "    filtered_chunks = []\n",
    "    for chunk in document_chunks:\n",
    "        if chunk.strip():\n",
    "            filtered_chunks.append(chunk)\n",
    "        \n",
    "    vector_embedding = langchain_create_chunk_embeddings(\n",
    "        model_name = used_configuration['model-name'],\n",
    "        chunks = filtered_chunks\n",
    "    )\n",
    "\n",
    "    packet = {\n",
    "        'chunks': filtered_chunks,\n",
    "        'embeddings': vector_embedding\n",
    "    }\n",
    "    \n",
    "    return packet\n",
    "\n",
    "def format_chunk(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    chunk = re.sub(r'[^\\w\\s]', '', document_chunk)\n",
    "    chunk = re.sub(r'\\s+', ' ', chunk) \n",
    "    chunk = chunk.strip()\n",
    "    chunk = chunk.lower()\n",
    "    # This helps to remove unique hashes for duplicates such as:\n",
    "    # task_id = task_id )\n",
    "    # task_id = task_id \n",
    "    # task_id = task_id )\n",
    "    return chunk\n",
    "\n",
    "def generate_chunk_hash(\n",
    "    document_chunk: any\n",
    ") -> any:\n",
    "    cleaned_chunk = format_chunk(\n",
    "        document_chunk = document_chunk\n",
    "    )\n",
    "    return hashlib.md5(cleaned_chunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "def create_vector_points(\n",
    "    qdrant_client: any,\n",
    "    document_database: any,\n",
    "    document_collection: any,\n",
    "    document_type: any,\n",
    "    document_id: str, \n",
    "    document_chunks: any,\n",
    "    document_embeddings: any,\n",
    "    vector_collection: any\n",
    "):\n",
    "    #print('Create points')\n",
    "    vector_points = []\n",
    "    vector_index = 0\n",
    "    added_hashes = []\n",
    "    for chunk in document_chunks:\n",
    "        vector_id = document_id + '-' + str(vector_index + 1)\n",
    "        vector_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, vector_id))\n",
    "\n",
    "        chunk_hash = generate_chunk_hash(\n",
    "            document_chunk = chunk\n",
    "        )\n",
    "        \n",
    "        existing_chunks = qdrant_search_data(\n",
    "            qdrant_client = qdrant_client,\n",
    "            collection_name = vector_collection,\n",
    "            scroll_filter = models.Filter(\n",
    "                must = [\n",
    "                    models.FieldCondition(\n",
    "                        key = 'chunk_hash',\n",
    "                        match = models.MatchValue(\n",
    "                            value = chunk_hash\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            limit = 1\n",
    "        )\n",
    "        # Removes duplicates\n",
    "        if len(existing_chunks[0]) == 0:\n",
    "            if not chunk_hash in added_hashes:\n",
    "                given_vector = document_embeddings[vector_index]\n",
    "\n",
    "                chunk_point = PointStruct(\n",
    "                    id = vector_uuid, \n",
    "                    vector = given_vector,\n",
    "                    payload = {\n",
    "                        'database': document_database,\n",
    "                        'collection': document_collection,\n",
    "                        'document': document_id,\n",
    "                        'type': document_type,\n",
    "                        'chunk': chunk,\n",
    "                        'chunk_hash': chunk_hash\n",
    "                    }\n",
    "                )\n",
    "                added_hashes.append(chunk_hash)\n",
    "                vector_points.append(chunk_point)\n",
    "        vector_index += 1\n",
    "    return vector_points\n",
    "\n",
    "def store_document_vectors(\n",
    "    qdrant_client: any,\n",
    "    document_database,\n",
    "    document_collection,\n",
    "    document: any,\n",
    "    configuration: any,\n",
    "    vector_collection: str\n",
    ") -> bool:\n",
    "    #print('Store vectors')\n",
    "    document_id = str(document['_id'])\n",
    "    document_type = document['type']\n",
    "\n",
    "    document_packet = {}\n",
    "    try:\n",
    "        document_packet = create_document_packet(\n",
    "            document = document,\n",
    "            configuration = configuration\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(document_database,document_collection,document_id)\n",
    "        print(e)\n",
    "\n",
    "    if 0 == len(document_packet):\n",
    "        return False\n",
    "        \n",
    "    document_chunks = document_packet['chunks']\n",
    "    document_embeddings = document_packet['embeddings']\n",
    "    \n",
    "    if 0 == len(document_embeddings):\n",
    "        return False\n",
    "    \n",
    "    vector_collections = qdrant_list_collections(\n",
    "        qdrant_client = qdrant_client\n",
    "    )\n",
    "    \n",
    "    collection_created = None\n",
    "    if not vector_collection in vector_collections:\n",
    "        collection_configuration = VectorParams(\n",
    "            size = len(document_embeddings[0]), \n",
    "            distance = Distance.COSINE\n",
    "        )\n",
    "        collection_created = qdrant_create_collection(\n",
    "            qdrant_client = qdrant_client,\n",
    "            collection_name = vector_collection,\n",
    "            configuration = collection_configuration\n",
    "        )\n",
    "\n",
    "    vector_points = create_vector_points(\n",
    "        qdrant_client = qdrant_client,\n",
    "        document_database = document_database,\n",
    "        document_collection = document_collection,\n",
    "        document_type = document_type,\n",
    "        document_id = document_id,\n",
    "        document_chunks = document_chunks,\n",
    "        document_embeddings = document_embeddings,\n",
    "        vector_collection = vector_collection\n",
    "    )\n",
    "\n",
    "    if 0 == len(vector_points):\n",
    "        return False\n",
    "    \n",
    "    points_stored = qdrant_upsert_points(\n",
    "        qdrant_client = qdrant_client, \n",
    "        collection_name = vector_collection,\n",
    "        points = vector_points\n",
    "    )\n",
    "    \n",
    "    return True\n",
    "\n",
    "def store_vectors(\n",
    "    qdrant_client: any,\n",
    "    configuration: any,\n",
    "    storage_documents: any\n",
    "):\n",
    "    print('Storing vectors')\n",
    "    # create progress logs\n",
    "    amount_of_databases = len(storage_documents)\n",
    "    database_index = 1\n",
    "    for document_database, document_collections in storage_documents.items():\n",
    "        vector_collection = document_database.replace('|','-') + '-embeddings'\n",
    "        amount_of_collections = len(document_collections)\n",
    "        collection_index = 1\n",
    "        for document_collection, documents in document_collections.items():\n",
    "            amount_of_documents = len(documents)\n",
    "            document_index = 1\n",
    "            for document in documents:\n",
    "                stored = store_document_vectors(\n",
    "                    qdrant_client = qdrant_client,\n",
    "                    document_database = document_database,\n",
    "                    document_collection = document_collection,\n",
    "                    document = document,\n",
    "                    configuration = configuration,\n",
    "                    vector_collection = vector_collection\n",
    "                )\n",
    "                print('Documents: ' + str(document_index) + '|' + str(amount_of_documents))\n",
    "                document_index += 1\n",
    "            print('Collections: ' + str(collection_index) + '|' + str(amount_of_collections))\n",
    "            collection_index += 1\n",
    "        print('Databases: ' + str(database_index) + '|' + str(amount_of_databases))\n",
    "        database_index += 1\n",
    "    print('Vectors stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c446d6-dca4-4d47-9498-1a9c2441f1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfniila/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/qdrant_client/qdrant_remote.py:130: UserWarning: Api key is used with an insecure connection.\n",
      "  warnings.warn(\"Api key is used with an insecure connection.\")\n"
     ]
    }
   ],
   "source": [
    "qdrant_client = qdrant_setup_client(\n",
    "    api_key = 'qdrant_key',\n",
    "    address = '127.0.0.1', \n",
    "    port = '6333'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ab5ac0-2963-42f7-a67f-d62380bdbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_configuration = {\n",
    "    'python': {\n",
    "        'language': Language.PYTHON,\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'markdown': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'yaml': {\n",
    "        'chunk-size': 50,\n",
    "        'chunk-overlap': 0,\n",
    "        'model-name': 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42fdc095-6e46-4b77-96e4-6822a6ab7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing vectors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvector_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_documents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstored_documents\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 207\u001b[0m, in \u001b[0;36mcreate_vectors\u001b[0;34m(qdrant_client, configuration, storage_documents)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m document_collection, documents \u001b[38;5;129;01min\u001b[39;00m document_collections\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m--> 207\u001b[0m             stored \u001b[38;5;241m=\u001b[39m \u001b[43mstore_document_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdocument_database\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdocument_collection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvector_collection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvector_collection\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVectors stored\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 142\u001b[0m, in \u001b[0;36mstore_document_vectors\u001b[0;34m(qdrant_client, document_database, document_collection, document, configuration, vector_collection)\u001b[0m\n\u001b[1;32m    140\u001b[0m document_packet \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     document_packet \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_document_packet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfiguration\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(document_database,document_collection,document_id)\n",
      "Cell \u001b[0;32mIn[56], line 36\u001b[0m, in \u001b[0;36mcreate_document_packet\u001b[0;34m(document, configuration)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m     34\u001b[0m         filtered_chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[0;32m---> 36\u001b[0m vector_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mlangchain_create_chunk_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mused_configuration\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel-name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfiltered_chunks\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m packet \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m: filtered_chunks,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: vector_embedding\n\u001b[1;32m     44\u001b[0m }\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packet\n",
      "Cell \u001b[0;32mIn[51], line 43\u001b[0m, in \u001b[0;36mlangchain_create_chunk_embeddings\u001b[0;34m(model_name, chunks)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlangchain_create_chunk_embeddings\u001b[39m(\n\u001b[1;32m     40\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     41\u001b[0m     chunks: \u001b[38;5;28many\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28many\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     chunk_embeddings \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39membed_documents(\n\u001b[1;32m     47\u001b[0m         texts \u001b[38;5;241m=\u001b[39m chunks\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunk_embeddings\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py:61\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:287\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    278\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[1;32m    281\u001b[0m     model_name_or_path,\n\u001b[1;32m    282\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    286\u001b[0m ):\n\u001b[0;32m--> 287\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    300\u001b[0m         model_name_or_path,\n\u001b[1;32m    301\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    309\u001b[0m     )\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1487\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_kwargs:\n\u001b[1;32m   1485\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_args\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(config_kwargs)\n\u001b[0;32m-> 1487\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:53\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 53\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:976\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    974\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 976\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    978\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1303\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1756\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1670\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:399\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Project/cloud-hpc-oss-mlops-platform/applications/development/LLMs/pipeline/llm_venv/lib/python3.10/site-packages/urllib3/connectionpool.py:463\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_vectors(\n",
    "    qdrant_client = qdrant_client,\n",
    "    configuration = vector_configuration,\n",
    "    storage_documents = stored_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07548d-e73d-49f4-afce-cb74d798974e",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a989b4b-c898-433d-809a-ebf091e24516",
   "metadata": {},
   "source": [
    "## SpaCy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664663d3-39ac-4acb-bc1b-e741daebf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_document_keywords(\n",
    "    document: str\n",
    "):\n",
    "    doc = nlp(document.lower())\n",
    "    \n",
    "    keywords = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop               \n",
    "        and not token.is_punct              \n",
    "        and not token.is_space              \n",
    "        and len(token) > 1                  \n",
    "    ]\n",
    "    \n",
    "    keywords = list(set(keywords))\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8d0c9-7e6a-477e-bd1e-f5a3a5925a6c",
   "metadata": {},
   "source": [
    "## Meili Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06b314-ccbc-41cf-8160-2bb92286e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meilisearch as ms\n",
    "\n",
    "def meili_is_client(\n",
    "    storage_client: any\n",
    ") -> any:\n",
    "    try:\n",
    "        return isinstance(storage_client, ms.Connection)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "def meili_setup_client(\n",
    "    host: str, \n",
    "    api_key: str\n",
    ") -> any:\n",
    "    try:\n",
    "        meili_client = ms.Client(\n",
    "            url = host, \n",
    "            api_key = api_key\n",
    "        )\n",
    "        return meili_client \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_get_index( \n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_check_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        meili_client.get_index(\n",
    "            uid = index_name\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def meili_remove_index(\n",
    "    meili_client: any, \n",
    "    index_name: str\n",
    ") -> bool:\n",
    "    try:\n",
    "        response = meili_client.index(\n",
    "            index_name = index_name\n",
    "        ).delete()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_list_indexes(\n",
    "    meili_client: any\n",
    ") -> bool:\n",
    "    try:\n",
    "        indexes = meili_client.get_indexes()\n",
    "        return indexes\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_add_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    documents: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.add_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_set_filterable(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    attributes: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_filterable_attributes(attributes)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_search_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    query: any, \n",
    "    options: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_get_index(\n",
    "            meili_client = meili_client,\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.search(\n",
    "            query,\n",
    "            options\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def meili_update_documents(\n",
    "    meili_client, \n",
    "    index_name, \n",
    "    documents\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.update_documents(\n",
    "            documents = documents\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def meili_delete_documents(\n",
    "    meili_client: any, \n",
    "    index_name: str, \n",
    "    ids: any\n",
    ") -> any:\n",
    "    try:\n",
    "        index = meili_client.index(\n",
    "            index_name = index_name\n",
    "        )\n",
    "        response = index.delete_documents(\n",
    "            document_ids = ids\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b71ef-af43-4e45-a4d2-84e41acccb87",
   "metadata": {},
   "source": [
    "## Keyword Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4bef1-d26c-44a3-a6b6-c1249a4caf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def store_database_keywords(\n",
    "    meili_client: any\n",
    ") -> any:\n",
    "    \n",
    "\n",
    "def store_keywords(\n",
    "    meili_client: any,\n",
    "    storage_documents: any\n",
    "):\n",
    "    for database, collections in storage_documents.items():\n",
    "        keyword_collection_name = database + '-keywords'\n",
    "        keyword_documents = []\n",
    "        for collection, documents in collections.items():\n",
    "            document_index = 0\n",
    "            for document in documents:\n",
    "                document_id = str(document['_id'])\n",
    "                document_data = document['data']\n",
    "                document_type = document['type']\n",
    "                \n",
    "                document_keywords = get_document_keywords(\n",
    "                    document = document_data\n",
    "                )\n",
    "                \n",
    "                keyword_id = document_id + '-' + str(document_index + 1)\n",
    "                keyword_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, keyword_id))\n",
    "    \n",
    "                payload = {\n",
    "                    'id': keyword_uuid,\n",
    "                    'database': database,\n",
    "                    'collection': collection,\n",
    "                    'document': document_id,\n",
    "                    'type': document_type,\n",
    "                    'keywords': document_keywords\n",
    "                }\n",
    "    \n",
    "                keyword_documents.append(payload)\n",
    "            \n",
    "        stored = meili_add_documents(\n",
    "            meili_client = meili_client,\n",
    "            index_name = keyword_collection_name,\n",
    "            documents = keyword_documents\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da8311-8864-46fe-b533-61908700ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "meili_client = meili_setup_client(\n",
    "    host = 'http://127.0.0.1:7700', \n",
    "    api_key = 'meili_key'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d8d0f-d008-4013-b352-4960815777f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a340932c-f57d-464c-9d0e-0baa4c20077f",
   "metadata": {},
   "source": [
    "## Hybrid Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd2a9e-4a99-45b2-83cb-da2f9895c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prompt(\n",
    "    prompt: str\n",
    ") -> any:\n",
    "    prompt = prompt.lower()\n",
    "    prompt = re.sub(r'\\s+', ' ', prompt)\n",
    "    prompt = re.sub(r'[^\\w\\s]', '', prompt)\n",
    "    return prompt.strip()\n",
    "\n",
    "def generate_prompt_embedding_query(\n",
    "    model_name: str,\n",
    "    prompt: any\n",
    ") -> any:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = model_name\n",
    "    )\n",
    "    embedding = embedding_model.embed_documents(\n",
    "        texts = [prompt]\n",
    "    )\n",
    "    return embedding[0]\n",
    "\n",
    "def spacy_find_keywords(\n",
    "    text: str\n",
    "):\n",
    "    formatted = nlp(text.lower())\n",
    "    \n",
    "    keywords = [\n",
    "        token.lemma_ for token in formatted\n",
    "        if not token.is_stop               \n",
    "        and not token.is_punct              \n",
    "        and not token.is_space              \n",
    "        and len(token) > 1                  \n",
    "    ]\n",
    "    \n",
    "    keywords = list(set(keywords))\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def generate_prompt_keyword_query(\n",
    "    prompt: any\n",
    ") -> any:\n",
    "    keywords = spacy_find_keywords(\n",
    "        text = prompt\n",
    "    )\n",
    "    keyword_query = ' OR '.join([f'keywords = \"{keyword}\"' for keyword in keywords])\n",
    "    return keyword_query\n",
    "\n",
    "def calculate_keyword_score(\n",
    "    keyword_query: str,\n",
    "    keyword_list: any\n",
    ") -> any:\n",
    "    match = 0\n",
    "    asked_keywords = keyword_query.split('OR')\n",
    "    for asked_keyword in asked_keywords:\n",
    "        formatted = asked_keyword.replace('keywords =', '')\n",
    "        formatted = formatted.replace('\"', '')\n",
    "        formatted = formatted.replace(' ', '')\n",
    "        \n",
    "        if formatted in keyword_list:\n",
    "            match += 1\n",
    "            \n",
    "    query_length = len(asked_keywords)\n",
    "    keyword_length = len(keyword_list)\n",
    "\n",
    "    if match == 0:\n",
    "        return 0.0\n",
    "\n",
    "    normalized = match / ((query_length * keyword_length) ** 0.5)\n",
    "    return normalized\n",
    "    \n",
    "def vector_search_collection(\n",
    "    vector_client: any,\n",
    "    search_client: any,\n",
    "    prompt: str,\n",
    "    top_k: int\n",
    "):\n",
    "\n",
    "    cleaned_prompt = clean_prompt(\n",
    "        prompt = prompt\n",
    "    )\n",
    "\n",
    "    prompt_embedding_query = generate_prompt_embedding_query(\n",
    "        model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        prompt = cleaned_prompt\n",
    "    )\n",
    "\n",
    "    prompt_keyword_query = generate_prompt_keyword_query(\n",
    "        prompt = cleaned_prompt\n",
    "    )\n",
    "\n",
    "    qdrant_collections = [\n",
    "        'llm-rag-code-embeddings',\n",
    "        'llm-rag-workflows-embeddings'\n",
    "    ]\n",
    "    \n",
    "    recommeded_cases = []\n",
    "    for collection in qdrant_collections:\n",
    "        results = qdrant_search_vectors(\n",
    "            qdrant_client = vector_client,  \n",
    "            collection_name = collection,\n",
    "            query_vector = prompt_embedding_query,\n",
    "            limit = top_k\n",
    "        ) \n",
    "        \n",
    "        for result in results:\n",
    "            res_database = result.payload['database']\n",
    "            res_collection = result.payload['collection']\n",
    "            res_document = result.payload['document']\n",
    "            res_type = result.payload['type']\n",
    "            res_score = result.score\n",
    "            \n",
    "            res_case = {\n",
    "                'source': 'vector',\n",
    "                'database': res_database,\n",
    "                'collection': res_collection,\n",
    "                'document': res_document,\n",
    "                'type': res_type,\n",
    "                'score': res_score\n",
    "            }\n",
    "            \n",
    "            recommeded_cases.append(res_case)\n",
    "            \n",
    "    meili_collections = [\n",
    "        'llm-rag-code-keywords',\n",
    "        'llm-rag-workflows-keywords'\n",
    "    ]\n",
    "\n",
    "    recommeded_keyword_cases = []\n",
    "    for index in meili_collections:\n",
    "        results = meili_search_documents(\n",
    "            meili_client = search_client, \n",
    "            index_name = index, \n",
    "            query = \"\", \n",
    "            options = {\n",
    "                'filter': prompt_keyword_query,\n",
    "                'attributesToRetrieve': ['database','collection','document', 'keywords'],\n",
    "                'limit': top_k\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for result in results['hits']:\n",
    "            res_database = result['database']\n",
    "            res_collection = result['collection']\n",
    "            res_document = result['document']\n",
    "            res_keywords = result['keywords']\n",
    "            \n",
    "            res_score = calculate_keyword_score(\n",
    "                keyword_query = prompt_keyword_query,\n",
    "                keyword_list = res_keywords\n",
    "            )\n",
    "\n",
    "            res_case = {\n",
    "                'source': 'search',\n",
    "                'database': res_database,\n",
    "                'collection': res_collection,\n",
    "                'document': res_document,\n",
    "                'type': res_type,\n",
    "                'score': res_score\n",
    "            }\n",
    "\n",
    "            recommeded_cases.append(res_case)\n",
    "\n",
    "    return recommeded_cases\n",
    "\n",
    "def get_top_document_metadata(\n",
    "    collection: str,\n",
    "    alpha: float\n",
    ") -> any:\n",
    "    df = pd.DataFrame(collection)\n",
    "    ids_with_both = df.groupby('document')['source'].nunique()\n",
    "    ids_with_both = ids_with_both[ids_with_both > 1].index\n",
    "    filtered_df = df[df['document'].isin(ids_with_both)]\n",
    "\n",
    "    matched_documents = []\n",
    "    for index_i, row_i in filtered_df[filtered_df['source'] == 'vector'].iterrows():\n",
    "        vector_source = row_i['source']\n",
    "        vector_database = row_i['database']\n",
    "        vector_collection = row_i['collection']\n",
    "        vector_id = row_i['document']\n",
    "        vector_type = row_i['type']\n",
    "        vector_score = row_i['score']\n",
    "        \n",
    "        for index_j, row_j in filtered_df[filtered_df['source'] == 'search'].iterrows():\n",
    "            search_source = row_j['source']\n",
    "            search_database = row_j['database']\n",
    "            search_collection = row_j['collection']\n",
    "            search_id = row_j['document']\n",
    "            search_type = row_j['type']\n",
    "            search_score = row_j['score']\n",
    "            \n",
    "            if vector_database == search_database:\n",
    "                if vector_collection == search_collection:\n",
    "                    if vector_type == search_type:\n",
    "                        if vector_id == search_id:\n",
    "                            hybrid_score = vector_score * alpha + search_score * (1-alpha)\n",
    "    \n",
    "                            matched_documents.append({\n",
    "                                'source': 'hybrid',\n",
    "                                'database': search_database,\n",
    "                                'collection': search_collection,\n",
    "                                'document': search_id,\n",
    "                                'score': hybrid_score\n",
    "                            })\n",
    "    \n",
    "    match_df = pd.DataFrame(matched_documents)\n",
    "    print(match_df)\n",
    "    return match_df.nlargest(1, 'score').values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9dd18-5f6a-4869-a606-b8ed73cbdde9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
