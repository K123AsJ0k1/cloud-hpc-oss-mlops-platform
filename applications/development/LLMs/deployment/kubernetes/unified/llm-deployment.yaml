apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-server
  labels:
    app: llm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-server 
  template:
    metadata:
      labels:
        app: llm-server 
    spec:
      containers:
        - name: llm-server
          image: ghcr.io/open-webui/open-webui:ollama
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: ollama-volume
              mountPath: /root/.ollama
            - name: webui-volume
              mountPath: /app/backend/data
          env:
          - name: WEBUI_AUTH
            value: 'False'
      volumes:
        - name: ollama-volume
          persistentVolumeClaim:
            claimName: ollama-pvc
        - name: webui-volume
          persistentVolumeClaim:
            claimName: webui-pvc